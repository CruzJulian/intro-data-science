[
  {
    "objectID": "unidad_01.html",
    "href": "unidad_01.html",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "En esta etapa aplicamos los algoritmos de agregación presentados a nuestro dataset de pinturas de Bob Ross. Es necesario que tengamos muy claro el contexto de nuestro trabajo, en caso de que haya dudas podemos volver a la presentación del proyecto.\nLos algoritmos que vamos a aplicar son \\(k\\)-means, agregación jerárquica y DBSCAN. Para ejecutarlos utilizamos código en R y en Python. Al final, revisamos en una tabla cuántos grupos tenemos y cuántas pinturas tiene cada grupo. Esto con el fin de interpretar estos resultados con precisión en las siguientes unidades.\n\n\nLa primera etapa del proyecto está orientada a cumplir el primer objetivo específico:\n\nAplicar algoritmos de agregación al dataset e identificar los grupos subyacentes de las pinturas."
  },
  {
    "objectID": "unidad_01.html#objetivo-actual",
    "href": "unidad_01.html#objetivo-actual",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "La primera etapa del proyecto está orientada a cumplir el primer objetivo específico:\n\nAplicar algoritmos de agregación al dataset e identificar los grupos subyacentes de las pinturas."
  },
  {
    "objectID": "unidad_01.html#algoritmo-jerárquico",
    "href": "unidad_01.html#algoritmo-jerárquico",
    "title": "Proyecto: The joy of programming",
    "section": "Algoritmo jerárquico",
    "text": "Algoritmo jerárquico\nEl siguiente es el código para el algoritmo jerárquico. En primer lugar calculamos la matriz de distancias del conjunto de datos. y luego aplicamos el algoritmo especificando la distancia entre grupos, en este caso es \"ward\".\n\nRPython\n\n\n\ntb_pinturas_caract %&gt;% \n  dist(method = \"euclidean\") %&gt;% \n  hclust(method = \"ward.D2\") -&gt; ls_jerarquico \n\ntb_pinturas_caract %&gt;% dist(): Calcula la matriz de distancias entre las filas del DataFrame tb_pinturas_caract. El operador %&gt;% (pipe) se utiliza para encadenar funciones en un flujo.\n-&gt; mt_dist_pint: Asigna la matriz de distancias resultante a un objeto llamado mt_dist_pint.\nmt_dist_pint %&gt;% hclust(method = \"ward.D2\"): Realiza un clúster jerárquico utilizando el método de enlace “ward.D2” en la matriz de distancias mt_dist_pint.\n-&gt; ls_jerarquico: Asigna el resultado del clúster jerárquico a un objeto llamado ls_jerarquico.\n\n\n\nmod_jerarquico = AgglomerativeClustering(\n    n_clusters = 6, \n    metric = 'euclidean', \n    linkage ='ward'\n    )\nmod_jerarquico = mod_jerarquico.fit(tb_pinturas_caract)\n\nAgglomerativeClustering(...): Se instancia un objeto del tipo AgglomerativeClustering, que implementa el algoritmo de agrupamiento jerárquico.\nn_clusters=6: Especifica el número de clústeres deseados, en este caso, se establece en 6.\nmetric='euclidean': Define la métrica de distancia utilizada para calcular la similaridad entre puntos. En este caso, se utiliza la distancia euclidiana.\nlinkage='ward': Especifica el método de enlace utilizado en el algoritmo, en este caso, el enlace “ward” que minimiza la varianza entre los clústeres.\nmod_jerarquico.fit(tb_pinturas_caract): Se ajusta el modelo de agrupamiento jerárquico a los datos en tb_pinturas_caract. El modelo aprenderá a asignar las observaciones en el número especificado de clústeres utilizando la distancia euclidiana y el método de enlace “ward”.\n\n\n\nA continuación, elaboramos el dendrograma. En este observamos que al hacer un corte podemos obtener 6 grupos bien definidos.\n\nRPython\n\n\n\nls_jerarquico %&gt;% \n  fviz_dend(6, show_labels = FALSE, horiz = TRUE) +\n  geom_hline(yintercept = 9.5)\n\n\n\n\n\n\n\n\nls_jerarquico %&gt;% as.dendrogram() %&gt;% ...: Convierte el resultado del clúster jerárquico (ls_jerarquico) en un dendrograma utilizando la función as.dendrogram() y luego encadena varias funciones de configuración y visualización utilizando el operador %&gt;% (pipe).\nset(\"labels_col\", k=6): Establece colores diferentes para las etiquetas según los grupos definidos por k=6 clústeres.\nset(\"branches_k_color\", k=6): Asigna colores diferentes a las ramas del dendrograma según los grupos definidos por k=6 clústeres.\nset(\"labels\", NULL): Elimina las etiquetas originales del dendrograma.\nplot(horiz=TRUE, axes=FALSE): Realiza la visualización del dendrograma con orientación horizontal y sin mostrar ejes.\nabline(v = 9, lty = 2): Agrega una línea vertical discontinua en la posición 9 en el dendrograma. Esta línea nos proporciona la altura a la que se realiza la partición en 6 grupos.\n\n\n\nplt.figure(figsize=(10,8))\ndendrogram = sch.dendrogram(\n  sch.linkage(tb_pinturas_caract, method  = \"ward\"),\n  color_threshold = 10, orientation = \"left\", no_labels = True)\nplt.title('Dendrogram')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\nplt.figure(figsize=(10,8)): Crea una nueva figura de tamaño 10x8 pulgadas utilizando matplotlib.pyplot.\ndendrogram = sch.dendrogram(...): Calcula y genera el dendrograma utilizando la función dendrogram de scipy.cluster.hierarchy. Se utiliza sch.linkage para realizar el agrupamiento jerárquico con el método de enlace “ward”.\nsch.linkage(tb_pinturas_caract, method=\"ward\"): Realiza el agrupamiento jerárquico con el método de enlace “ward” en los datos contenidos en tb_pinturas_caract.\ncolor_threshold=9: Define el umbral de color para resaltar los clústeres en el dendrograma. Las ramas por debajo de este umbral se muestran en el mismo color.\norientation=\"left\": Orienta el dendrograma hacia la izquierda.\nno_labels=True: Suprime las etiquetas de las hojas del dendrograma.\nplt.title('Dendrogram'): Agrega un título al dendrograma.\nplt.show(): Muestra la figura con el dendrograma.\nplt.close(): Cierra la figura, liberando recursos y previniendo posibles superposiciones en visualizaciones futuras.\n\n\n\nLuego extraemos los grupos y los guardamos en una nueva columna de nuestros datos. Adicionalmente, revisamos el tamaño de cada grupo.\n\nRPython\n\n\n\ntb_grupos %&lt;&gt;% \n  mutate(\n    grupos_jerar = ls_jerarquico %&gt;% \n  cutree(6) \n  )\n\ntb_grupos %&gt;% \n  count(grupos_jerar) -&gt; tb_cuenta_grupos\n\n\n\n\n\n\ngrupos_jerar\nn\n\n\n\n\n1\n141\n\n\n2\n41\n\n\n3\n86\n\n\n4\n48\n\n\n5\n39\n\n\n6\n48\n\n\n\n\n\ntb_grupos %&lt;&gt;%: Utiliza el operador %&lt;&gt;% para realizar la asignación en el propio objeto tb_grupos, es decir, modifica tb_grupos in-place.\nmutate(...): Crea o modifica columnas en el DataFrame.\ngrupos_jerar: Se crea una nueva columna llamada grupos_jerar en el DataFrame tb_grupos.\nls_jerarquico %&gt;% cutree(6): Utiliza la función cutree para asignar a cada observación en ls_jerarquico un número de grupo (clúster) según la división en 6 grupos realizada previamente.\ntb_grupos %&gt;% count(grupos_jerar): Utiliza la función count de dplyr para contar la frecuencia de cada valor único en la columna grupos_jerar de tb_grupos.\n\n\n\ntb_grupos['grupos_jerar'] = (mod_jerarquico\n  .fit_predict(tb_pinturas_caract)\n  .tolist())\n\ntb_cuenta_grupos = (tb_grupos.grupos_jerar\n  .value_counts()\n  .rename('count')\n  .reset_index())\n\n\n\n\n\n\n\n\ngrupos_jerar\ncount\n\n\n\n\n0\n1\n113\n\n\n1\n0\n95\n\n\n2\n5\n64\n\n\n3\n2\n48\n\n\n4\n4\n43\n\n\n5\n3\n40\n\n\n\n\n\n\ntb_grupos[\"grupos_jerar\"] = mod_jerarquico.fit_predict(tb_pinturas_caract):\nmod_jerarquico.fit_predict(tb_pinturas_caract): Utiliza el método fit_predict de AgglomerativeClustering para realizar el ajuste y predicción de grupos jerárquicos en los datos contenidos en tb_pinturas_caract. Los resultados se asignan a una nueva columna llamada grupos_jerar en el DataFrame tb_grupos.\ntb_grupos.grupos_jerar.value_counts():\ntb_grupos.grupos_jerar: Accede a la columna grupos_jerar en el DataFrame tb_grupos.\nvalue_counts(): Calcula la frecuencia de cada valor único en la columna grupos_jerar."
  },
  {
    "objectID": "unidad_01.html#algoritmo-k-means",
    "href": "unidad_01.html#algoritmo-k-means",
    "title": "Proyecto: The joy of programming",
    "section": "Algoritmo \\(k\\)-means",
    "text": "Algoritmo \\(k\\)-means\nEl algoritmo \\(k\\)-means se ejecuta con el siguiente código.\n\nRPython\n\n\n\ntb_pinturas_caract %&gt;% \n  kmeans(6) -&gt; ls_kmeans\n\ntb_pinturas_caract %&gt;% kmeans(6) -&gt; ls_kmeans:\n%&gt;%: Operador pipe que pasa el resultado de la operación anterior como el primer argumento de la siguiente operación.\nkmeans(6): Aplica el algoritmo k-Means para realizar el agrupamiento en tb_pinturas_caract con 6 clústeres.\n-&gt; ls_kmeans: Asigna el resultado del agrupamiento a un objeto llamado ls_kmeans.\n\n\n\nmod_kmeans = KMeans(\n        init=\"random\",\n        n_clusters=6,\n        n_init = 1\n    )\n\nmod_kmeans = mod_kmeans.fit(tb_pinturas_caract)\n\nKMeans: Crea una instancia del modelo de k-Means utilizando la clase KMeans de scikit-learn.\ninit=\"random\": Especifica que la inicialización de los centroides se realice de manera aleatoria.\nn_clusters=6: Define el número de clústeres deseados, en este caso, se establece en 6.\nfit(tb_pinturas_caract): Ajusta el modelo de k-Means a los datos contenidos en tb_pinturas_caract. El modelo aprenderá a asignar las observaciones en el número especificado de clústeres.\n\n\n\nLuego extraemos los grupos y los guardamos en una nueva columna de nuestros datos. Adicionalmente, revisamos el tamaño de cada grupo.\n\nRPython\n\n\n\ntb_grupos %&lt;&gt;% \n  mutate(\n    grupos_kmeans = ls_kmeans %&gt;% \n  pluck(\"cluster\") \n  )\n\ntb_grupos %&gt;% \n  count(grupos_kmeans) -&gt; tb_cuenta_grupos\n\n\n\n\n\n\ngrupos_kmeans\nn\n\n\n\n\n1\n123\n\n\n2\n48\n\n\n3\n51\n\n\n4\n35\n\n\n5\n75\n\n\n6\n71\n\n\n\n\n\n%&lt;&gt;%: Operador pipe que realiza la asignación in-place al propio objeto.\nmutate(...): Crea o modifica columnas en el DataFrame.\ngrupos_kmeans: Se crea una nueva columna llamada grupos_kmeans en el DataFrame tb_grupos.\nls_kmeans %&gt;% pluck(\"cluster\"): Extrae la información de los clústeres asignados por el modelo de k-Means (ls_kmeans) utilizando la función pluck y la clave “cluster”.\ntb_grupos %&gt;% count(grupos_kmeans): Utiliza la función count de dplyr para contar la frecuencia de cada valor único en la columna grupos_kmeans de tb_grupos.\n\n\n\ntb_grupos['grupos_kmeans'] = mod_kmeans.labels_\n\ntb_cuenta_grupos = (tb_grupos.grupos_kmeans\n  .value_counts()\n  .rename('count')\n  .reset_index())\n\n\n\n\n\n\n\n\ngrupos_kmeans\ncount\n\n\n\n\n0\n2\n94\n\n\n1\n0\n84\n\n\n2\n1\n68\n\n\n3\n5\n60\n\n\n4\n4\n49\n\n\n5\n3\n48\n\n\n\n\n\n\nmod_kmeans.labels_: Accede a los resultados de la asignación de clústeres realizada por el modelo de k-Means (mod_kmeans). Los resultados se encuentran en el atributo labels_.\ntb_grupos['grupos_kmeans']: Crea una nueva columna llamada grupos_kmeans en el DataFrame tb_grupos y asigna los resultados de la asignación de clústeres.\ntb_grupos.grupos_kmeans: Accede a la columna grupos_kmeans en el DataFrame tb_grupos.\nvalue_counts(): Calcula la frecuencia de cada valor único en la columna grupos_kmeans."
  },
  {
    "objectID": "unidad_01.html#algoritmo-dbscan",
    "href": "unidad_01.html#algoritmo-dbscan",
    "title": "Proyecto: The joy of programming",
    "section": "Algoritmo DBSCAN",
    "text": "Algoritmo DBSCAN\nEl algoritmo DBSCAN se ejecuta con el siguiente código. Es importante probar distintas configuraciones para los hiperparámetros \\(\\epsilon\\) y \\(n_{min}\\). En este caso se utilizamos \\(\\epsilon = 1.5\\) y \\(n_{min} = 3\\).\n\nRPython\n\n\n\ntb_pinturas_caract %&gt;% \n  dbscan(eps = 1.5, minPts = 3) -&gt; ls_dbscan\n\n%&gt;%: Operador pipe que pasa el resultado de la operación anterior como el primer argumento de la siguiente operación.\ndbscan(eps = 1.5, minPts = 3): Aplica el algoritmo DBSCAN para realizar el agrupamiento en tb_pinturas_caract. Se especifican los parámetros eps (radio máximo para formar un clúster) y minPts (número mínimo de puntos para formar un clúster).\n-&gt; ls_dbscan: Asigna el resultado del agrupamiento a un objeto llamado ls_dbscan.\n\n\n\nmod_dbscan = DBSCAN(eps=1.5, min_samples=3)\n\nmod_dbscan = mod_dbscan.fit(tb_pinturas_caract)\n\nDBSCAN: Crea una instancia del modelo de DBSCAN utilizando la clase DBSCAN de scikit-learn.\neps=1.5: Especifica el radio máximo para formar un clúster.\nmin_samples=3: Especifica el número mínimo de puntos para formar un clúster.\nfit(tb_pinturas_caract): Ajusta el modelo DBSCAN a los datos contenidos en tb_pinturas_caract. El modelo aprenderá a asignar las observaciones en clústeres según los parámetros especificados.\n\n\n\nLuego extraemos los grupos y los guardamos en una nueva columna de nuestros datos. Adicionalmente, revisamos el tamaño de cada grupo.\n\nRPython\n\n\n\ntb_grupos %&lt;&gt;% \n  mutate(\n    grupos_dbscan = ls_dbscan %&gt;% \n  pluck(\"cluster\") \n  )\n\ntb_grupos %&gt;% \n  count(grupos_dbscan) -&gt; tb_cuenta_grupos\n\n\n\n\n\n\ngrupos_dbscan\nn\n\n\n\n\n0\n123\n\n\n1\n233\n\n\n2\n6\n\n\n3\n7\n\n\n4\n21\n\n\n5\n8\n\n\n6\n5\n\n\n\n\n\n%&lt;&gt;%: Operador pipe que realiza la asignación in-place al propio objeto.\nmutate(...): Crea o modifica columnas en el DataFrame.\ngrupos_dbscan: Se crea una nueva columna llamada grupos_dbscan en el DataFrame tb_pinturas.\nls_dbscan %&gt;% pluck(\"cluster\"): Extrae la información de los clústeres asignados por el modelo DBSCAN (ls_dbscan) utilizando la función pluck y la clave “cluster”.\ntb_pinturas %&gt;% count(grupos_dbscan): Utiliza la función count de dplyr para contar la frecuencia de cada valor único en la columna grupos_dbscan de tb_pinturas.\n\n\n\ntb_grupos['grupos_dbscan'] = mod_dbscan.labels_\n\ntb_cuenta_grupos = (tb_grupos.grupos_dbscan\n  .value_counts()\n  .rename('count')\n  .reset_index())\n\n\n\n\n\n\n\n\ngrupos_dbscan\ncount\n\n\n\n\n0\n0\n233\n\n\n1\n-1\n123\n\n\n2\n3\n21\n\n\n3\n4\n8\n\n\n4\n2\n7\n\n\n5\n1\n6\n\n\n6\n5\n5\n\n\n\n\n\n\nmod_dbscan.labels_: Accede a los resultados de la asignación de clústeres realizada por el modelo de DBSCAN (mod_dbscan). Los resultados se encuentran en el atributo labels_.\ntb_grupos['grupos_dbscan']: Crea una nueva columna llamada grupos_dbscan en el DataFrame tb_grupos y asigna los resultados de la asignación de clústeres.\ntb_grupos.grupos_dbscan: Accede a la columna grupos_dbscan en el DataFrame tb_grupos.\nvalue_counts(): Calcula la frecuencia de cada valor único en la columna grupos_dbscan."
  },
  {
    "objectID": "unidad_01.html#guardar-los-resultados",
    "href": "unidad_01.html#guardar-los-resultados",
    "title": "Proyecto: The joy of programming",
    "section": "Guardar los resultados",
    "text": "Guardar los resultados\nNo se nos puede olvidar guardar nuestros resultados. Para esto podemos utilizar archivos tipo .csv.\n\nRPython\n\n\n\nwrite_csv(tb_grupos, mi_setup$archivo_resultados)\n\nEste código guarda los resultados de los procedimientos realizados. Explicación:\n\nwrite_csv: Es una función del paquete readr que permite guardar datos en texto plano.\ntb_grupos: Es el dataframe que contiene los resultados obtenidos.\nmi_setup$archivo_resultados: Es la entrada en la lista de configuración que contiene la ruta del archivo donde se guardan los resultados.\n\n\n\n\ntb_grupos.to_csv(mi_setup[\"archivo_resultados\"], index = False)\n\nEste código guarda los resultados de los procedimientos realizados. Explicación:\n\n.to_csv: es el método de la librería pandas para escribir datos en texto plano. Se utiliza la instrucción index = False para indicar se omita en el archivo resultado la columna del índice de fila.\ntb_grupos: Es el dataframe que contiene los resultados obtenidos.\nmi_setup[\"archivo_resultados\"]: Es la entrada en la lista de configuración que contiene la ruta del archivo donde se guardan los resultados."
  },
  {
    "objectID": "unidad_02.html",
    "href": "unidad_02.html",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "En esta etapa aplicamos los algoritmos de reducción de dimensiones presentados a nuestro dataset de pinturas de Bob Ross. Podemos repasar la explicación teórica de estos algoritmos en el material del curso. La implementación que se presenta a continuación, se ejecuta en dos lenguajes R y Python.\nLos algoritmos que se presentan en esta etapa son análisis de componentes principales y t-SNE. La aplicación de estos algoritmos tiene finalidad la comprensión de los datos a través de distintas visualizaciones; que resumen las características de las pinturas, los elementos que estas contienen y su estructura de similaridad. Además, estas expresiones gráficas se utilizan para visualizar los grupos resultantes de la etapa anterior.\n\n\nLa segunda etapa del proyecto está orientada a cumplir el segundo objetivo específico:\n\nAplicar algoritmos de reducción de dimensiones a las pinturas con el fin de crear visualizaciones e interpretaciones."
  },
  {
    "objectID": "unidad_02.html#objetivo-actual",
    "href": "unidad_02.html#objetivo-actual",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "La segunda etapa del proyecto está orientada a cumplir el segundo objetivo específico:\n\nAplicar algoritmos de reducción de dimensiones a las pinturas con el fin de crear visualizaciones e interpretaciones."
  },
  {
    "objectID": "unidad_02.html#análisis-de-componentes-principales",
    "href": "unidad_02.html#análisis-de-componentes-principales",
    "title": "Proyecto: The joy of programming",
    "section": "Análisis de componentes principales",
    "text": "Análisis de componentes principales\nIniciamos aplicando el análisis de componentes principales al conjunto de datos.\n\nRPython\n\n\n\nPCA(tb_pinturas_caract, graph = FALSE, ncp = 2) -&gt; ls_pca_resultado\n\nls_pca_resultado %&gt;% \n  pluck(\"ind\", \"coord\") %&gt;% \n  as_tibble() %&gt;% \n  setNames(c(\"X\", \"Y\")) -&gt; tb_pca\n\nEste código realiza un análisis de Componentes Principales (PCA) sobre el DataFrame tb_pinturas_caract y posteriormente manipula los resultados para obtener un nuevo DataFrame llamado tb_pca. A continuación, se presenta la explicación paso a paso:\n\nRealizar el Análisis de Componentes Principales (PCA):\n\nSe utiliza la función PCA del paquete FactoMineR para realizar un análisis de Componentes Principales en el DataFrame tb_pinturas_caract.\nLos parámetros graph = FALSE indican que no se deben generar gráficos durante el análisis.\nEl parámetro ncp = 2 especifica que se deben retener las dos primeras componentes principales.\nEl resultado se almacena en la variable ls_pca_resultado.\n\nManipular los Resultados del PCA:\n\nSe utiliza la tubería (%&gt;%) junto con las funciones de purrr y dplyr para realizar varias operaciones en los resultados del PCA.\nls_pca_resultado %&gt;% pluck(\"ind\", \"coord\"):\n\nLa función pluck se utiliza para extraer las coordenadas de las observaciones del resultado del PCA.\n\n%&gt;% as_tibble():\n\nSe utiliza la función as_tibble para convertir las coordenadas a un formato de tibble.\n\n%&gt;% setNames(c(\"X\", \"Y\")):\n\nLa función setNames se usa para renombrar las columnas del tibble como “X” y “Y”.\n\nEl resultado final se almacena en el DataFrame tb_pca.\n\n\n\n\n\narr_pinturas_standar = StandardScaler().fit_transform(tb_pinturas_caract)\n\n\nmod_pca = PCA(n_components=2)\nmod_pca = mod_pca.fit(arr_pinturas_standar)\n\ntb_pca = pd.DataFrame(\n    data    = mod_pca.transform(arr_pinturas_standar),\n    columns = [\"X\", \"Y\"]\n)\n\nEste código utiliza la librería scikit-learn para realizar un análisis de Componentes Principales (PCA) sobre el DataFrame tb_pinturas_caract y crea un nuevo DataFrame llamado tb_pca. A continuación, se presenta la explicación paso a paso:\nExplicación paso a paso del código:\n\nEstandarizar los Datos:\nSe utiliza StandardScaler para estandarizar el DataFrame tb_pinturas_caract. Esto significa que cada característica (columna) se ajusta para tener una media de cero y una desviación estándar de uno. El método fit_transform realiza el ajuste y la transformación en una sola llamada.\nCrear y Ajustar el Modelo PCA:\nSe crea una instancia de la clase PCA con la especificación de retener dos componentes principales (n_components=2). Luego, se ajusta el modelo a los datos estandarizados utilizando el método fit.\nTransformar los Datos con PCA:\nSe utiliza el modelo PCA entrenado para transformar los datos estandarizados. Esto significa proyectar los datos originales en el espacio de las dos primeras componentes principales.\nCrear el DataFrame tb_pca:\nSe crea un nuevo DataFrame llamado tb_pca utilizando las componentes principales obtenidas. Este DataFrame tiene dos columnas, “X” y “Y”, que representan las dos dimensiones principales del espacio transformado por PCA.\n\n\n\n\nAhora tenemos un nuevo dataframe tb_pca en el que guardamos las coordenadas de nuestras pinturas. Vamos a visualizarlas:\n\nRPython\n\n\n\ntb_pca %&gt;% \n  ggplot +\n  aes(x = X, y  = Y) +\n  geom_point()\n\n\n\n\n\n\n\n\nEste código genera un gráfico de dispersión utilizando las coordenadas “X” y “Y” del DataFrame tb_pca. Cada punto en el gráfico representa una observación en el espacio de las dos primeras componentes principales obtenidas a través del análisis de Componentes Principales (PCA). Explicación paso a paso del código:\n\nOperador %&gt;% (pipe):\n\nEl operador %&gt;% se utiliza para encadenar las operaciones, pasando el resultado de una operación como entrada a la siguiente.\n\nggplot:\n\nSe inicia la construcción de un gráfico utilizando la librería ggplot2.\n\naes(x = X, y = Y):\n\nSe especifica que las coordenadas “X” y “Y” del DataFrame tb_pca se utilizarán como ejes x e y, respectivamente.\n\ngeom_point():\n\nAgrega una capa de puntos al gráfico, creando un gráfico de dispersión.\n\n\n\n\n\nsns.scatterplot(x = \"X\", y = \"Y\", data = tb_pca)\n\n\n\n\n\n\n\n\nEl gráfico de dispersión representa cada punto como una observación del DataFrame tb_pca. Las coordenadas “X” y “Y” se utilizan como posiciones en los ejes x e y, proporcionando una visualización de las dos primeras componentes principales obtenidas a través del análisis de Componentes Principales (PCA) utilizando la librería Seaborn. Explicación paso a paso del código:\n\nsns.scatterplot: Se utiliza la función sns.scatterplot para crear un gráfico de dispersión.\nx=\"X\", y=\"Y\" especifica que las coordenadas “X” y “Y” del DataFrame tb_pca se utilizarán como ejes x e y, respectivamente.\ndata=tb_pca indica que los datos provienen de este DataFrame.\n\n\n\n\nTeniendo los resultados del análisis previo, podemos usar el dataframe tb_grupos en esta visualización para revisar los grupos. Es necesario cambiar la variable indicadora del grupo a texto, de lo contrario no vamos a tener una buena visualización.\n\nRPython\n\n\n\ntb_pca %&gt;%\n  bind_cols(tb_grupos) %&gt;% \n  mutate(\n    # en esta línea podemos cambiar los grupos\n    grupo = as.character(grupos_jerar)\n    ) %&gt;% \n  ggplot +\n  aes(x = X, y  = Y, col = grupo) +\n  geom_point() \n\n\n\n\n\n\n\n\nSe obtiene un gráfico de dispersión donde cada punto representa una observación del DataFrame resultante de combinar tb_pca y tb_grupos. Los puntos se colorean según la columna “grupo”. Este gráfico proporciona una visualización de las dos primeras componentes principales junto con la información de grupos proporcionada por tb_grupos. Explicación paso a paso del código:\n\nUtilizar bind_cols para combinar DataFrames:\n\ntb_pca %&gt;% bind_cols(tb_grupos): Utiliza el operador %&gt;% para encadenar operaciones. bind_cols combina horizontalmente (cbind en R) los DataFrames tb_pca y tb_grupos.\n\nAgregar una Columna de Grupos:\n\nmutate(...): Utiliza mutate para agregar o modificar columnas en el DataFrame resultante de la combinación.\ngrupo = as.character(grupos_jerar): Agrega una nueva columna llamada “grupo”, que se obtiene de la columna grupos_jerar. Se convierte a tipo de dato caracter (as.character) para asegurar que sea tratado como categoría.\n\nCrear un Gráfico de Dispersión con Colores por Grupo:\n\nggplot: Inicia la construcción de un gráfico utilizando la librería ggplot2.\naes(x = X, y = Y, col = grupo): Especifica que las coordenadas “X” y “Y” del DataFrame se utilizarán como ejes x e y, respectivamente, y se asignarán colores según la columna “grupo”.\n\nAgregar Puntos al Gráfico:\n\ngeom_point(): Agrega una capa de puntos al gráfico, creando un gráfico de dispersión.\n\n\n\n\n\ntb_tmp = pd.concat([tb_pca, tb_grupos], axis = 1)\n\n# en esta línea podemos cambiar los grupos\ntb_tmp[\"grupo\"] = tb_tmp[\"grupos_jerar\"].astype(\"string\")\n\nsns.scatterplot(x = \"X\", y = \"Y\", hue = \"grupo\", data = tb_tmp)\n\n\n\n\n\n\n\n\nSe obtiene un gráfico de dispersión donde cada punto representa una observación del DataFrame tb_tmp, con las coordenadas “X” y “Y” como posiciones en los ejes x e y, respectivamente. Los puntos se colorearán según la información de la columna “grupo”, proporcionando una visualización de las dos primeras componentes principales junto con la información de grupos proveniente de tb_grupos. Explicación paso a paso del código:\n\nConcatenar DataFrames Horizontalmente:\n\ntb_tmp = pd.concat([tb_pca, tb_grupos], axis=1): Se utiliza la función concat de Pandas para concatenar horizontalmente (axis=1) los DataFrames tb_pca y tb_grupos. El resultado es almacenado en el DataFrame tb_tmp.\n\nAgregar una Columna de Grupos al DataFrame Resultante:\n\ntb_tmp[\"grupo\"] = tb_tmp[\"grupos_jerar\"].astype(\"string\"): Se agrega una nueva columna llamada “grupo” al DataFrame tb_tmp. Esta columna se obtiene de la columna “grupos_jerar” y se convierte al tipo de dato string (astype(\"string\")).\n\nCrear un Gráfico de Dispersión con Seaborn:\n\nsns.scatterplot(x=\"X\", y=\"Y\", hue=\"grupo\", data=tb_tmp): Se utiliza la función scatterplot de Seaborn para crear un gráfico de dispersión.\n\nx=\"X\", y=\"Y\" especifica que las coordenadas “X” y “Y” del DataFrame tb_tmp se utilizarán como ejes x e y, respectivamente.\nhue=\"grupo\" asigna colores según la columna “grupo”, lo que proporciona información de los grupos en el gráfico.\n\n\n\n\n\n\nA continuación, hacemos una gráfica de las pinturas usando las pinturas. De esta forma vamos a poder revisar cuáles pinturas resultan cercanas y a qué regiones pertenecen. Estas gráficas son computacionalmente exigentes; es recomendable guardarlas (no mostrarlas de inmediato), a menos que estemos trabajando en un computador muy potente.\n\nRPython\n\n\n\ntb_pca %&gt;%\n  mutate(\n    archivos_pinturas = file.path(mi_setup$carpeta_pinturas, dir(mi_setup$carpeta_pinturas))\n  ) %&gt;% ggplot() +\n  geom_image(aes(x = X, y = Y, image = archivos_pinturas), size = 0.03) + \n  theme_void() -&gt; gr_tmp # Es mejor guardar la imagen que abrirla en la sesión\n\nggsave(mi_setup$gr_acp_file, gr_tmp, width = 200, height = 200, units = \"mm\")\n\n\nSe elabora un gráfico de dispersión utilizando las imágenes de las pinturas en vez de puntos. Explicación paso a paso del código:\n\nModificar el DataFrame con mutate:\n\nSe utiliza mutate para agregar una nueva columna llamada “archivos_pinturas” al DataFrame tb_pca. Esta columna contiene la ruta completa de los archivos de pinturas ubicados en la carpeta especificada en mi_setup$carpeta_pinturas.\nLa estructura de tuberías (%&gt;%) se utiliza para encadenar las operaciones, pasando el resultado de una a la siguiente.\n\nCrear un Gráfico con Imágenes:\n\nggplot() + geom_image(...): Se inicia la construcción de un gráfico utilizando la librería ggplot2. Se utiliza geom_image para agregar imágenes al gráfico, utilizando las coordenadas “X” y “Y” del DataFrame tb_pca y la ruta de las imágenes en la columna “archivos_pinturas”.\n\nEstablecer un Tema Visual:\n\ntheme_void(): Aplica un tema visual que elimina elementos como ejes y fondos, dejando solo las imágenes.\n\nGuardar el Gráfico en una Variable:\n\n-&gt; gr_tmp: Se utiliza -&gt; para asignar el gráfico resultante a la variable gr_tmp.\n\nGuardar el Gráfico como Archivo:\n\nggsave(...): Se utiliza ggsave para guardar el gráfico en un archivo. La ruta y el nombre del archivo se especifican en mi_setup$gr_acp_file. Se ajustan también las dimensiones del archivo y la unidad de medida.\n\n\n\n\n\ndef getImage(path):\n    return OffsetImage(plt.imread(path), zoom=.1, alpha = 1)\n\nnombres_archivos_pinturas = os.listdir(mi_setup[\"carpeta_pinturas\"])\n\nnombres_archivos_pinturas.sort()\n\ntb_pca[\"archivos_pinturas\"] = list(map(\n  lambda dir: os.path.join(mi_setup[\"carpeta_pinturas\"],dir),\n  nombres_archivos_pinturas\n  ))\n\nfig, ax = plt.subplots(figsize=(25, 25))\nax.scatter(tb_pca[\"X\"], tb_pca[\"Y\"], color=\"white\")\n\nfor index, row in tb_pca.iterrows():\n  ab = AnnotationBbox(getImage(row[\"archivos_pinturas\"]), (row[\"X\"], row[\"Y\"]), frameon=False)\n  ax.add_artist(ab)\n\n\nplt.savefig(mi_setup[\"gr_acp_file\"], dpi = 100)\n\n\nSe elabora un gráfico de dispersión utilizando las imágenes de las pinturas en vez de puntos. Explicación paso a paso del código:\n\nDefinir la Función getImage:\n\nSe define una función llamada getImage que toma una ruta de archivo como argumento y devuelve un objeto OffsetImage que contiene la imagen con ciertas propiedades de zoom y transparencia.\n\nObtener Nombres de Archivos de Pinturas:\n\nSe obtienen los nombres de los archivos de pinturas en la carpeta especificada en mi_setup[\"carpeta_pinturas\"] y se almacenan en la lista nombres_archivos_pinturas. Esta lista se ordena alfabéticamente.\n\nAsignar Rutas Completas a las Imágenes en el DataFrame:\n\nSe utiliza map junto con lambda para crear una nueva columna en el DataFrame tb_pca llamada “archivos_pinturas”, que contiene las rutas completas de las imágenes de pinturas.\n\nCrear un Gráfico de Dispersión Vacío:\n\nSe crea un gráfico de dispersión vacío utilizando plt.subplots, con un tamaño de figura de 25x25.\n\nAgregar Puntos Blancos al Gráfico:\n\nSe agrega una capa de puntos blancos al gráfico de dispersión en las coordenadas especificadas por las columnas “X” y “Y” del DataFrame tb_pca.\n\nAgregar Imágenes al Gráfico:\n\nSe utiliza un bucle for para iterar sobre las filas de tb_pca. Para cada fila, se crea un objeto AnnotationBbox que contiene la imagen de la pintura y se agrega al gráfico en las coordenadas correspondientes.\n\nGuardar el Gráfico como Archivo:\n\nSe guarda el gráfico como un archivo de imagen en la ruta especificada en mi_setup[\"gr_acp_file\"], con una resolución de 100 dpi.\n\n\n\n\n\nMuy bien, es hora de pasar al t-SNE. Queremos hacer las mismas gráficas pero esta vez con el algoritmo t-SNE y observar los cambios."
  },
  {
    "objectID": "unidad_02.html#t-sne",
    "href": "unidad_02.html#t-sne",
    "title": "Proyecto: The joy of programming",
    "section": "t-SNE",
    "text": "t-SNE\nEn primer lugar realizamos la reducción a 2 dimensiones aplicando el algoritmo t-SNE a nuestro dataset de características tb_pinturas_caract. En este ejemplo, utilizamos un perplexity = 20, pero podríamos utilizar cualquier otro valor entre 1 y 50.\n\nRPython\n\n\n\ntsne(\n  dist(tb_pinturas_caract),\n  perplexity = 20, \n  k = 2, \n  initial_dims = ncol(tb_pinturas_caract)\n) -&gt; mt_tsne_resultado\n\nmt_tsne_resultado %&gt;% \n  as_tibble(.name_repair = \"minimal\") %&gt;% \n  setNames(c(\"X\", \"Y\")) -&gt; tb_tsne\n\nExplicación paso a paso del código:\n\nAplicar el Método t-SNE:\n\nSe utiliza la función tsne para aplicar el método t-SNE a la matriz de distancias de las características de las pinturas contenidas en tb_pinturas_caract. Se especifican parámetros como la perplexidad, el número de dimensiones, y las dimensiones iniciales.\n\nAsignar el Resultado a una Variable:\n\nEl resultado de la aplicación de t-SNE se asigna a la variable mt_tsne_resultado.\n\nTransformar el Resultado a un DataFrame Tibble:\n\nSe utiliza %&gt;% para encadenar operaciones. El resultado de t-SNE se transforma a un DataFrame tibble y se renombran las columnas como “X” y “Y”. El resultado se almacena en el DataFrame tb_tsne.\n\n\n\n\n\nmod_tsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=20)\n\nmt_tsne_resultado = mod_tsne.fit_transform(tb_pinturas_caract)\n\ntb_tsne = pd.DataFrame(\n    data    = mt_tsne_resultado,\n    columns = [\"X\", \"Y\"]\n)\n\nExplicación paso a paso del código:\n\nConfiguración del Modelo t-SNE:\n\nSe instancia un modelo t-SNE utilizando la clase TSNE del paquete sklearn. Se especifican parámetros como el número de componentes, la tasa de aprendizaje, el método de inicialización y la perplexidad.\n\nAjuste del Modelo y Transformación de los Datos:\n\nSe ajusta el modelo t-SNE a las características de las pinturas contenidas en tb_pinturas_caract utilizando el método fit_transform. Esto realiza el proceso de reducción de dimensionalidad y devuelve las coordenadas en el espacio de baja dimensión.\n\nCreación de un DataFrame con los Resultados:\n\nSe crea un DataFrame llamado tb_tsne con las coordenadas resultantes del t-SNE, asignando nombres a las columnas como “X” y “Y”.\n\n\n\n\n\nAhora tenemos un nuevo dataframe tb_tsne en el que guardamos las coordenadas de nuestras pinturas. Vamos a visualizarlas:\n\nRPython\n\n\n\ntb_tsne %&gt;% \n  ggplot +\n  aes(x = X, y  = Y) +\n  geom_point() \n\n\n\n\n\n\n\n\nEste código genera un gráfico de dispersión utilizando las coordenadas “X” y “Y” del DataFrame tb_tsne. Cada punto en el gráfico representa una observación en el espacio bidimensional obtenido a través del algoritmo t-SNE. Explicación paso a paso del código:\n\nOperador %&gt;% (pipe):\n\nEl operador %&gt;% se utiliza para encadenar las operaciones, pasando el resultado de una operación como entrada a la siguiente.\n\nggplot:\n\nSe inicia la construcción de un gráfico utilizando la librería ggplot2.\n\naes(x = X, y = Y):\n\nSe especifica que las coordenadas “X” y “Y” del DataFrame tb_tsne se utilizan como ejes x e y, respectivamente.\n\ngeom_point():\n\nAgrega una capa de puntos al gráfico, creando un gráfico de dispersión.\n\n\n\n\n\nsns.scatterplot(x = \"X\", y = \"Y\", data = tb_tsne)\n\n\n\n\n\n\n\n\nEl gráfico de dispersión representa cada punto como una observación del DataFrame tb_tsne. Las coordenadas “X” y “Y” se utilizan como posiciones en los ejes x e y, proporcionando una visualización del espacio bidimensional obtenido a través del algoritmo t-SNE, utilizando la librería Seaborn. Explicación paso a paso del código:\n\nsns.scatterplot: Se utiliza la función sns.scatterplot para crear un gráfico de dispersión.\nx=\"X\", y=\"Y\" especifica que las coordenadas “X” y “Y” del DataFrame tb_tsne se utilizan como ejes x e y, respectivamente.\ndata=tb_tsne indica que los datos provienen de este DataFrame.\n\n\n\n\nTeniendo los resultados del análisis previo, podemos usar el dataframe tb_grupos en esta visualización para revisar los grupos. Es necesario cambiar la variable indicadora del grupo a texto, de lo contrario no vamos a tener una buena visualización.\n\nRPython\n\n\n\ntb_tsne %&gt;%\n  bind_cols(tb_grupos) %&gt;% \n  mutate(\n    # en esta línea podemos cambiar los grupos\n    grupo = as.character(grupos_jerar)\n    ) %&gt;% \n  ggplot +\n  aes(x = X, y  = Y, col = grupo) +\n  geom_point() \n\n\n\n\n\n\n\n\nSe obtiene un gráfico de dispersión donde cada punto representa una observación del DataFrame resultante de combinar tb_tsne y tb_grupos. Los puntos se colorean según la columna “grupo”. Este gráfico proporciona una visualización del espacio bidimensional obtenido a través del algoritmo t-SNE junto con la información de grupos proporcionada por tb_grupos. Explicación paso a paso del código:\n\nUtilizar bind_cols para combinar DataFrames:\n\ntb_tsne %&gt;% bind_cols(tb_grupos): Utiliza el operador %&gt;% para encadenar operaciones. bind_cols combina horizontalmente (cbind en R) los DataFrames tb_tsne y tb_grupos.\n\nAgregar una Columna de Grupos:\n\nmutate(...): Utiliza mutate para agregar o modificar columnas en el DataFrame resultante de la combinación.\ngrupo = as.character(grupos_jerar): Agrega una nueva columna llamada “grupo”, que se obtiene de la columna grupos_jerar. Se convierte a tipo de dato caracter (as.character) para asegurar que sea tratado como categoría.\n\nCrear un Gráfico de Dispersión con Colores por Grupo:\n\nggplot: Inicia la construcción de un gráfico utilizando la librería ggplot2.\naes(x = X, y = Y, col = grupo): Especifica que las coordenadas “X” y “Y” del DataFrame se utilizarán como ejes x e y, respectivamente, y se asignarán colores según la columna “grupo”.\n\nAgregar Puntos al Gráfico:\n\ngeom_point(): Agrega una capa de puntos al gráfico, creando un gráfico de dispersión.\n\n\n\n\n\ntb_tmp = pd.concat([tb_tsne, tb_grupos], axis = 1)\n\n# en esta línea podemos cambiar los grupos\ntb_tmp[\"grupo\"] = tb_tmp[\"grupos_jerar\"].astype(\"string\")\n\nsns.scatterplot(x = \"X\", y = \"Y\", hue = \"grupo\", data = tb_tmp)\n\n\n\n\n\n\n\n\nSe obtiene un gráfico de dispersión donde cada punto representa una observación del DataFrame tb_tmp, con las coordenadas “X” y “Y” como posiciones en los ejes x e y, respectivamente. Los puntos se colorean según la información de la columna “grupo”, proporcionando una visualización del espacio bidimensional obtenido a través del algoritmo t-SNE junto con la información de grupos proveniente de tb_grupos. Explicación paso a paso del código:\n\nConcatenar DataFrames Horizontalmente:\n\ntb_tmp = pd.concat([tb_tsne, tb_grupos], axis=1): Se utiliza la función concat de la librería pandas para concatenar horizontalmente (axis=1) los DataFrames tb_tsne y tb_grupos. El resultado es almacenado en el DataFrame tb_tmp.\n\nAgregar una Columna de Grupos al DataFrame Resultante:\n\ntb_tmp[\"grupo\"] = tb_tmp[\"grupos_jerar\"].astype(\"string\"): Se agrega una nueva columna llamada “grupo” al DataFrame tb_tmp. Esta columna se obtiene de la columna “grupos_jerar” y se convierte al tipo de dato string (.astype(\"string\")).\n\nCrear un Gráfico de Dispersión con Seaborn:\n\nsns.scatterplot(x=\"X\", y=\"Y\", hue=\"grupo\", data=tb_tmp): Se utiliza la función scatterplot de Seaborn para crear un gráfico de dispersión.\n\nx=\"X\", y=\"Y\" especifica que las coordenadas “X” y “Y” del DataFrame tb_tmp se utilizarán como ejes x e y, respectivamente.\nhue=\"grupo\" asigna colores según la columna “grupo”, lo que proporciona información de los grupos en el gráfico.\n\n\n\n\n\n\nA continuación, hacemos una gráfica de las pinturas usando las pinturas. De esta forma vamos a poder revisar cuáles pinturas resultan cercanas y a qué regiones pertenecen. Estas gráficas son computacionalmente exigentes; es recomendable guardarlas (no mostrarlas de inmediato), a menos que estemos trabajando en un computador muy potente.\n\nRPython\n\n\n\ntb_tsne %&gt;%\n  mutate(\n    archivos_pinturas = file.path(mi_setup$carpeta_pinturas, dir(mi_setup$carpeta_pinturas))\n  ) %&gt;% ggplot() +\n  geom_image(aes(x = X, y = Y, image = archivos_pinturas), size = 0.03) + \n  theme_void() -&gt; gr_tmp # Es mejor guardar la imagen que abrirla en la sesión\n\nggsave(mi_setup$gr_tsne_file, gr_tmp, width = 200, height = 200, units = \"mm\")\n\n\nSe elabora un gráfico de dispersión utilizando las imágenes de las pinturas en vez de puntos. Explicación paso a paso del código:\n\nModificar el DataFrame con mutate:\n\nSe utiliza mutate para agregar una nueva columna llamada “archivos_pinturas” al DataFrame tb_tsne. Esta columna contiene la ruta completa de los archivos de pinturas ubicados en la carpeta especificada en mi_setup$carpeta_pinturas.\nLa estructura de tuberías (%&gt;%) se utiliza para encadenar las operaciones, pasando el resultado de una a la siguiente.\n\nCrear un Gráfico con Imágenes:\n\nggplot() + geom_image(...): Se inicia la construcción de un gráfico utilizando la librería ggplot2. Se utiliza geom_image para agregar imágenes al gráfico, utilizando las coordenadas “X” y “Y” del DataFrame tb_tsne y la ruta de las imágenes en la columna “archivos_pinturas”.\n\nEstablecer un Tema Visual:\n\ntheme_void(): Aplica un tema visual que elimina elementos como ejes y fondos, dejando solo las imágenes.\n\nGuardar el Gráfico en una Variable:\n\n-&gt; gr_tmp: Se utiliza -&gt; para asignar el gráfico resultante a la variable gr_tmp.\n\nGuardar el Gráfico como Archivo:\n\nggsave(...): Se utiliza ggsave para guardar el gráfico en un archivo. La ruta y el nombre del archivo se especifican en mi_setup$gr_tsne_file. Se ajustan también las dimensiones del archivo y la unidad de medida.\n\n\n\n\n\n# En caso de que no hayamos corrido el código previo:\n# def getImage(path):\n#     return OffsetImage(plt.imread(path), zoom=.1, alpha = 1)\n# \n# nombres_archivos_pinturas = os.listdir(mi_setup[\"carpeta_pinturas\"])\n# \n# nombres_archivos_pinturas.sort()\n\ntb_tsne[\"archivos_pinturas\"] = list(map(\n  lambda dir: os.path.join(mi_setup[\"carpeta_pinturas\"],dir),\n  nombres_archivos_pinturas\n  ))\n\nfig, ax = plt.subplots(figsize=(25, 25))\nax.scatter(tb_tsne[\"X\"], tb_tsne[\"Y\"], color=\"white\")\n\nfor index, row in tb_tsne.iterrows():\n  ab = AnnotationBbox(getImage(row[\"archivos_pinturas\"]), (row[\"X\"], row[\"Y\"]), frameon=False)\n  ax.add_artist(ab)\n\nplt.savefig(mi_setup[\"gr_tsne_file\"], dpi = 100)\n\n\nSe elabora un gráfico de dispersión utilizando las imágenes de las pinturas en vez de puntos. Explicación paso a paso del código:\n\nDefinir la Función getImage:\n\nSe define una función llamada getImage que toma una ruta de archivo como argumento y devuelve un objeto OffsetImage que contiene la imagen con ciertas propiedades de zoom y transparencia.\n\nObtener Nombres de Archivos de Pinturas:\n\nSe obtienen los nombres de los archivos de pinturas en la carpeta especificada en mi_setup[\"carpeta_pinturas\"] y se almacenan en la lista nombres_archivos_pinturas. Esta lista se ordena alfabéticamente.\n\nAsignar Rutas Completas a las Imágenes en el DataFrame:\n\nSe utiliza map junto con lambda para crear una nueva columna en el DataFrame tb_tsne llamada “archivos_pinturas”, que contiene las rutas completas de las imágenes de pinturas.\n\nCrear un Gráfico de Dispersión Vacío:\n\nSe crea un gráfico de dispersión vacío utilizando plt.subplots, con un tamaño de figura de 25x25.\n\nAgregar Puntos Blancos al Gráfico:\n\nSe agrega una capa de puntos blancos al gráfico de dispersión en las coordenadas especificadas por las columnas “X” y “Y” del DataFrame tb_tsne.\n\nAgregar Imágenes al Gráfico:\n\nSe utiliza un bucle for para iterar sobre las filas de tb_tsne. Para cada fila, se crea un objeto AnnotationBbox que contiene la imagen de la pintura y se agrega al gráfico en las coordenadas correspondientes.\n\nGuardar el Gráfico como Archivo:\n\nSe guarda el gráfico como un archivo de imagen en la ruta especificada en mi_setup[\"gr_tsne_file\"], con una resolución de 100 dpi."
  },
  {
    "objectID": "unidad_03.html",
    "href": "unidad_03.html",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "En esta etapa se aplican los métodos de evaluación de la calidad de los agrupamientos que han sido presentados en el curso para evaluar la calidad de los grupos resultantes de la primera etapa. Al igual que las etapas anteriores, la implementación de los métodos de evaluación se realiza en lenguajes para el manejo de datos R y Python.\nLas bases teóricas para la comprensión de los métodos presentados se encuentran en el material del curso. Los métodos que se aplican a continuación son: evaluación a partir de información externa, método del codo, validación cruzada y coeficiente silueta.\n\n\nLa tercera etapa del proyecto está orientada a cumplir el tercer objetivo específico:\n\nEvaluar las tendencias identificadas en los datos de las pinturas y seleccionar el mejor resultado. Caracterizarlo e interpretarlo."
  },
  {
    "objectID": "unidad_03.html#objetivo-actual",
    "href": "unidad_03.html#objetivo-actual",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "La tercera etapa del proyecto está orientada a cumplir el tercer objetivo específico:\n\nEvaluar las tendencias identificadas en los datos de las pinturas y seleccionar el mejor resultado. Caracterizarlo e interpretarlo."
  },
  {
    "objectID": "unidad_03.html#evaluación-a-partir-de-información-externa",
    "href": "unidad_03.html#evaluación-a-partir-de-información-externa",
    "title": "Proyecto: The joy of programming",
    "section": "Evaluación a partir de información externa",
    "text": "Evaluación a partir de información externa\nPara nuestro dataset de pinturas no tenemos el caso de información externa; pero podemos simular un escenario muy parecido. En primer lugar debemos programar la función ps, que nos permite obtener la fuerza de predicción de un sistema de grupos estimados sobre un sistema de grupos verdaderos. El código es el siguiente:\n\nRPython\n\n\n\nps &lt;- function(true_groups, estim_groups){\n  \n  sprintf(\"Id_%04d\", seq_along(true_groups)) -&gt; id_data\n  \n  setNames(\n    true_groups,\n    id_data\n  ) -&gt; vc_true\n  \n  setNames(\n    estim_groups,\n    id_data\n  ) -&gt; vc_estim\n  \nexpand_grid(id_1 = id_data, id_2 = id_data) %&gt;% \n  mutate(\n    true_1 = vc_true[id_1],\n    true_2 = vc_true[id_2],\n    paired_true = as.numeric(true_1 == true_2),\n    estim_1 = vc_estim[id_1],\n    estim_2 = vc_estim[id_2],\n    paired_estim = as.numeric(estim_1 == estim_2),\n    no_perm = as.numeric(paired_true &gt; paired_estim)\n  ) %&gt;% \n  group_by(true_1) %&gt;% \n  summarise(\n    paired_true = sum(paired_true),\n    no_perm = sum(no_perm)\n  ) %&gt;% \n  filter(paired_true != 1) %&gt;% \n  mutate(\n    ps = 1 - no_perm/(paired_true - sqrt(paired_true))\n  ) %&gt;% pull(ps) %&gt;% min -&gt; PS\n  \n  PS\n\n}\n\nEsta función calcula la fuerza de predicción (PS) basada en la comparación de grupos verdaderos y estimados utilizando permutaciones. La lógica exacta detrás del cálculo de la fuerza de predicción reside en la manipulación de datos y las transformaciones realizadas en el dataframe generado por expand_grid.\n\nGeneración de identificadores únicos (id_data):\n\nSe crea una lista de identificadores únicos formateados como “Id_XXXX”, donde XXXX representa un número secuencial de cuatro dígitos. La longitud de la lista está determinada por la cantidad de elementos en true_groups.\nsprintf(\"Id_%04d\", seq_along(true_groups)) -&gt; id_data: Crea un vector de identificación (id_data) utilizando la función sprintf para formatear números como cadenas de texto con un patrón específico (\"Id_%04d\") agregando ceros a la izquierda a la secuencia seq_along(true_groups) de números que van desde 1 hasta la longitud de true_groups.\n\nCreación de diccionarios para mapeo (vc_true y vc_estim):\n\nSe crean diccionarios que mapean los identificadores a los valores correspondientes en los grupos verdaderos (vc_true) y estimados (vc_estim).\nsetNames(true_groups, id_data) -&gt; vc_true: Asigna nombres al vector true_groups utilizando los identificadores en id_data. El resultado se guarda en vc_true.\nsetNames(estim_groups, id_data) -&gt; vc_estim: Similar al paso anterior, asigna nombres al vector estim_groups utilizando los identificadores en id_data. El resultado se guarda en vc_estim.\n\nGeneración de dataframe con todas las combinaciones posibles:\n\nSe generan todas las combinaciones posibles de pares de identificadores en id_data. Estos pares se convierten en columnas ‘id_1’ y ‘id_2’.\nexpand_grid(id_1 = id_data, id_2 = id_data) %&gt;% ...: Utiliza la función expand_grid para crear un dataframe con todas las combinaciones posibles de identificadores de id_data. Este dataframe contiene las columnas id_1 e id_2, que son todas las parejas posibles de observaciones. El operador %&gt;% (pipe) da continuidad a la aplicación de operaciones y transformaciones.\n\nMapeo de valores verdaderos y estimados:\n\nSe agregan columnas al dataFrame df que contienen los valores correspondientes a ‘id_1’ y ‘id_2’ para ambos grupos verdaderos y estimados.\nmutate(...): Agrega nuevas columnas al dataframe.\nSe crean las columnas true_1 y true_2, que muestran en qué grupo, de los grupos verdaderos, se encuentran las observaciones correspondientes a las columnas id_1 e id_2 y la columna paired_true que indica que la pareja correspondiente a id_1 e id_2 se encuentra en el mismo grupo verdadero.\nAnálogamente, se crean las columnas estim_1y estim_2, que muestran en qué grupo, de los grupos estimados, se encuentran las observaciones correspondientes a las columnas id_1 e id_2 y la columna paired_estimque indica que la pareja correspondiente a id_1 e id_2 se encuentra en el mismo grupo estimado.\n\nCálculo de no_perm:\n\nSe compara el número de parejas coincidentes entre paired_true y paired_estim, y se almacena en la columna no_perm. Luego, se agrupa el DataFrame por los valores verdaderos (‘true_1’) y se realiza un resumen.\nLa columna no_perm muestra los casos en los que una pareja en el mismo grupo verdadero (paired_true = 1) no se encuentra en el mismo grupo estimado (paired_estim = 0); es decir, es una pareja que no es permanente.\ngroup_by(true_1) %&gt;% summarise(...): Agrupa los datos por la columna true_1 y calcula la suma de paired_true y no_perm para cada grupo. Con esto se tiene la cantidad de parejas en los grupos reales (paired_true) y la cantidad de parejas que no son permanentes para cada cluster verdadero.\n\nCálculo de la fuerza de predicción (ps):\n\nSe calcula la fuerza de predicción (ps) utilizando la fórmula específica basada en los valores calculados en el paso anterior.\nfilter(paired_true != 1) %&gt;% mutate(...): Retira los grupos verdaderos de un solo elemento; son aquellos donde paired_true es igual a 1.\nps = 1 - no_perm/(paired_true - sqrt(paired_true)) calcula la fuerza de predicción para cada grupo verdadero.\n\nDevolver el valor mínimo de ps (PS):\n\nLa función devuelve el valor mínimo de la fuerza de predicción calculada.\npull(ps) %&gt;% min -&gt; PS: Extrae la columna ps y calcula el mínimo. El resultado se asigna a la variable PS.\nPS: Devuelve el valor calculado de PS.\n\n\n\n\n\ndef ps(true_groups, estim_groups):\n  id_data = [f\"Id_{i:04d}\" for i in range(len(true_groups))]\n  \n  vc_true = dict(zip(id_data, true_groups))\n  vc_estim = dict(zip(id_data, estim_groups))\n  \n  df = pd.DataFrame(list(product(id_data, repeat=2)), columns=['id_1', 'id_2'])\n  df['true_1'] = df['id_1'].map(vc_true)\n  df['true_2'] = df['id_2'].map(vc_true)\n  df['paired_true'] = np.where(df['true_1'] == df['true_2'], 1, 0)\n  df['estim_1'] = df['id_1'].map(vc_estim)\n  df['estim_2'] = df['id_2'].map(vc_estim)\n  df['paired_estim'] = np.where(df['estim_1'] == df['estim_2'], 1, 0)\n  df['no_perm'] = np.where(df['paired_true'] &gt; df['paired_estim'], 1, 0)\n  \n  result_df = df.groupby('true_1').agg(paired_true=('paired_true', 'sum'),\n  no_perm=('no_perm', 'sum')).reset_index()\n  result_df = result_df[result_df['paired_true'] != 1]\n  result_df['ps'] = 1 - result_df['no_perm'] / (result_df['paired_true'] - np.sqrt(result_df['paired_true']))\n  \n  PS = result_df['ps'].min()\n  \n  return PS\n\nLa función ps realiza los siguientes pasos:\n\nGeneración de identificadores únicos (id_data):\n\nSe crea una lista de identificadores únicos formateados como “Id_XXXX”, donde XXXX representa un número secuencial de cuatro dígitos. La longitud de la lista está determinada por la cantidad de elementos en true_groups.\nrange(len(true_groups)): Genera una secuencia de números desde 0 hasta la longitud de la lista true_groups menos 1. Esto proporciona índices únicos para cada elemento en la lista true_groups.\nf\"Id_{i:04d}\": Utiliza una cadena de formato f-string para crear cadenas de texto con el formato “Id_XXXX”, donde XXXX representa el índice con un relleno de ceros a la izquierda para asegurar que siempre tenga cuatro dígitos.\n[...]: Comprende la comprensión de listas, que es una forma concisa de crear listas. En este caso, se genera una lista que contiene las cadenas de texto resultantes.\n\nCreación de diccionarios para mapeo (vc_true y vc_estim):\n\nSe crean diccionarios que mapean los identificadores a los valores correspondientes en los grupos verdaderos (vc_true) y estimados (vc_estim).\nzip(id_data, true_groups): Combina los elementos de las listas id_data y true_groups en pares de tuplas. Cada tupla contiene un elemento de id_data emparejado con el correspondiente elemento de true_groups.\ndict(...): Convierte las tuplas generadas por zip en un diccionario, donde el primer elemento de cada tupla (de id_data) se convierte en la clave y el segundo elemento (de true_groups) se convierte en el valor.\nvc_true = dict(zip(id_data, true_groups)) crea un diccionario (vc_true) donde cada identificador único en id_data se asigna al valor correspondiente en true_groups.\nvc_estim = dict(zip(id_data, estim_groups)) realiza la misma operación pero para las listas id_data y estim_groups, creando un diccionario (vc_estim) con los valores estimados asociados a los identificadores únicos.\n\nGeneración de dataframe con todas las combinaciones posibles (df):\n\nSe utiliza la función product del módulo itertools para generar todas las combinaciones posibles de pares de identificadores en id_data. Estos pares se convierten en columnas ‘id_1’ y ‘id_2’ en el dataframe df.\nproduct(id_data, repeat=2): Utiliza la función product del módulo itertools para generar todas las combinaciones posibles de pares de elementos en la lista id_data. El parámetro repeat=2 indica que se deben generar combinaciones de longitud 2 (pares).\nlist(...): Convierte el objeto iterable resultante de product en una lista.\npd.DataFrame(...): Crea un dataframe de Pandas a partir de la lista generada. El dataframe tendrá dos columnas llamadas ‘id_1’ y ‘id_2’.\ndf = pd.DataFrame(list(product(id_data, repeat=2)), columns=['id_1', 'id_2']) crea un dataframe donde cada fila contiene todas las combinaciones posibles de pares de identificadores únicos en id_data. Cada par de identificadores se coloca en las columnas ‘id_1’ e ‘id_2’, respectivamente.\n\nMapeo de valores verdaderos y estimados en el dataframe (df):\n\nSe agregan columnas al dataframe df que contienen los valores correspondientes a ‘id_1’ y ‘id_2’ para ambos grupos verdaderos y estimados.\nSe crean las columnas true_1 y true_2, que muestran en qué grupo, de los grupos verdaderos, se encuentran las observaciones correspondientes a las columnas id_1 e id_2 y la columna paired_true que indica que la pareja correspondiente a id_1 e id_2 se encuentra en el mismo grupo verdadero.\nAnálogamente, se crean las columnas estim_1y estim_2, que muestran en qué grupo, de los grupos estimados, se encuentran las observaciones correspondientes a las columnas id_1 e id_2 y la columna paired_estimque indica que la pareja correspondiente a id_1 e id_2 se encuentra en el mismo grupo estimado.\n\nCálculo de no_perm y result_df:\n\nSe compara el número de parejas coincidentes entre paired_true y paired_estim, y se almacena en la columna no_perm. Luego, se agrupa el DataFrame por los valores verdaderos (‘true_1’) y se realiza un resumen.\nLa columna no_perm muestra los casos en los que una pareja en el mismo grupo verdadero (paired_true = 1) no se encuentra en el mismo grupo estimado (paired_estim = 0); es decir, es una pareja que no es permanente.\nresult_df = df.groupby('true_1'): Agrupa los datos por la columna true_1.\n.agg(paired_true=('paired_true', 'sum'), no_perm=('no_perm', 'sum')) calcula la suma de paired_true y no_perm para cada grupo. Con esto se tiene la cantidad de parejas en los grupos reales (paired_true) y la cantidad de parejas que no son permanentes para cada cluster verdadero.\n\nCálculo de la fuerza de predicción (ps):\n\nSe calcula la fuerza de predicción (ps) utilizando la fórmula específica basada en los valores calculados en el paso anterior.\nresult_df[result_df['paired_true'] != 1]: Retira los grupos verdaderos de un solo elemento; son aquellos donde paired_true es igual a 1.\nresult_df['ps'] = 1 - result_df['no_perm'] / (result_df['paired_true'] - np.sqrt(result_df['paired_true'])) calcula la fuerza de predicción para cada grupo verdadero.\n\nDevolver el valor mínimo de ps (PS):\n\nLa función devuelve el valor mínimo de la fuerza de predicción calculada.\nPS = result_df['ps'].min(): Extrae la columna ps y calcula el mínimo. El resultado se asigna a la variable PS.\nreturn PS: Devuelve el valor calculado de PS.\n\n\n\n\n\nYa con nuestra función ps, podemos simular el caso de la información externa. Supongamos que tenemos una columna que distribuye las pinturas en 16 grupos dependiendo de si presentan o no montañas, playas, estructuras o invierno. Para fines prácticos, estas 16 categorías se toman como las categorías verdaderas y se mide la aproximación de los grupos conformados previamente con estas categorías. Para eso utilizamos la función ps de la siguiente manera:\n\nRPython\n\n\n\ntb_pinturas_caract %&gt;% \n  mutate(\n    true_clust = sprintf(\"%d%d%d%d\", MOUNTAIN, BEACH, STRUCTURE, WINTER)\n  ) %&gt;% pull(true_clust) -&gt; true_cluster\n\nps(true_cluster, tb_grupos$grupos_jerar)\n\nps(true_cluster, tb_grupos$grupos_kmeans)\n\nps(true_cluster, tb_grupos$grupos_dbscan)\n\nEl código crea una nueva variable llamada true_clust en el dataframe tb_pinturas_caract, que representa un código de cluster formado por la concatenación de las variables MOUNTAIN, BEACH, STRUCTURE, y WINTER. Luego, se extrae esa variable y se asigna a la variable true_cluster. Esta variable se toma como una agrupación verdadera para probar la evaluación a partir de información externa.\n\ntb_pinturas_caract %&gt;%: Indica que se aplicarán operaciones al objeto tb_pinturas_caract utilizando un pipe (%&gt;%), lo que significa que el resultado de la operación previa se pasa como argumento a la siguiente operación.\nmutate(true_clust = sprintf(\"%d%d%d%d\", MOUNTAIN, BEACH, STRUCTURE, WINTER)): Agrega una nueva columna llamada true_clust al dataframe tb_pinturas_caract. El valor de esta columna se genera utilizando la función sprintf para formatear una cadena de texto. Los valores MOUNTAIN, BEACH, STRUCTURE, y WINTER se concatenan.\n%&gt;% pull(true_clust): Extrae la columna recién creada (true_clust) del dataframe resultante y asigna el resultado a la variable true_cluster. Luego se procede a evaluar los grupos conformados anteriormente contrastándolos con esta columna de grupos verdaderos.\n\n\n\n\ntrue_cluster = tb_pinturas_caract.apply(\n    lambda row: \"{:d}{:d}{:d}{:d}\".format(\n        row['MOUNTAIN'], row['BEACH'], row['STRUCTURE'], row['WINTER']\n    ), axis=1\n).tolist()\n\n\nps(true_cluster, tb_grupos['grupos_jerar'])\nps(true_cluster, tb_grupos['grupos_kmeans'])\nps(true_cluster, tb_grupos['grupos_dbscan'])\n\nEl código crea la lista true_cluster, donde cada elemento de la lista es una cadena de texto que representa un código de cluster formado por la concatenación de los valores de las columnas ‘MOUNTAIN’, ‘BEACH’, ‘STRUCTURE’, y ‘WINTER’ para cada fila en el DataFrame tb_pinturas_caract. Esta lista se toma como una agrupación verdadera para probar la evaluación a partir de información externa.\n\ntb_pinturas_caract.apply(...): Se utiliza la función apply de Pandas para aplicar una función a lo largo de las filas del DataFrame tb_pinturas_caract.\nlambda row: \"{:d}{:d}{:d}{:d}\".format(...): Se define una función lambda que toma una fila (row) del DataFrame y devuelve una cadena de texto formateada. En este caso, la cadena de texto se forma concatenando los valores de las columnas ‘MOUNTAIN’, ‘BEACH’, ‘STRUCTURE’, y ‘WINTER’.\naxis=1: Indica que la función lambda se aplicará a lo largo de las filas.\n.tolist(): Convierte el resultado a una lista. Luego se procede a evaluar los grupos conformados anteriormente contrastándolos con esta columna de grupos verdaderos.\n\n\n\n\n¿Cuánto da cada resultado? ¿Cómo se interpreta? ¿Cuál es el método que mejor se adapta a nuestra variable categórica simulada?"
  },
  {
    "objectID": "unidad_03.html#método-del-codo",
    "href": "unidad_03.html#método-del-codo",
    "title": "Proyecto: The joy of programming",
    "section": "Método del codo",
    "text": "Método del codo\nEl método del codo tiene como objetivo encontrar un número de grupos \\(k\\) óptimo. Por esta razón, suele estar enfocado en la estimación mediante el algoritmo \\(k\\)-means. Veamos la construcción de los tráficos correspondientes.\n\nRPython\n\n\n\nfviz_nbclust(x = tb_pinturas_caract, FUNcluster = kmeans, method = \"wss\", k.max = 10)\n\n\n\n\n\n\n\n\nEl código utiliza la función fviz_nbclust para visualizar diferentes métodos y determinar el número óptimo de clusters en el conjunto de datos tb_pinturas_caract. La métrica evaluada es la suma de los cuadrados dentro de los clusters (WSS), y la visualización puede ayudar a identificar el número óptimo de clusters al observar el codo en la gráfica resultante.\n\nfviz_nbclust(...): Utiliza la función fviz_nbclust del paquete factoextra para determinar el número óptimo de clusters en un conjunto de datos.\nx = tb_pinturas_caract: Se especifica que el conjunto de datos tb_pinturas_caract es el objeto en el que se realizará el análisis de clusters.\nFUNcluster = kmeans: Indica que se utilizará el algoritmo de k-means para realizar el análisis de clusters, utilizando la función kmeans.\nmethod = \"wss\": Selecciona el método para determinar el número óptimo de clusters. En este caso, se utiliza “wss” (Within-Cluster Sum of Squares), que evalúa la varianza dentro de los clusters.\nk.max = 10: Especifica el número máximo de clusters a considerar. En este caso, se evaluará hasta un máximo de 10 clusters.\n\n\n\n\nk_values = range(1, 11)\n\nkmeans_model = KMeans(n_init = 1)\n\nvisualizer = KElbowVisualizer(kmeans_model, k=k_values, metric='distortion', timings=False)\n\nvisualizer = visualizer.fit(tb_pinturas_caract)\n\nvisualizer.show()\n\n\n\n\n\n\n\nplt.close()\n\nEl código utiliza el método del codo para determinar el número óptimo de clusters en el conjunto de datos tb_pinturas_caract utilizando el algoritmo de k-medias. La visualización de la distorsión en función del número de clusters ayuda a identificar el punto en el que la distorsión deja de disminuir rápidamente, lo que puede indicar el número óptimo de clusters para el conjunto de datos.\n\nDefinición de k_values: Se crea una secuencia de valores desde 1 hasta 10 para representar diferentes números de clusters.\nInicialización de kmeans_model: Se crea una instancia del modelo de KMeans (k-medias) con n_init=1, indicando que el algoritmo se ejecutará solo una vez con una inicialización diferente.\nCreación del visualizador visualizer: Se utiliza KElbowVisualizer del paquete yellowbrick para visualizar la métrica de “distorsión” en función del número de clusters. La distorsión mide cuánto se alejan las muestras dentro de un cluster promedio. Se utiliza timings=False para evitar la visualización de tiempos de ejecución.\nAjuste del modelo al conjunto de datos: Se ajusta el modelo de k-medias al conjunto de datos tb_pinturas_caract utilizando el método fit() del visualizador.\nVisualización y cierre del gráfico: Se muestra la visualización generada por el visualizador y se cierra la figura gráfica después de ser visualizada.\n\n\n\n\nEn ambos casos, podemos ver el gráfico del método del codo y establecer un número óptimo de grupos."
  },
  {
    "objectID": "unidad_03.html#validación-cruzada",
    "href": "unidad_03.html#validación-cruzada",
    "title": "Proyecto: The joy of programming",
    "section": "Validación cruzada",
    "text": "Validación cruzada\nLos métodos de validación cruzada se basan en la distribución de dos subconjuntos de datos, para entrenamiento del modelo y para prueba. Siguiendo lo aprendido, en primera instancia debemos escribir una función que realice los siguientes pasos dado un conjunto de datos.\n\nDivida el conjunto de datos en segmentos de entrenamiento y prueba.\nEstime la estructura de grupos usando el procedimiento a evaluar. En este caso lo realizamos usando \\(k\\)-means, pero podemos cambiar el código para que funcione con otros métodos.\nA partir del modelo estimado por medio de los datos de entrenamiento, pronosticar los grupos correspondientes a los individuos del conjunto de datos de prueba. Estas categorías se toman como categorías verdaderas,\nAjustar el modelo con los mismos hiperparámetros al conjunto de datos de prueba. De esta forma se obtiene una nueva estructura de grupos. Estas categorías se toman como categorías estimadas.\nComparar las categorías estimadas con las verdaderas utilizando la función ps definida enteriormente.\n\n\nRPython\n\n\n\nps_cv_k &lt;- function(k, tb_data, cv_ratio = 3/4){\n  \n  tb_data %&gt;% \n    initial_split(prop = cv_ratio) -&gt; split_data\n  \n  split_data %&gt;% \n    training %&gt;% \n    kmeans(k) -&gt; ls_kmeans\n  \n  tb_centers &lt;- ls_kmeans$centers\n  n_centers &lt;- nrow(tb_centers)\n  \n  split_data %&gt;%\n    testing %&gt;% \n    rbind(tb_centers) %&gt;% \n    dist %&gt;% \n    as.matrix %&gt;% \n    extract(seq(n_centers), -seq(n_centers)) %&gt;% t %&gt;% \n    \"-\"() %&gt;% \n    max.col() -&gt; vc_predicted\n  \n  split_data %&gt;% \n    testing %&gt;% \n    kmeans(k) %&gt;%\n    pluck(\"cluster\") -&gt; vc_test_clusters\n  \n  \n  ps(vc_predicted, vc_test_clusters)\n  \n}\n\nPaso 1: tb_data %&gt;% initial_split(prop = cv_ratio) -&gt; split_data. Dividir los datos en conjuntos de entrenamiento y prueba.\nPaso 2: split_data %&gt;% training %&gt;% kmeans(k) -&gt; ls_kmeans Aplicar k-medias (kmeans) al conjunto de entrenamiento.\nPaso 3: tb_centers &lt;- ls_kmeans$centers; n_centers &lt;- nrow(tb_centers) Obtener los centros de los clusters del modelo k-medias\nPaso 4: Determinar el grupo de cada observación en el conjunto de prueba, este se toma como etiqueta verdadera.\n\nsplit_data %&gt;% testing %&gt;% rbind(tb_centers) %&gt;% dist %&gt;%  ... Calcular las distancias de los puntos de prueba a los centros de los clusters.\nextract(seq(n_centers), -seq(n_centers)) %&gt;% t Eliminar las distancias a los propios centros\n\"-\"() %&gt;% max.col() -&gt; vc_predicted Encontrar el índice del centro más cercano para cada punto de prueba.\n\nPaso 5: Aplicar el algoritmo y encontrar nuevos grupos para los datos de prueba, estos se toman como etiquetas a evaluar.\n\nsplit_data %&gt;% testing %&gt;% kmeans(k) Aplicar k-medias al conjunto de prueba\npluck(\"cluster\") -&gt; vc_test_clusters Obtener la asignación de clusters para los puntos de prueba\n\nPaso 6: ps(vc_predicted, vc_test_clusters) Calcular y devolver la medida de validez del clustering (ps).\n\n\n\ndef ps_cv_k(k, tb_data, cv_ratio=3/4):\n\n  train_data, test_data = train_test_split(tb_data, test_size=1-cv_ratio)\n  \n  kmeans_model = KMeans(n_clusters=k, n_init=1)\n  kmeans_model = kmeans_model.fit(train_data)\n  \n  vc_predicted = kmeans_model.predict(test_data).tolist()\n  \n  kmeans_test = KMeans(n_clusters=k, n_init=1)\n  vc_test_clusters = kmeans_test.fit_predict(test_data).tolist()\n  \n  return ps(vc_predicted, vc_test_clusters)\n\nPaso 1: train_data, test_data = train_test_split(tb_data, test_size=1-cv_ratio) Dividir los datos en conjuntos de entrenamiento y prueba\nPaso 2: kmeans_model = KMeans(n_clusters=k, n_init=1); kmeans_model = kmeans_model.fit(train_data) Aplicar k-medias (KMeans) al conjunto de entrenamiento\nPaso 3: vc_predicted = kmeans_model.predict(test_data).tolist() Obtener las asignaciones de cluster para los puntos de prueba\nPaso 4: kmeans_test = KMeans(n_clusters=k, n_init=1); vc_test_clusters = kmeans_test.fit_predict(test_data).tolist() Aplicar k-medias al conjunto de prueba\nPaso 5: return ps(vc_predicted, vc_test_clusters) Calcular y devolver la medida de validez del clustering (ps)\n\n\n\nLuego de tener nuestra función lista, resulta sencillo realizar un estudio de simulación que repita la estimación de la fuerza de predicción un número arbitrario de veces. Con esto podemos establecer la distribución de nuestro estadístico de validación y además obtener su promedio.\n\nRPython\n\n\n\nK_seq &lt;- 2:10\n\nmap(\n  1:50,\n  function(iter){\n    tibble(\n      iter = iter,\n      K = K_seq,\n      PS = map_dbl(\n        K,\n        ps_cv_k,\n        tb_data = tb_pinturas_caract,\n        cv_ratio = 0.6\n      )\n    )\n    \n  }\n) %&gt;% bind_rows() -&gt; tb_sim\n\ntb_sim %&gt;% group_by(K) %&gt;% summarise(PS = mean(PS)) -&gt; tb_sim_mean\n\nggplot() +\n  geom_jitter(aes(K, PS), tb_sim, size = 0.7) + \n  geom_point(aes(K, PS), tb_sim_mean, colour = \"#55aacc\", size = 3) +\n  geom_line(aes(K, PS), tb_sim_mean, colour = \"#55aacc\", size = 1) + \n  scale_x_continuous(breaks = K_seq)\n\n\n\n\n\n\n\n\nEl código realiza 50 iteraciones, en cada iteración evalúa la medida de validez del clustering (PS) para diferentes valores de K utilizando la función ps_cv_k. Luego, se calcula la media de la medida de validez para cada valor de K a lo largo de todas las iteraciones. Finalmente, se visualizan los resultados mediante un gráfico utilizando ggplot2, donde los puntos representan las medidas de validez para cada iteración y el punto y la línea en azul representan la media para cada valor de K. La dispersión de puntos proporciona una idea de la variabilidad de la medida de validez para cada valor de K.\nPaso 1: K_seq &lt;- 2:10 Definir una secuencia de valores para K (número de clusters)\nPaso 2: map(1:50,function(iter){...}) %&gt;% Realizar un mapeo (map) sobre las iteraciones de 1 a 50\nPaso 3: tibble(...) Para cada iteración, calcular la medida de validez del clustering (PS)\nPaso 4: bind_rows() -&gt; tb_sim Unir los resultados de todas las iteraciones en un solo marco de datos\nPaso 5: tb_sim %&gt;% group_by(K) %&gt;% summarise(PS = mean(PS)) -&gt; tb_sim_mean Calcular la media de la medida de validez del clustering para cada valor de K\nPaso 6: ggplot() + ... Crear un gráfico utilizando ggplot2 para visualizar los resultados\n\n\n\nK_seq = range(2, 11)\n\ntb_sim_list = []\n\nfor iter in range(1, 51):\n  ps_values = list(map(\n    lambda k: ps_cv_k(k, tb_data=tb_pinturas_caract, cv_ratio=0.6),\n    K_seq\n    ))\n  iter_data = pd.DataFrame({\n    'iter': [iter] * len(K_seq),\n    'K': K_seq,\n    'PS': ps_values\n    })\n    \n  tb_sim_list.append(iter_data)\n\n\ntb_sim = pd.concat(tb_sim_list, ignore_index=True)\n\ntb_sim_mean = tb_sim.groupby('K')['PS'].mean().reset_index()\n\n\n\nsns.stripplot(\n  x='K', y='PS', \n  data=tb_sim, size=1.3, legend=False, \n  color = \"black\"\n  )\nsns.pointplot(x='K', y='PS', data=tb_sim_mean, color='#55aacc')\nplt.xlabel('Número de clústeres (K)')\nplt.ylabel('Índice de Pureza (PS)')\nplt.xticks(K_seq)\n\n\n\n\n\n\n\n\ncódigo realiza 50 iteraciones, en cada iteración evalúa la medida de validez del clustering (PS) para diferentes valores de K utilizando la función ps_cv_k. Luego, se concatenan los resultados de todas las iteraciones en un solo dataframe (tb_sim). Se calcula la media de la medida de validez para cada valor de K y se visualizan los resultados mediante un gráfico utilizando la biblioteca seaborn. Los puntos en el gráfico representan las medidas de validez para cada iteración, y los puntos y la línea azul representan la media para cada valor de K. La dispersión de puntos proporciona una idea de la variabilidad de la medida de validez para cada valor de K.\nPaso 1: K_seq = range(2, 11) Definir una secuencia de valores para K (número de clústeres)\nPaso 2: tb_sim_list = [] Inicializar una lista para almacenar los resultados de cada iteración\nPaso 3: for iter in range(1, 51): Realizar un bucle sobre las iteraciones de 1 a 50, al interior de este bucle se realizan los pasos 4, 5 y 6.\nPaso 4: ps_values = list(map(lambda k: ps_cv_k(k, tb_data=tb_pinturas_caract, cv_ratio=0.6), K_seq)) Calcular la medida de validez del clustering (PS) para cada valor de K\nPaso 5: iter_data = pd.DataFrame({...}) Crear un DataFrame con los resultados de la iteración actual\nPaso 6: tb_sim_list.append(iter_data) Agregar los resultados a la lista\nPaso 7: tb_sim = pd.concat(tb_sim_list, ignore_index=True) Concatenar los resultados de todas las iteraciones en un solo DataFrame\nPaso 8: tb_sim_mean = tb_sim.groupby('K')['PS'].mean().reset_index() Calcular la media de la medida de validez del clustering para cada valor de K\nPaso 9: sns.... Crear un gráfico utilizando seaborn para visualizar los resultados\n\n\n\nLuego de simular los resultados y obtener la medida promedio de la fuerza de predicción es necesario interpretar. Si la fuerza de predicción es alta (cercana a 1), significa que la estructura de grupos estimados es muy parecida a los verdaderos. Lo que indica que el método recupera aproximadamente los mismos grupos. Si la fuerza de predicción es baja significa que el algoritmo está encontrando distintos grupos al examinar los datos de prueba y al pronosticarlos mediante la estructura estimada usando los datos de entrenamiento. Esto puede suceder por dos motivos. 1. Puede ser que los datos no presenten una esstructura de agrupamiento, o 2. Puede ser que los hiperparámetros del algoritmo, como su número de grupos, o su distancia entre grupos (linkage) o su epsilon, sean incorrectos."
  },
  {
    "objectID": "unidad_03.html#coeficiente-silueta",
    "href": "unidad_03.html#coeficiente-silueta",
    "title": "Proyecto: The joy of programming",
    "section": "Coeficiente Silueta",
    "text": "Coeficiente Silueta\nEl coeficiente silueta es una métrica de ajuste de un algoritmo de agrupación. También puede ser usado para la selección del número de grupos \\(k\\) en una estimación de \\(k\\)-means. El código es el siguiente.\n\nRPython\n\n\n\nfviz_nbclust(\n  x = tb_pinturas_caract, \n  FUNcluster = kmeans, \n  method = \"silhouette\",\n  k.max = 15\n  ) \n\n\n\n\n\n\n\n\nEl código utiliza la función fviz_nbclust para visualizar determinar el número óptimo de clusters en el conjunto de datos tb_pinturas_caract. La métrica evaluada es el coeficiente silueta, y la visualización permite identificar el número óptimo de clusters al observar el cambiuo en el coeficiente silueta en función del número de clusters.\n\nfviz_nbclust(...): Utiliza la función fviz_nbclust del paquete factoextra en R para determinar el número óptimo de clusters en un conjunto de datos.\nx = tb_pinturas_caract: Se especifica que el conjunto de datos tb_pinturas_caract es el objeto en el que se realizará el análisis de clusters.\nFUNcluster = kmeans: Indica que se utilizará el algoritmo de k-means para realizar el análisis de clusters, utilizando la función kmeans.\nmethod = \"silhouette\": Selecciona el método para determinar el número óptimo de clusters. En este caso, se utiliza “silhouette”, que es una medida de cuán bien definidos están los clusters.\nk.max = 15: Especifica el número máximo de clusters a considerar. En este caso, se evaluará hasta un máximo de 15 clusters.\n\n\n\n\nk_values = range(2, 16)\n\nkmeans_model = KMeans(n_init = 1)\n\nvisualizer = KElbowVisualizer(\n  kmeans_model, k=k_values, metric='silhouette', timings=False\n  )\n\nvisualizer = visualizer.fit(tb_pinturas_caract)\n\nvisualizer.show()\n\n\n\n\n\n\n\nplt.close()\n\nEl código utiliza el método del codo para determinar el número óptimo de clusters en el conjunto de datos tb_pinturas_caract utilizando el algoritmo de k-medias. La visualización de la métrica de silhouette en función del número de clusters ayuda a identificar el punto en el que los clusters son más cohesivos y separados, lo que permite identificar el número óptimo de clusters para el conjunto de datos.\n\nDefinición de k_values: Se crea una secuencia de valores desde 2 hasta 15 para representar diferentes números de clusters.\nInicialización de kmeans_model: Se crea una instancia del modelo de KMeans (k-medias) con n_init=1, indicando que el algoritmo se ejecutará solo una vez con una inicialización diferente.\nCreación del visualizador visualizer: Se utiliza KElbowVisualizer del paquete yellowbrick para visualizar la métrica de “silhouette” en función del número de clusters. La métrica de silhouette mide cuán similar es un objeto a su propio cluster (cohesión) en comparación con otros clusters (separación). Se utiliza timings=False para evitar la visualización de tiempos de ejecución.\nAjuste del modelo al conjunto de datos: Se ajusta el modelo de k-medias al conjunto de datos tb_pinturas_caract utilizando el método fit() del visualizador.\nVisualización y cierre del gráfico: Se muestra la visualización generada por el visualizador y se cierra la figura gráfica después de ser visualizada.\n\n\n\n\nEn el gráfico es posible ver el número óptimo de grupos. Pero también es importante aprender a calcular el coeficientes silueta para cualquier resultado. Por esta razón, incluimos los siguientes fragmentos de código.\n\nRPython\n\n\n\ntb_grupos %&gt;% \n  pull(grupos_jerar) %&gt;% \n  silhouette(\n    dist(tb_pinturas_caract)\n  ) %&gt;% summary %&gt;% pluck(\"avg.width\") -&gt; jerar_sil\n\n\ntb_grupos %&gt;% \n  pull(grupos_kmeans) %&gt;% \n  silhouette(\n    dist(tb_pinturas_caract)\n  ) %&gt;% summary %&gt;% pluck(\"avg.width\") -&gt; kmeans_sil\n\n\nSe utiliza la función pull para extraer la columna de grupos jerárquicos (grupos_jerar) del dataframe tb_grupos.\nSe aplica la función silhouette para calcular el coeficiente de silueta. Se utiliza la función dist para calcular la matriz de distancias entre las observaciones en tb_pinturas_caract.\nSe utiliza la función summary para resumir los resultados del coeficiente de silueta.\nSe utiliza la función pluck para extraer el ancho promedio del coeficiente de silueta y se almacena en la variable jerar_sil.\n\nLuego, se repite el mismo proceso para los grupos de k-means (grupos_kmeans) y se almacena el ancho promedio del coeficiente de silueta en la variable kmeans_sil.\nEstos valores (jerar_sil y kmeans_sil) proporcionarán información sobre la calidad de la agrupación según el coeficiente de silueta para los grupos jerárquicos y de k-means, respectivamente. Un coeficiente de silueta más cercano a 1 indica una mejor separación y cohesión de los grupos.\n\n\n\njerar_sil = silhouette_score(tb_pinturas_caract, tb_grupos[\"grupos_jerar\"])\n\nkmeans_sil =silhouette_score(tb_pinturas_caract, tb_grupos[\"grupos_kmeans\"])\n\n\nSe utiliza la función silhouette_score de scikit-learn.\ntb_pinturas_caract es el conjunto de datos de características de las pinturas.\ntb_grupos[\"grupos_jerar\"] proporciona los grupos jerárquicos asignados a cada observación.\nEl resultado se almacena en la variable jerar_sil.\nSe utiliza la misma función silhouette_score.\ntb_grupos[\"grupos_kmeans\"] proporciona los grupos de k-means asignados a cada observación.\nEl resultado se almacena en la variable kmeans_sil.\n\nEl coeficiente de silueta es una medida de la calidad de la agrupación, y un valor más cercano a 1 indica una mejor separación y cohesión de los grupos. Estos valores (jerar_sil y kmeans_sil) proporcionarán información sobre la calidad de la agrupación para los grupos jerárquicos y de k-means, respectivamente.\n\n\n\n¿Cuál es el resultado para el clustering jerárquico? ¿Cuál es el resultado para el algoritmo \\(k\\)-means?"
  },
  {
    "objectID": "unidad_03.html#visualización-de-los-grupos-conformados",
    "href": "unidad_03.html#visualización-de-los-grupos-conformados",
    "title": "Proyecto: The joy of programming",
    "section": "Visualización de los grupos conformados",
    "text": "Visualización de los grupos conformados\nEn el siguiente código, hemos dispuesto la caracterización de los grupos conformados mediante el algoritmo \\(k\\)-means. Utilizando el mapa de calor, hemos establecido las principales tendencias usando las siguientes etiquetas: \"RIVER\" (ríos), \"MOUNTAIN\" (montañas), \"AUTUMN\" (otoño), \"HOME\" (hogar), \"SEA\" (mar), PEAK (picos nevados) y \"WINTER\" (invierno). Podemos darnos cuenta de que, los resultados en python difieren de los resultados en R. Esto se debe a la fuerte dependencia que tiene \\(k\\)-means sobre los puntos iniciales. Es muy probable que las agrupaciones generadas con otros métodos nos permitan encontrar otras etiquetas, en el código están los espacios para la interpretación.\n\nRPython\n\n\n\nc(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\") -&gt; nombres_kmeans\nc(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\") -&gt; nombres_jerar\nc(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\") -&gt; nombres_dbscan\n\ntb_grupos %&gt;% \n  mutate(\n    # Podemos cambiar este código para elegir otro agrupamiento\n    # tendencia = nombres_jerar[grupos_jerar],\n    tendencia = nombres_kmeans[grupos_kmeans],\n    # tendencia = nombres_dbscan[grupos_dbscan + 1],\n  ) %&gt;% \n  select(-starts_with(\"grupos\")) %&gt;% \n  inner_join(\n    tb_pinturas\n  ) %&gt;% \n  group_by(tendencia) %&gt;% \n  summarise(across(is.numeric, mean)) -&gt; tb_centroides\n\n\ntb_centroides %&gt;% \n  select(- tendencia) %&gt;% \n  map_dbl(var) %&gt;% \n  as_tibble(rownames = \"item\") %&gt;% \n  top_n(30, value) %&gt;% \n  pull(item) -&gt; nm_items_relevantes\n\n\ntb_centroides %&gt;% \n  gather(\"rotulo\", \"value\", -tendencia) %&gt;% \n  filter(rotulo %in% nm_items_relevantes) %&gt;% \n  ggplot +\n  aes(x = tendencia, y = rotulo, fill = value) +\n  geom_raster() +\n  geom_text(aes(label = percent(value, accuracy = 1.0)), hjust = 1) +\n  scale_x_discrete(name = \"Tendencias\", position = \"top\") +\n  scale_y_discrete(name = \"Objetos\") +\n  scale_fill_gradient(high = \"#bb8899\", low = \"#ffeeee\", guide = \"none\") +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(hjust=0)\n  ) \n\n\n\n\n\n\n\n\nDefinición de Nombres de Grupos: Se definen nombres para los grupos obtenidos mediante k-medias, jerárquicos y DBSCAN.\nCreación de Centroides:\n\nSe agrega una columna “tendencia” al dataframe tb_grupos basada en los nombres de los grupos de k-medias.\nSe eliminan las columnas de grupos originales.\nSe realiza una unión con el marco de datos tb_pinturas.\nSe calculan los centroides (medias) de las variables numéricas para cada grupo.\n\nSelección de Elementos Relevantes:\n\nSe seleccionan las 30 variables con mayor varianza entre los centroides.\n\nVisualización con ggplot2:\n\nSe utiliza ggplot2 para representar gráficamente los elementos relevantes.\nSe utiliza geom_raster para mostrar colores proporcionales a los valores.\nSe utiliza geom_text para agregar etiquetas de porcentaje a los valores.\nSe personalizan los ejes y la paleta de colores.\n\n\n\n\nnombres_kmeans = dict(zip(\n  range(6),\n  [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n  ))\n\nnombres_jerar = dict(zip(\n  range(6),\n  [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n  ))\n\nnombres_dbscan = dict(zip(\n  [x - 1 for x in range(6)],\n  [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n  ))\n\n\n# Podemos cambiar este código para elegir otro agrupamiento\ntb_grupos[\"tendencia\"] = tb_grupos[\"grupos_kmeans\"].map(nombres_kmeans)\n# tb_grupos[\"tendencia\"] = tb_grupos[\"grupos_jerar\"].map(nombres_jerar)\n# tb_grupos[\"tendencia\"] = tb_grupos[\"grupos_dbscan\"].map(nombres_dbscan)\n\ntb_joined = pd.merge(\n  tb_grupos.drop(tb_grupos.filter(like='grupos').columns, axis=1),\n  tb_pinturas,\n  how='inner')\n\ntb_centroides = (tb_joined\n  .drop([\"EPISODE\", \"TITLE\"], axis=1)\n  .groupby('tendencia').agg('mean'))\n\nnm_items_relevantes = (tb_centroides\n  .apply(np.var, axis = 0)\n  .sort_values()[-30 :]\n  .index)\n\ntb_long = pd.melt(\n  tb_centroides.reset_index(), \n  id_vars=['tendencia'], var_name='rotulo', value_name='value'\n  )\ntb_filtered = tb_long[tb_long['rotulo'].isin(nm_items_relevantes)]\n\n\ncustom_palette = sns.color_palette(\"blend:#fee,#b89\", as_cmap=True)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n  pd.pivot_table(\n    tb_filtered, values='value', index='rotulo', columns='tendencia'\n    ),\n  annot=True, fmt=\".1%\", cmap = custom_palette,\n  cbar=False\n  )\nplt.xlabel('Tendencias')\nplt.ylabel('Objetos')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\nDefinición de Nombres de Grupos: Se definen diccionarios para mapear los índices de los grupos a nombres representativos para k-medias, jerárquicos y DBSCAN.\nMapeo de Tendencias: Se agrega una columna “tendencia” al marco de datos tb_grupos basada en los nombres de los grupos de k-medias (puedes cambiar esto para otros agrupamientos).\nUnión de Marcos de Datos: Se realiza una unión del marco de datos tb_grupos con tb_pinturas basada en las tendencias.\nCálculo de Centroides: Se calculan los centroides (medias) para cada tendencia.\nSelección de Elementos Relevantes: Se seleccionan las 30 variables con mayor varianza entre los centroides.\nDerretir el Marco de Datos: Se derrete el marco de datos para facilitar su uso en seaborn.\nVisualización con Seaborn y Matplotlib:\n\nSe utiliza seaborn para crear un mapa de calor utilizando el marco de datos derretido.\nSe personaliza el mapa de calor y se muestra utilizando matplotlib."
  },
  {
    "objectID": "unidad_03.html#análisis",
    "href": "unidad_03.html#análisis",
    "title": "Proyecto: The joy of programming",
    "section": "Análisis",
    "text": "Análisis\nA lo largo de los ejercicios de validación, hemos encontrado varias alternativas para dar soporte a nuestros resultados. Al respecto es necesario realizar un análisis de los resultados que hemos observado.\n\nLos resultados no son estables: en los ejercicios de validación, podemos observar resultados inconsistentes. Al cambiar de método, al cambiar de lenguaje o al cambiar de métrica. En un principio, esto puede parecer confuso, pero obedece a una realidad evidente desde la unidad anterior. La estructura interna de los datos presenta dos grupos bien diferenciados. Un primer grupo con pinturas de playas, palmeras y océanos; y otro grupo con paisajes del interior. Los demás grupos no se encuentran lo suficientemente separados, de hecho se mezclan entre sí.\nLas métricas de evaluación de los grupos realizados no son las más altas. De hecho, muchas veces tenemos un mejor ajuste para 2, 3 o 4 grupos. Esto hace que nuestras agrupaciones (6 grupos) no constituyan la forma más orgánica de agrupar las pinturas.\nEl más afectado es el algoritmo DBSCAN. Diseñado para capturar estructuras de grupos bien definidos, el algoritmo DBSCAN en este escenario no ha tenido un buen desempeño, generando grupos de tamaños muy diferentes y etiquetando como ruido a la mayoría de las observaciones. Se trata de un algoritmo que debe ser usado en problemas con grupos más separados.\nAunque la falta de una estructura de grupos en los datos le resta estabilidad a los algoritmos, es posible llegar a una caracterización definida. Hemos logrado obtener caracterizaciones de las pinturas forzando los grupos."
  },
  {
    "objectID": "unidad_04.html",
    "href": "unidad_04.html",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "En esta etapa se evalúan los resultados de los métodos de reducción de dimensiones presentados en la segunda etapa del proyecto. En el material del curso es posible encontrar las bases teóricas de los métodos de evaluación implementados en esta etapa, es importante consultarlo en caso de inquietudes. Al igual que las etapas anteriores, la implementación de los métodos de evaluación se realiza en lenguajes para el manejo de datos R y Python.\nLos métodos aplicados para la evaluación de los resultados de los métodos de reducción de dimensiones son: proporción de la variabilidad y test de Mantel.\n\n\nLa cuarta etapa del proyecto está orientada a cumplir el cuarto objetivo específico:\n\nEvaluar los resultados de los algoritmos de reducción de dimensiones y crear las visualizaciones e interpretaciones correspondientes."
  },
  {
    "objectID": "unidad_04.html#objetivo-actual",
    "href": "unidad_04.html#objetivo-actual",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "La cuarta etapa del proyecto está orientada a cumplir el cuarto objetivo específico:\n\nEvaluar los resultados de los algoritmos de reducción de dimensiones y crear las visualizaciones e interpretaciones correspondientes."
  },
  {
    "objectID": "unidad_04.html#análisis-de-componentes-principales",
    "href": "unidad_04.html#análisis-de-componentes-principales",
    "title": "Proyecto: The joy of programming",
    "section": "Análisis de Componentes principales",
    "text": "Análisis de Componentes principales\nAplicamos el análisis de componentes principales al conjunto de datos.\n\nRPython\n\n\n\nPCA(tb_pinturas_caract, graph = FALSE, ncp = 2) -&gt; ls_pca_resultado\n\nls_pca_resultado %&gt;% \n  pluck(\"ind\", \"coord\") %&gt;% \n  as_tibble() %&gt;% \n  setNames(c(\"X\", \"Y\")) -&gt; tb_pca\n\nEste código realiza un análisis de Componentes Principales (PCA) sobre el dataframe tb_pinturas_caract y posteriormente manipula los resultados para obtener un nuevo dataframe llamado tb_pca. A continuación, se presenta la explicación paso a paso:\nPCA(tb_pinturas_caract, ...): Se utiliza la función PCA del paquete FactoMineR para realizar un análisis de componentes crincipales en el dataframe tb_pinturas_caract.\nPCA(..., graph = FALSE, ncp = 2): El argumento graph = FALSE indica que no se deben generar gráficos durante el análisis. El argumento ncp = 2 especifica que se deben retener los dos primeros componentes principales.\n-&gt; ls_pca_resultado: El resultado es una lista que se almacena en el objeto ls_pca_resultado.\nls_pca_resultado %&gt;% pluck(\"ind\", \"coord\"): La función pluck se utiliza para extraer las coordenadas de las observaciones del resultado del PCA.\n%&gt;% as_tibble(): Se utiliza la función as_tibble para convertir las coordenadas a un formato de tibble.\n%&gt;% setNames(c(\"X\", \"Y\")): La función setNames se usa para renombrar las columnas del tibble como “X” y “Y”.\n-&gt; tb_pca: El resultado final es un datframe que se almacena en el objeto tb_pca.\n\n\n\narr_pinturas_standar = StandardScaler().fit_transform(tb_pinturas_caract)\n\n\nmod_pca = PCA(n_components=10)\nmod_pca = mod_pca.fit(arr_pinturas_standar)\n\ntb_pca = pd.DataFrame(\n    data    = mod_pca.transform(arr_pinturas_standar)[: , [0, 1]],\n    columns = [\"X\", \"Y\"]\n)\n\nEste código utiliza la librería scikit-learn para realizar un análisis de Componentes Principales (PCA) sobre el DataFrame tb_pinturas_caract y crea un nuevo DataFrame llamado tb_pca. A continuación, se presenta la explicación paso a paso:\narr_pinturas_standar =: El resultado de esta operacíon, que es un arreglo, se guarda en el objeto arr_pinturas_standar.\n**StandardScaler():** Se utilizaStandardScalerpara estandarizar el dataframetb_pinturas_caract`. Esto significa que cada característica (columna) se ajusta para tener una media de cero y una desviación estándar de uno.\n.fit_transform(tb_pinturas_caract): El método fit_transform realiza el ajuste y la transformación en una sola llamada.\nmod_pca =: El objeto resultante es un modelo, que se guarda en el objeto mod_pca.\nPCA(n_components=10): Se crea una instancia de la clase PCA con la especificación de retener dos componentes principales (n_components=2).\nmod_pca = mod_pca.fit(arr_pinturas_standar): Luego, se ajusta el modelo a los datos estandarizados utilizando el método fit.\nmod_pca.transform(arr_pinturas_standar): Se utiliza el modelo PCA entrenado para transformar los datos estandarizados. Esto significa proyectar los datos originales en el espacio de las dos primeras componentes principales.\ntb_pca = pd.DataFrame(data = ..., columns = ...): Se crea un nuevo dataframe llamado tb_pca utilizando las componentes principales obtenidas. Este dataframe tiene dos columnas, “X” y “Y”, que representan las dos dimensiones principales del espacio de los componentes principales."
  },
  {
    "objectID": "unidad_04.html#t-sne",
    "href": "unidad_04.html#t-sne",
    "title": "Proyecto: The joy of programming",
    "section": "t-SNE",
    "text": "t-SNE\nRealizamos la reducción a 2 dimensiones aplicando el algoritmo t-SNE a nuestro dataset de características tb_pinturas_caract. En este ejemplo, utilizamos un perplexity = 20, pero podríamos utilizar cualquier otro valor entre 1 y 50.\n\nRPython\n\n\n\ntsne(\n  dist(tb_pinturas_caract),\n  perplexity = 20, \n  k = 2, \n  initial_dims = ncol(tb_pinturas_caract)\n) -&gt; mt_tsne_resultado\n\nmt_tsne_resultado %&gt;% \n  as_tibble(.name_repair = \"minimal\") %&gt;% \n  setNames(c(\"X\", \"Y\")) -&gt; tb_tsne\n\ntsne(dist(tb_pinturas_caract),...): Se utiliza la función tsne para aplicar el método t-SNE a la matriz de distancias de las características de las pinturas contenidas en tb_pinturas_caract. Se especifican parámetros como la perplexidad, el número de dimensiones, y las dimensiones iniciales.\n... -&gt; mt_tsne_resultado: El resultado de la aplicación de t-SNE es una matriz, que se asigna al objeto mt_tsne_resultado.\nmt_tsne_resultado %&gt;% as_tibble(.name_repair = \"minimal\"):  Se utiliza %&gt;% para encadenar operaciones. El resultado de t-SNE se transforma a un dataframe mediante la función as_tibble.\n... %&gt;% setNames(c(\"X\", \"Y\")) -&gt; tb_tsne: Se renombran las columnas como “X” y “Y”. El resultado es un dataframe que se almacena en el objeto tb_tsne.\n\n\n\nmod_tsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=20)\n\nmt_tsne_resultado = mod_tsne.fit_transform(tb_pinturas_caract)\n\ntb_tsne = pd.DataFrame(\n    data    = mt_tsne_resultado,\n    columns = [\"X\", \"Y\"]\n)\n\nmod_tsne = TSNE(n_components=2, ...): Se instancia un modelo t-SNE utilizando la clase TSNE del paquete sklearn. Se especifican parámetros como el número de componentes, la tasa de aprendizaje, el método de inicialización y la perplexidad. La instancia es un modelo que se guarda en el objeto mod_tsne.\nmod_tsne.fit_transform(tb_pinturas_caract): Se ajusta el modelo t-SNE usando los datos de las pinturas contenidas en tb_pinturas_caract utilizando el método fit_transform. Esto realiza el proceso de reducción de dimensionalidad y devuelve las coordenadas en el espacio de baja dimensión en formato de matriz, que se guardae en el objeto mt_tsne_resultado.\ntb_tsne = pd.DataFrame(...): Se crea un DataFrame llamado tb_tsne con las coordenadas resultantes del t-SNE, asignando nombres a las columnas como “X” y “Y”."
  },
  {
    "objectID": "unidad_04.html#proporción-de-variabilidad-conservada",
    "href": "unidad_04.html#proporción-de-variabilidad-conservada",
    "title": "Proyecto: The joy of programming",
    "section": "Proporción de variabilidad conservada",
    "text": "Proporción de variabilidad conservada\nCalculamos la varianza recogida por el análisis de componentes principales.\n\nRPython\n\n\n\nls_pca_resultado %&gt;% \n  get_eig %&gt;% \n  head(10) -&gt; tb_varianza\n\n\n\n\n\n\n\neigenvalue\nvariance.percent\ncumulative.variance.percent\n\n\n\n\nDim.1\n5.56\n8.42\n8.42\n\n\nDim.2\n3.45\n5.22\n13.64\n\n\nDim.3\n3.05\n4.62\n18.26\n\n\nDim.4\n2.34\n3.54\n21.80\n\n\nDim.5\n2.16\n3.27\n25.07\n\n\nDim.6\n2.06\n3.12\n28.19\n\n\nDim.7\n1.98\n3.00\n31.19\n\n\nDim.8\n1.88\n2.85\n34.04\n\n\nDim.9\n1.80\n2.73\n36.77\n\n\nDim.10\n1.68\n2.54\n39.31\n\n\n\n\n\nls_pca_resultado %&gt;% get_eig: Extrae los valores propios (eigenvalues) del análisis de componentes principales mediante get_eig.\n... %&gt;% head(10): Toma las primeras 10 filas de estos valores propios usando head(10).\n... -&gt; tb_varianza: El resultado se almacena en el objeto tb_varianza.\n\n\n\ntb_varianza = pd.DataFrame({\n  'Componente': \n    np.array(range(len(mod_pca.explained_variance_ratio_))) + 1,\n  'Varianza Explicada': mod_pca.explained_variance_ratio_,\n  'Varianza acumulada': mod_pca.explained_variance_ratio_.cumsum()\n})\n\n\n\n\n\n\n\n\nComponente\nVarianza Explicada\nVarianza acumulada\n\n\n\n\n0\n1\n0.084240\n0.084240\n\n\n1\n2\n0.052203\n0.136443\n\n\n2\n3\n0.046177\n0.182620\n\n\n3\n4\n0.035389\n0.218008\n\n\n4\n5\n0.032660\n0.250668\n\n\n5\n6\n0.031196\n0.281864\n\n\n6\n7\n0.030028\n0.311892\n\n\n7\n8\n0.028464\n0.340356\n\n\n8\n9\n0.027341\n0.367697\n\n\n9\n10\n0.025430\n0.393127\n\n\n\n\n\n\ntb_varianza = pd.DataFrame(...): Se crea un dataframe llamado tb_varianza que contiene la varianza explicada y la varianza acumulada asociadas con cada componente principal.\nmod_pca.explained_variance_ratio_: Proporciona la proporción de varianza explicada por cada componente.\nmod_pca.explained_variance_ratio_.cumsum(): Proporciona la varianza acumulada.\n\n\n\nA partir de la varianza recogida generamos el gráfico de sedimentación\n\nRPython\n\n\n\nfviz_screeplot(ls_pca_resultado)\n\n\n\n\n\n\n\n\nEl gráfico de sedimentación muestra la proporción de varianza explicada por cada componente principal en el eje y contra el número de componente principal en el eje x. Este gráfico es útil para determinar cuántos componentes principales retener en función de la cantidad de varianza que explican.\nfviz_screeplot:  es una función de visualización que se utiliza para trazar el gráfico de sedimentación de un análisis de componentes principales (PCA).\nls_pca_resultado: es el resultado del análisis de componentes principales previo.\n\n\n\nplt.plot(tb_varianza[\"Componente\"], tb_varianza[\"Varianza Explicada\"], 'ok-')\nplt.bar(tb_varianza[\"Componente\"], tb_varianza[\"Varianza Explicada\"])\nplt.xlabel('Número de Componentes Principales')\nplt.ylabel('Varianza Explicada')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\nplt.plot(..., 'ok-'): - Se utiliza plt.plot para trazar una línea que conecta los puntos (número de componente principal, proporción de varianza explicada). Se especifica 'ok-' para que los puntos sean de color negro ('k'), redondos ('o') y conectados por líneas ('-').\nplt.bar(...): Se utiliza plt.bar para trazar un gráfico de barras de la proporción de varianza explicada por cada componente principal. Esto proporciona una representación visual adicional de la distribución de la varianza entre los componentes principales.\nplt.xlabel(...); plt.ylabel(...): Se añaden etiquetas al eje x y al eje y,\nplt.show(); plt.close(): Se muestra la figura y luego se cierra."
  },
  {
    "objectID": "unidad_04.html#test-de-mantel",
    "href": "unidad_04.html#test-de-mantel",
    "title": "Proyecto: The joy of programming",
    "section": "Test de Mantel",
    "text": "Test de Mantel\nLa prueba de Mantel permite establecer la correlación entre dos matrices de distancias con un nivel de significancia dado.\n\nRPython\n\n\n\ntb_pinturas_caract %&gt;% dist -&gt; mt_dist_pinturas\n\ntb_pca %&gt;% dist -&gt; mt_dist_pca\n\nmantel.randtest(\n   mt_dist_pinturas, mt_dist_pca, \n   nrepet = 1000\n   ) -&gt; mantel_result_pca\n\nmantel_result_pca\n\nMonte-Carlo test\nCall: mantel.randtest(m1 = mt_dist_pinturas, m2 = mt_dist_pca, nrepet = 1000)\n\nObservation: 0.6786 \n\nBased on 1000 replicates\nSimulated p-value: 0.000999001 \nAlternative hypothesis: greater \n\n     Std.Obs  Expectation     Variance \n2.630284e+01 4.859162e-04 6.646608e-04 \n\ntb_tsne %&gt;% dist -&gt; mt_dist_tsne\n\nmantel.randtest(\n   mt_dist_pinturas, mt_dist_tsne, \n   nrepet = 1000\n   ) -&gt; mantel_result_tsne\n\nmantel_result_tsne\n\nMonte-Carlo test\nCall: mantel.randtest(m1 = mt_dist_pinturas, m2 = mt_dist_tsne, nrepet = 1000)\n\nObservation: 0.6670998 \n\nBased on 1000 replicates\nSimulated p-value: 0.000999001 \nAlternative hypothesis: greater \n\n     Std.Obs  Expectation     Variance \n3.482720e+01 3.515885e-04 3.665106e-04 \n\n\nEste código realiza pruebas de Mantel para comparar la similitud entre las matrices de distancia de las características originales (tb_pinturas_caract) con las matrices de distancia de las coordenadas en el espacio de componentes principales (tb_pca) y en el espacio de t-SNE (tb_tsne).\ntb_pinturas_caract %&gt;% dist -&gt; mt_dist_pinturas: Calcula la matriz de distancias entre las filas de tb_pinturas_caract y almacena el resultado en mt_dist_pinturas. El operador %&gt;% (pipe) se utiliza para pasar el resultado de una operación como argumento a la siguiente.\ntb_pca %&gt;% dist -&gt; mt_dist_pca: Calcula la matriz de distancias entre las filas de tb_pca, que contiene las coordenadas de las observaciones en el espacio de componentes principales, y almacena el resultado en mt_dist_pca.\nmantel.randtest(mt_dist_pinturas, mt_dist_pca, ...): Realiza una prueba de Mantel entre las matrices de distancia mt_dist_pinturas y mt_dist_pca; mantel.randtest del paquete ade4 es la función que realiza una prueba de Mantel.\nmantel.randtest(..., nrepet = 1000): indica que se deben realizar 1000 permutaciones aleatorias para calcular el p-valor.\nmantel_result_pca: Almacena los resultados de la prueba de Mantel entre las distancias de tb_pinturas_caract y tb_pca.\ntb_tsne %&gt;% dist -&gt; mt_dist_tsne: Calcula la matriz de distancias entre las filas de tb_tsne, que contiene las coordenadas de las observaciones en el espacio de t-SNE, y almacena el resultado en mt_dist_tsne.\nmantel.randtest(mt_dist_pinturas, mt_dist_tsne, ...): Realiza una prueba de Mantel entre las matrices de distancia mt_dist_pinturas y mt_dist_tsne; mantel.randtest del paquete ade4 es la función que realiza una prueba de Mantel.\nmantel.randtest(..., nrepet = 1000): Se utilizan 1000 permutaciones aleatorias para calcular el p-valor.\nmantel_result_tsne: Almacena los resultados de la prueba de Mantel entre las distancias de tb_pinturas_caract y tb_tsne.\n\n\n\nmt_dist_pinturas = distance_matrix(tb_pinturas_caract.values, tb_pinturas_caract.values)\n\nmt_dist_pca = distance_matrix(tb_pca.values, tb_pca.values)\n\nmantel_result_pca = mantel.test(mt_dist_pinturas, mt_dist_pca, perms=1000, method='pearson', tail='upper')\n\n\nmantel_result_pca.r\n\n0.6785999550226706\n\nmantel_result_pca.p\n\n0.001\n\nmt_dist_tsne = distance_matrix(tb_tsne.values, tb_tsne.values)\n\nmantel_result_tsne = mantel.test(mt_dist_pinturas, mt_dist_tsne, perms=1000, method='pearson', tail='upper')\n\n\nmantel_result_tsne.r\n\n0.6791782039610739\n\nmantel_result_tsne.p\n\n0.001\n\n\nEste código realiza pruebas de Mantel para comparar la similitud entre las matrices de distancia euclidiana de las características originales (tb_pinturas_caract) con las matrices de distancia euclidiana de las coordenadas en el espacio de componentes principales (tb_pca) y en el espacio de t-SNE (tb_tsne).\nmt_dist_pinturas = distance_matrix(...): Calcula la matriz de distancias euclidianas entre todas las filas de tb_pinturas_caract. Guarda el resultado en el objeto mt_dist_pinturas.\ntb_pinturas_caract.values: se utiliza para obtener la representación de matriz de los datos.\nmt_dist_pca = distance_matrix(): Similar a la línea anterior, calcula la matriz de distancias euclidianas entre todas las filas de tb_pca. Guarda el resultado en el objeto mt_dist_pca.\ntb_pca.values: se utiliza para obtener la representación de matriz de las coordenadas en el espacio de componentes principales.\nmantel_result_pca = mantel.test(...): Realiza una prueba de Mantel entre las matrices de distancia; mantel.test del paquete mantel es la función que realiza una prueba de Mantel. El resultado se guarda en el objeto mantel_result_pca.\nmantel.test(mt_dist_pinturas, mt_dist_pca,...): las matrices de distancia evaluadas son la matriz de distancias de los datos de las pinturas mt_dist_pinturas y la matriz de distancias de la representación de los datos en el espacio de componentes principales mt_dist_pca.\nmantel.test(..., perms, method, tail): Se utilizan los siguientes argumentos para la función: perms=1000 indica que se deben realizar 1000 permutaciones aleatorias para calcular el p-valor; method='pearson' especifica que se debe usar la correlación de Pearson como medida de asociación; tail='upper' indica que se está interesado en una prueba de una cola (una dirección).\nmantel_result_pca.r: Muestra el coeficiente de correlación de Mantel obtenido en la prueba de Mantel para tb_pca.\nmantel_result_pca.p: Muestra el p-valor asociado con la prueba de Mantel para tb_pca.\nmt_dist_tsne = distance_matrix(...): Calcula la matriz de distancias euclidianas entre todas las filas de tb_tsne. Guarda el resultado en el objeto mt_dist_tsne.\ntb_tsne.values: se utiliza para obtener la representación de matriz de las coordenadas en el espacio de t-SNE.\nmantel_result_tsne = mantel.test(...): Realiza una prueba de Mantel entre las matrices de distancia; mantel.test del paquete mantel es la función que realiza una prueba de Mantel. El resultado se guarda en el objeto mantel_result_tsne.\nmantel.test(mt_dist_pinturas, mt_dist_tsne, ...): las matrices de distancia evaluadas son la matriz de distancias de los datos de las pinturas mt_dist_pinturas y la matriz de distancias de la representación de los datos en el espacio t-SNE mt_dist_tsne.\nmantel.test(..., perms, method, tail): Utiliza los mismos parámetros que la prueba de Mantel anterior.\nmantel_result_tsne.r: Muestra el coeficiente de correlación de Mantel obtenido en la prueba de Mantel para tb_tsne.\nmantel_result_tsne.p: Muestra el p-valor asociado con la prueba de Mantel para tb_tsne."
  },
  {
    "objectID": "unidad_04.html#círculo-de-correlaciones",
    "href": "unidad_04.html#círculo-de-correlaciones",
    "title": "Proyecto: The joy of programming",
    "section": "Círculo de correlaciones",
    "text": "Círculo de correlaciones\nEl círculo de correlaciones nos ayuda a interpretar el análisis de componentes principales. Las variables son representadas por medio de flechas que se distribuyen en el círculo unitario.\n\nCada variable se representa por medio de una flecha.\nLa dirección de la flecha indica la dirección del plano hacia donde esa variable crece.\nLa longitud de la flecha indica el nivel de representación que tiene esta variable en el plano de los componentes principales.\nEl ángulo entre dos flechas permite intuir su correlación. Flechas con un ángulo muy agudo suelen representar variables con una correlación positiva fuerte. Flechas opuestas, con un ángulo muy obtuso o muy abierto, suelen representar variables con una alta correlación negativa; y flechas que presentan un ángulo recto están generalmente asociadas a variables con una correlación cercana a cero.\n\n\nRPython\n\n\n\nfviz_pca_var(ls_pca_resultado, geom = \"arrow\")\n\n\n\n\n\n\n\nfviz_pca_var(\n  ls_pca_resultado,\n  repel = TRUE,\n  select.var = list(cos2 = 0.15)\n  )\n\n\n\n\n\n\n\n\nEste código genera dos visualizaciones del análisis de componentes principales. La primera muestra la contribución de todas las variables mediante flechas en el espacio de componentes principales. La segunda visualización resalta específicamente las variables con una contribución significativa, utilizando el parámetro select.var para filtrar las variables basándose en su coseno cuadrado. El uso de repel = TRUE ayuda a mejorar la legibilidad de las etiquetas evitando solapamientos.\nfviz_pca_var(...): La función fviz_pca_var se utiliza para visualizar la contribución de las variables originales en el espacio de componentes principales.\nls_pca_resultado: es el resultado de un análisis de componentes principales previo.\ngeom = \"arrow\": El parámetro geom = \"arrow\" indica que se deben utilizar flechas para representar la contribución de las variables.\nrepel = TRUE: indica que las etiquetas de las variables deben colocarse de manera que eviten solapamientos.\nselect.var = list(cos2 = 0.15): selecciona las variables cuyo coseno cuadrado (cos2) es mayor o igual a 0.15, lo que significa que estas variables tienen una contribución sustancial en al menos uno de los ejes principales.\n\n\n\n(fig, ax) = plt.subplots(figsize=(8, 8))\nfor i in range(0, mod_pca.components_.shape[1]):\n  ax.arrow(\n    0, 0, \n    mod_pca.components_[0, i]*1.5,  \n    mod_pca.components_[1, i]*1.5,  \n    head_width=0.05,\n    head_length=0.08, fc='steelblue', ec='steelblue'\n  )\n    \nan = np.linspace(0, 2 * np.pi, 100)\nplt.plot(np.cos(an), np.sin(an))  \nplt.axis('equal')\nax.set_title('Variable factor map')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\n\n\nthreshold = 0.15\nmax_representation = (np.abs(mod_pca.components_)\n  .take([0, 1], axis=0)\n  .max(0))\n\ncondition = np.where(max_representation &gt; threshold)[0]\n\n(fig, ax) = plt.subplots(figsize=(8, 8))\nfor i in condition:\n  ax.arrow(\n    0, 0, \n    mod_pca.components_[0, i]*1.5,  \n    mod_pca.components_[1, i]*1.5,  \n    head_width=0.05,\n    head_length=0.08, fc='steelblue', ec='steelblue'\n  )\n    \n  plt.text(\n    mod_pca.components_[0, i]*1.7,\n    mod_pca.components_[1, i]*1.7,\n    tb_pinturas_caract.columns.values[i]\n  )\n\n\nan = np.linspace(0, 2 * np.pi, 100)\nplt.plot(np.cos(an), np.sin(an))  \nplt.axis('equal')\nax.set_title('Variable factor map')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\nEste código crea dos visualizaciones del mapa de factores variables en un espacio de componentes principales. La primera visualización muestra todas las variables, mientras que la segunda resalta las variables con contribución significativa según un umbral predefinido.\n(fig, ax) = plt.subplots(figsize=(8, 8)): Se crea una nueva figura y ejes con un tamaño de 8x8.\nSe dibujan las flechas usando un bucle.\nfor i in range(0, mod_pca.components_.shape[1]):: Se utiliza un bucle for para iterar sobre todas las variables (columnas) en los componentes principales.\nax.arrow(...): Se añade una flecha para cada variable en el espacio de componentes principales.\nSe añade un círculo unitario para proporcionar una escala.\nan = np.linspace(...): Se genera un conjunto de puntos para un círculo completo.\nplt.plot(np.cos(an), np.sin(an)) Se añade un círculo unitario utilizando las funciones seno y coseno.\ny finalmente\nplt.axis('equal'): Se ajusta la escala de los ejes para que tengan la misma proporción.\nax.set_title('Variable factor map'): Se establece el título del gráfico.\nplt.show(); plt.close(): Se muestra la figura y luego se cierra.\nPara el segundo gráfico, seguimos los mismos pasos adicionando un umbral de 0.15.\nthreshold = 0.15: Se define un umbral para la contribución significativa de las variables.\n(np.abs(...).take(...).max(...)): Se calcula la representación máxima en los dos primeros componentes principales para cada variable. Este resultado se guarda en el objeto max_representation.\nnp.where(max_representation &gt; threshold)[0]: Se encuentran las variables con una contribución mayor que el umbral. Este resultado se guarda en el objeto condition.\n(fig, ax) = plt.subplots(figsize=(8, 8)): Se crea una nueva figura y ejes con un tamaño de 8x8.\nSe dibujan las flechas usando un bucle.\nfor i in range(0, mod_pca.components_.shape[1]):: Se utiliza un bucle for para iterar sobre todas las variables (columnas) en los componentes principales.\nax.arrow(...): Se añade una flecha para cada variable en el espacio de componentes principales.\nplt.text(...): Se añaden los nombres de las variables que fueron filtradas por el umbral.\nSe añade un círculo unitario para proporcionar una escala.\nan = np.linspace(...): Se genera un conjunto de puntos para un círculo completo.\nplt.plot(np.cos(an), np.sin(an)) Se añade un círculo unitario utilizando las funciones seno y coseno.\nPor último, se agregan algunos detalles\nplt.axis('equal'): Se ajusta la escala de los ejes para que tengan la misma proporción.\nax.set_title('Variable factor map'): Se establece el título del gráfico.\nplt.show(); plt.close(): Se muestra la figura y luego se cierra."
  },
  {
    "objectID": "unidad_04.html#visualización-de-datos-reducidos",
    "href": "unidad_04.html#visualización-de-datos-reducidos",
    "title": "Proyecto: The joy of programming",
    "section": "Visualización de datos reducidos",
    "text": "Visualización de datos reducidos\nEn el transcurso del proyecto hemos aprendido a visualizar nuestros resultados utilizando las técnicas de reducción de dimensiones. En esta unidad abordamos dos visualizaciones más. El biplot, para visualizar los resultados del análisis de componentes principales, y el diccionario de gráficos para los resultados del algoritmo t-SNE.\nEl biplot consiste en generar, en una sola visualización, la nube de puntos acompañada del círculo de correlaciones. De manera que se interpreta para cada dato en qué variables tiene mayores valores.\n\nRPython\n\n\n\nfviz_pca_biplot(\n  ls_pca_resultado, geom.ind = \"point\", \n  geom.var =  c(\"arrow\", \"text\"), repel = TRUE,\n  select.var = list(cos2 = 0.15)\n)\n\n\n\n\n\n\n\n\nEste código utiliza la función fviz_pca_biplot para crear un biplot que representa las observaciones como puntos y las variables como flechas con etiquetas de texto. Se destacan las variables con un coseno cuadrado (cos2) mayor al 0.15. Este tipo de visualización es común en análisis de componentes principales para entender la relación entre observaciones y variables.\nfviz_pca_biplot: es una función que crea un biplot a partir de los resultados de un análisis de componentes principales.\ngeom.ind = \"point\": indica que las observaciones deben representarse como puntos en el biplot.\ngeom.var = c(\"arrow\", \"text\"): especifica que las variables se deben representar como flechas y con etiquetas de texto en el biplot.\nrepel = TRUE: indica que las etiquetas de las variables deben ajustarse automáticamente para evitar superposiciones.\nselect.var = list(cos2 = 0.15): selecciona las variables basadas en un umbral de coseno cuadrado (cos2). En este caso, las variables con un cos2 mayor al 0.15 serán resaltadas en el biplot.\n\n\n\nthreshold = 0.15\nmax_representation = (np.abs(mod_pca.components_)\n  .take([0, 1], axis=0)\n  .max(0))\n\ncondition = np.where(max_representation &gt; threshold)[0]\n\n(fig, ax) = plt.subplots(figsize=(10, 10))\n\nax.plot(tb_pca[\"X\"], tb_pca[\"Y\"], \"ko\", markersize = 1.5)\n\nfor i in condition:\n  ax.arrow(\n    0, 0, \n    mod_pca.components_[0, i]*10,  \n    mod_pca.components_[1, i]*10,  \n    head_width=0.2,\n    head_length=0.3, fc='steelblue', ec='steelblue'\n  )\n    \n  ax.text(\n    mod_pca.components_[0, i]*15,\n    mod_pca.components_[1, i]*15,\n    tb_pinturas_caract.columns.values[i]\n  )\n\nax.axis('equal')\nax.set_title('Biplot')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\nEste código crea un biplot que muestra tanto las observaciones como las variables en el espacio de los dos primeros componentes principales. Las variables significativas se destacan con flechas y etiquetas en el biplot.\nthreshold = 0.15: Se define un umbral para la contribución significativa de las variables.\n(np.abs(...).take(...).max(...)): Se calcula la representación máxima en los dos primeros componentes principales para cada variable. Este resultado se guarda en el objeto max_representation.\nnp.where(max_representation &gt; threshold)[0]: Se encuentran las variables con una contribución mayor que el umbral. Este resultado se guarda en el objeto condition.\n(fig, ax) = plt.subplots(figsize=(8, 8)): Se crea una nueva figura y ejes con un tamaño de 8x8.\nax.plot(tb_pca[\"X\"], tb_pca[\"Y\"], \"ko\", markersize = 1.5): Se grafican los puntos del espacio de componentes principales.\nMediante un bucle se dibujan las flechas y los textos:\nfor i in range(0, mod_pca.components_.shape[1]):: Se utiliza un bucle for para iterar sobre todas las variables (columnas) en los componentes principales.\nax.arrow(...): Se añade una flecha para cada variable en el espacio de componentes principales.\nplt.text(...): Se añaden los nombres de las variables que fueron filtradas por el umbral.\nSe realizan ajustes a los ejes y al título del gráfico.\nplt.axis('equal'): Se ajusta la escala de los ejes para que tengan la misma proporción.\nax.set_title('Biplot'): Se establece el título del gráfico.\nplt.show(); plt.close(): Se muestra la figura y luego se cierra.\n\n\n\nEn el caso del t-SNE podemos visualizar un gráfico para cada variable de interés. Este diccionario de gráficos nos ayuda a interpretar el resultado, conociendo cuáles variables se expresan mejor en qué regiones del plano.\n\nRPython\n\n\n\nc(\n  \"OCEAN\", \"MOUNTAIN\", \"WINTER\",\"STRUCTURE\",\n  \"TREE\", \"CONIFER\", \"DECIDUOUS\", \"SNOWY_MOUNTAIN\"\n  ) -&gt; nm_items_relevantes\n\ntb_pinturas_caract %&gt;% \n  select(all_of(nm_items_relevantes)) %&gt;% \n  bind_cols(tb_tsne) %&gt;% \n  pivot_longer(\n    all_of(nm_items_relevantes),\n    names_to = \"objeto\",\n    values_to = \"presencia\"\n  ) %&gt;% \n  ggplot + aes(x = X, y = Y, colour = as.character(presencia)) +\n  geom_point() + \n  scale_color_discrete(guide = \"none\", type = c(\"#AA8888\", \"#88aa88\")) +\n  facet_wrap(~objeto, nrow = 4)\n\n\n\n\n\n\n\n\nEste código crea un gráfico de dispersión facetado que representa la presencia o ausencia de objetos relevantes en un espacio bidimensional (X e Y de tb_tsne). Cada panel del gráfico representa un objeto, y los puntos están coloreados según su presencia o ausencia.\nc(...) -&gt; nm_items_relevantes: crea un vector llamado nm_items_relevantes que contiene los nombres de las columnas relevantes.\ntb_pint.. %&gt;% select(all_of(nm_items_relevantes)): selecciona las columnas relevantes del dataframe tb_pinturas_caract\n... %&gt;% bind_cols(tb_tsne): combina las columnas seleccionadas horizontalmente (bind_cols) con el dataframe tb_tsne.\npivot_longer(...): utiliza la función pivot_longer para convertir los datos de formato ancho a largo.\nall_of(nm_items_relevantes): Las columnas definidas en nm_items_relevantes se transforman en las columnas objeto y presencia.\nnames_to = \"objeto\", values_to = \"presencia\": Una nueva columna denominada objeto contiene los nombres de las variables relevantes, mientras una nueva columna denominada presencia contiene sus valores.\nggplot + aes(...) + geom_point() es la estructura básica de un gráfico de dispersión.\naes(x = X, y = Y, colour = as.character(presencia)): Las estéticas (aes) definen la posición en X, Y y el color de los puntos según la presencia de objetos definidos en nm_items_relevantes.\nscale_color_discrete(...) personaliza los colores que señalan la presencia o ausencia de las características relevantes.\nfacet_wrap(~objeto, ..) distribuye el gráfico en diferentes paneles para cada objeto definido en nm_items_relevantes.\n\n\n\nnm_items_relevantes = [[\"CONIFER\", \"DECIDUOUS\"], [\"MOUNTAIN\", \"OCEAN\"], [\"SNOWY_MOUNTAIN\", \"STRUCTURE\"], [\"TREE\",  \"WINTER\"]] \n\n\ncolors = {0: 'rosybrown', 1: 'darkolivegreen'}\n\nfig, ax = plt.subplots(nrows = 4, ncols=2)\n\nfor i, sublist in enumerate(nm_items_relevantes):\n  for j, item in enumerate(sublist):\n    \n    col = [colors[i] for i in tb_pinturas_caract[item].to_list()]\n    \n    ax[i, j].scatter(tb_tsne[\"X\"], tb_tsne[\"Y\"], c = col, s = 2)\n    ax[i, j].set_title(item)\n\nfig.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\nEste código crea subgráficos para cada par de elementos relevantes y visualiza los puntos en un espacio bidimensional. Los puntos están coloreados según la presencia de los elementos en las pinturas, que se registra en el dataframe tb_pinturas_caract; cada subgráfico representa una variable.\nnm_items_relevantes: es una lista de listas que contiene grupos de elementos relevantes.\ncolors: es un diccionario que asigna valores (0 o 1) a colores (‘rosybrown’ o ‘darkolivegreen’).\nfig, ax = plt.subplots(nrows=4, ncols=2): crea subgráficos en una matriz de 4 filas y 2 columnas.\nfor i, sublist in ...: for j, item in ...: Es un bucle anidado itera sobre las listas de objetos en nm_items_relevantes..\nfor i, sublist in ...: for j, item in ...: Es un bucle anidado itera sobre las listas de objetos en nm_items_relevantes.\ncol = [colors[i] for i in ...]: Para cada objeto en la variable item, se asigna un color según el diccionario colors.\n.scatter(...): agrega puntos al gráfico de dispersión en los subgráficos.\nset_title(...): Se establece el título de cada subgráfico como el nombre del objeto.\nfig.tight_layout(): ajusta automáticamente la disposición de los subgráficos para evitar superposiciones.\nplt.show(); plt.close(): Se muestra la figura y luego se cierra."
  },
  {
    "objectID": "program_00_setup.html",
    "href": "program_00_setup.html",
    "title": "Configuración del ambiente de trabajo en R",
    "section": "",
    "text": "R es un lenguaje de programación de acceso libre, inicialmente diseñado y utilizado para realizar análisis estadístico.\nR también es un programa que instalamos para interpretar el código que escribimos.\nNo fue creado por ingenieros de software para el desarrollo de software, sino por estadísticos como un ambiente interactivo para el análisis de datos.\nAdemás de R trabajaremos con RStudio, un IDE (Integrated Development Environment) que funciona como una “máscara” sobre R, con más herramientas y facilidades.\nLa forma en que trabajaremos en R será a través de secuencias de comandos, conocidos como script, que se pueden ejecutar en cualquier momento. Estos scripts sirven como un registro del análisis realizamos.\n\n\n\n\n\nSe ejecuta en todos los sistemas operativos principales: Windows, Mac Os, UNIX/Linux.\nLos scripts y los objetos de datos se pueden compartir sin problemas entre plataformas.\nNo solamente permite realizar análisis estadístico, tambien es posible capturar información de páginas web, crear aplicaciones web, procesar texto, entre otras cosas.\nLa comunidad que lo usa es muy amplia (= muchísima ayuda e información en internet).\n\nStackoverflow\nR-Universe\nR-Bloggers\nTowards Data Science\nEntre otros.\n\nEs muy versátil para la creación de gráficos.\nTiene disponibles muchos paquetes para diferentes tipos de análisis.\n¡ES GRATIS!.\n\n\n\n\n\nManejar, combinar, limpiar y reorganizar datos.\nAnálisis estadístico, álgebra matricial, modelado, estadística avanzada.\nUsar un entorno gráfico poderoso para explorar datos o para publicar.\nTrabajar de forma reproducible a partir de scripts.\n\n\n\n\n\nLa documentación a veces es muy técnica o muy compacta.\nNo todo el código ha sido exhaustivamente testeado, es decir, no siempre hay garantías que las cosas funcionen.\nLa curva de aprendizaje es más difícil que programas como SPSS o Minitab (frente a Python, Stata o SAS es similar).\n\n\n\n\nhttps://cran.r-project.org/\n\n\nLuego de instalar R, se recomienda instalar Rtools.\n\n\n\n\n\nPor defecto\nRStudio - Recomendada. Es la que usaremos en este curso.\nJupyter\nEclipse\nEntre otras\n\n\n\n\n\n\nRcmdr\nRKWard\n\n\n\n\n\nSi se está trabajando en el panel console solamente es necesario presionar “Enter”.\nSi se está trabajando en un script es posible ejecutar los comandos presionando “Ctrl+Enter”, o haciendo clic en el botón Run en la parte derecha superior del panel de edición.\n\n\n\n\n\nCuando varias funciones son desarrolladas con un objetivos similar se suelen agrupar en paquetes, los cuales son colaborativamente distribuidos de forma gratuita.\nPara utilizar las funciones de un paquete es necesario instalar este paquete primero usando install.packages() y luego cargarlo en nuestro ambiente de trabajo usando library().\nSe instalan una vez y se llaman muchas veces.\nLista de paquetes del CRAN\nLista de paquetes de Bioconductor\nrdrr.io\n\n\n\n\n\n# Instalar el paquete installr\ninstall.packages(\"installr\")\n# Cargar el paquete\nlibrary(installr) \n# Comando para actualizar R\nupdateR()\n# Comando para actualizar paquetes\nupdate.packages(checkBuilt = TRUE, ask = FALSE)\n\n\n\n\n\nUsar projects (proyectos) para ajustar el ambiente de trabajo para un proyecto.\nPor reproducibilidad, facilidad y para tener los proyectos mejor organizados es mucho mejor usar projects.\nLa idea es que cada proyecto tenga su propio espacio y esté aislado, de tal manera que los recursos y códigos de uno no interfieran con otros proyectos.\nComo en todo lenguaje de programación, a todo lo que le pongamos un nombre (variables, funciones, datos) hay que ponerle un buen nombre.\n¡Siempre usa scripts! (si nunca antes has creado un ‘programa para un computador’ en este curso lo vas a hacer)\nSi aparecen errores, se puede copiar y pegar el error en Google, ChatGPT o Gemini.\nComentar el código anteponiendo el símbolo “#” en la línea donde se desea insertar el comentario. Este proceso se puede hacer apoyándose de herramientas de programación asistida.\nA R podemos preguntarle cosas.\n\n\n?ayuda\n?cualquier_cosa\n?mean\n??mean\n\n\n\n\nR cuenta con un paquete llamado knitr con el cual podemos generar dinámicamente reportes, análisis y documentos. La técnica es la misma para darle formato a los cuadernos de Jupyter/notebooks de Google Colab que ven en el curso de Python.\n\nElegant, flexible, and fast dynamic report generation with R\nRMarkdwon\n\n\n\n\n\n\n\nAbrir RStudio.\nClic en File &gt; New Project\n\nNew Directory &gt; New Project si queremos crear una nueva carpeta en nuestro equipo.\nExisting Directory si queremos alojar nuestro proyecto en una carpeta ya creada.\n\nUna vez creado el proyecto, podemos verificar que la ventana del programa se renombra y apunta a la carpeta donde está alojado.\nClic en File &gt; New File &gt; R Script. En la ventana que se carga ya podemos empezar a ejecutar nuestro código."
  },
  {
    "objectID": "program_00_setup.html#qué-es-r",
    "href": "program_00_setup.html#qué-es-r",
    "title": "Configuración del ambiente de trabajo en R",
    "section": "",
    "text": "R es un lenguaje de programación de acceso libre, inicialmente diseñado y utilizado para realizar análisis estadístico.\nR también es un programa que instalamos para interpretar el código que escribimos.\nNo fue creado por ingenieros de software para el desarrollo de software, sino por estadísticos como un ambiente interactivo para el análisis de datos.\nAdemás de R trabajaremos con RStudio, un IDE (Integrated Development Environment) que funciona como una “máscara” sobre R, con más herramientas y facilidades.\nLa forma en que trabajaremos en R será a través de secuencias de comandos, conocidos como script, que se pueden ejecutar en cualquier momento. Estos scripts sirven como un registro del análisis realizamos."
  },
  {
    "objectID": "program_00_setup.html#por-qué-nos-gusta",
    "href": "program_00_setup.html#por-qué-nos-gusta",
    "title": "Configuración del ambiente de trabajo en R",
    "section": "",
    "text": "Se ejecuta en todos los sistemas operativos principales: Windows, Mac Os, UNIX/Linux.\nLos scripts y los objetos de datos se pueden compartir sin problemas entre plataformas.\nNo solamente permite realizar análisis estadístico, tambien es posible capturar información de páginas web, crear aplicaciones web, procesar texto, entre otras cosas.\nLa comunidad que lo usa es muy amplia (= muchísima ayuda e información en internet).\n\nStackoverflow\nR-Universe\nR-Bloggers\nTowards Data Science\nEntre otros.\n\nEs muy versátil para la creación de gráficos.\nTiene disponibles muchos paquetes para diferentes tipos de análisis.\n¡ES GRATIS!."
  },
  {
    "objectID": "program_00_setup.html#particularmente-útil-para",
    "href": "program_00_setup.html#particularmente-útil-para",
    "title": "Configuración del ambiente de trabajo en R",
    "section": "",
    "text": "Manejar, combinar, limpiar y reorganizar datos.\nAnálisis estadístico, álgebra matricial, modelado, estadística avanzada.\nUsar un entorno gráfico poderoso para explorar datos o para publicar.\nTrabajar de forma reproducible a partir de scripts."
  },
  {
    "objectID": "program_00_setup.html#advertencias",
    "href": "program_00_setup.html#advertencias",
    "title": "Configuración del ambiente de trabajo en R",
    "section": "",
    "text": "La documentación a veces es muy técnica o muy compacta.\nNo todo el código ha sido exhaustivamente testeado, es decir, no siempre hay garantías que las cosas funcionen.\nLa curva de aprendizaje es más difícil que programas como SPSS o Minitab (frente a Python, Stata o SAS es similar)."
  },
  {
    "objectID": "program_00_setup.html#instalación-de-r",
    "href": "program_00_setup.html#instalación-de-r",
    "title": "Configuración del ambiente de trabajo en R",
    "section": "",
    "text": "https://cran.r-project.org/\n\n\nLuego de instalar R, se recomienda instalar Rtools."
  },
  {
    "objectID": "program_00_setup.html#interfaces-de-usuario",
    "href": "program_00_setup.html#interfaces-de-usuario",
    "title": "Configuración del ambiente de trabajo en R",
    "section": "",
    "text": "Por defecto\nRStudio - Recomendada. Es la que usaremos en este curso.\nJupyter\nEclipse\nEntre otras"
  },
  {
    "objectID": "program_00_setup.html#front-ends-gráficos",
    "href": "program_00_setup.html#front-ends-gráficos",
    "title": "Configuración del ambiente de trabajo en R",
    "section": "",
    "text": "Rcmdr\nRKWard"
  },
  {
    "objectID": "program_00_setup.html#cómo-correr-comandoscódigo",
    "href": "program_00_setup.html#cómo-correr-comandoscódigo",
    "title": "Configuración del ambiente de trabajo en R",
    "section": "",
    "text": "Si se está trabajando en el panel console solamente es necesario presionar “Enter”.\nSi se está trabajando en un script es posible ejecutar los comandos presionando “Ctrl+Enter”, o haciendo clic en el botón Run en la parte derecha superior del panel de edición."
  },
  {
    "objectID": "program_00_setup.html#grupos-de-funciones-paquetes",
    "href": "program_00_setup.html#grupos-de-funciones-paquetes",
    "title": "Configuración del ambiente de trabajo en R",
    "section": "",
    "text": "Cuando varias funciones son desarrolladas con un objetivos similar se suelen agrupar en paquetes, los cuales son colaborativamente distribuidos de forma gratuita.\nPara utilizar las funciones de un paquete es necesario instalar este paquete primero usando install.packages() y luego cargarlo en nuestro ambiente de trabajo usando library().\nSe instalan una vez y se llaman muchas veces.\nLista de paquetes del CRAN\nLista de paquetes de Bioconductor\nrdrr.io"
  },
  {
    "objectID": "program_00_setup.html#actualizar-r-si-ya-lo-tenías-instalado-y-los-paquetes",
    "href": "program_00_setup.html#actualizar-r-si-ya-lo-tenías-instalado-y-los-paquetes",
    "title": "Configuración del ambiente de trabajo en R",
    "section": "",
    "text": "# Instalar el paquete installr\ninstall.packages(\"installr\")\n# Cargar el paquete\nlibrary(installr) \n# Comando para actualizar R\nupdateR()\n# Comando para actualizar paquetes\nupdate.packages(checkBuilt = TRUE, ask = FALSE)"
  },
  {
    "objectID": "program_00_setup.html#buenas-prácticas",
    "href": "program_00_setup.html#buenas-prácticas",
    "title": "Configuración del ambiente de trabajo en R",
    "section": "",
    "text": "Usar projects (proyectos) para ajustar el ambiente de trabajo para un proyecto.\nPor reproducibilidad, facilidad y para tener los proyectos mejor organizados es mucho mejor usar projects.\nLa idea es que cada proyecto tenga su propio espacio y esté aislado, de tal manera que los recursos y códigos de uno no interfieran con otros proyectos.\nComo en todo lenguaje de programación, a todo lo que le pongamos un nombre (variables, funciones, datos) hay que ponerle un buen nombre.\n¡Siempre usa scripts! (si nunca antes has creado un ‘programa para un computador’ en este curso lo vas a hacer)\nSi aparecen errores, se puede copiar y pegar el error en Google, ChatGPT o Gemini.\nComentar el código anteponiendo el símbolo “#” en la línea donde se desea insertar el comentario. Este proceso se puede hacer apoyándose de herramientas de programación asistida.\nA R podemos preguntarle cosas.\n\n\n?ayuda\n?cualquier_cosa\n?mean\n??mean"
  },
  {
    "objectID": "program_00_setup.html#informes-automáticos",
    "href": "program_00_setup.html#informes-automáticos",
    "title": "Configuración del ambiente de trabajo en R",
    "section": "",
    "text": "R cuenta con un paquete llamado knitr con el cual podemos generar dinámicamente reportes, análisis y documentos. La técnica es la misma para darle formato a los cuadernos de Jupyter/notebooks de Google Colab que ven en el curso de Python.\n\nElegant, flexible, and fast dynamic report generation with R\nRMarkdwon"
  },
  {
    "objectID": "program_00_setup.html#creación-de-nuestro-primer-proyecto",
    "href": "program_00_setup.html#creación-de-nuestro-primer-proyecto",
    "title": "Configuración del ambiente de trabajo en R",
    "section": "",
    "text": "Abrir RStudio.\nClic en File &gt; New Project\n\nNew Directory &gt; New Project si queremos crear una nueva carpeta en nuestro equipo.\nExisting Directory si queremos alojar nuestro proyecto en una carpeta ya creada.\n\nUna vez creado el proyecto, podemos verificar que la ventana del programa se renombra y apunta a la carpeta donde está alojado.\nClic en File &gt; New File &gt; R Script. En la ventana que se carga ya podemos empezar a ejecutar nuestro código."
  },
  {
    "objectID": "program_01_loading-data.html",
    "href": "program_01_loading-data.html",
    "title": "Carga de datos",
    "section": "",
    "text": "Conjunto de datos\n\n\nUna vez descargados los archivos de datos, vamos a ubicarlos al interior de una carpeta llamada data que a su vez crearemos dentro del directorio de nuestro proyecto.\n\n\n\n\n# Instalación del paquete tidyverse\ninstall.packages(\"tidyverse\")\n# Si hay problemas de compatibilidad\n# install.packages(\"tidyverse\", dependencies=TRUE, INSTALL_opts = c(\"--no-multiarch\"))\n# Instalación del paquete DBI\ninstall.packages(\"DBI\")\n# Instalación del paquete RMariaDB\ninstall.packages(\"RMariaDB\")\n\ninstall.packages(\"readxl\")\n\n\n# Cargamos los paquetes\nlibrary(\"tidyverse\")\nlibrary(\"DBI\")\nlibrary(\"RMariaDB\")\nlibrary(\"readxl\")\n\nNote que tidyverse es una colección de paquetes ampliamente usada en la ciencia de datos en R, por lo que al instalarlo se instalan a su vez numerosos paquetes que son útiles para cargar, manipular, limpiar, procesar, modelar y visualizar datos.\nVeámonos cómo cargar datos en distintos formatos.\n\n\n\n\n# Una forma de declararlo usando la función read_delim\nread_delim(\n  file = \"01_data/programacion/DataObesidad.txt\",\n  delim = \"|\", \n  locale=locale(decimal_mark = \".\")\n) -&gt; data_txt\n\n# Otra función para leer los datos usando la función read.table\nread.table(\n  file =  \"01_data/programacion/DataObesidad.txt\",\n  sep =  \"|\",\n  header = TRUE,\n  dec = \".\",\n  fileEncoding = \"UTF-8\"\n  ) -&gt; data_txt\n\n\n# Herramientas para examinar los datos\nstr(data_txt)\n\n\n# Herramientas para examinar los datos\nglimpse(data_txt)\n\nAl importar archivos de texto plano revisar:\n\nSímbolo separador\nSímbolo decimal\nCodificación\n\n\n\n\n\nread.csv(\n  file = \"01_data/programacion/DataObesidad.csv\"\n) -&gt; data_csv\n\nstr(data_csv)\n\nAl importar archivos separados por comas revisar:\n\nSímbolo separador (no siempre se separan con comas)\nSímbolo decimal\nCodificación\n\n\n\n\n\nread_excel(\n  path = \"01_data/programacion/DataObesidad.xls\"\n) -&gt; data_xls\n\nstr(data_xls)\n\nread_xlsx(\n  path = \"01_data/programacion/DataObesidad.xlsx\", \n  sheet= \"Obesidad\"\n) -&gt; data_xlsx\n\nstr(data_xlsx)\n\nAl importar datos desde Excel, asegúrese de especificar el nombre exacto de la hoja que contiene sus datos brutos.\n\n\n\n\nlibrary(\"haven\")\nread_dta(\n  file = \"data/DataObesidad.dta\"\n) -&gt; data_dta\n\nstr(data_dta)\n\n\n\n\n\nlibrary(\"haven\")\nread_sav(\n  file = \"data/DataObesidad.sav\"\n) -&gt; data_sav\n\nstr(data_sav)\n\n\n\n\nPodemos leer desde R hojas de cálculo públicas o privadas.\n\nHojas públicas\n\n\nlibrary(\"googlesheets4\")\n# Comando para leer sin autenticación\ngs4_deauth()\n# Declaro el enlace\nlink = \"https://docs.google.com/spreadsheets/d/1iM7i0nd3EPMzH79oZEo48nIcWbyT14ThsJ8ZO9r3F4Y/edit?usp=sharing\"\n# Lectura de datos\ndata_gs &lt;- read_sheet(link)\n\nstr(data_gs)\n\n\nHojas privadas\n\n\n# library(\"googlesheets4\")\n# Comando para realizar la autenticación vía web\ngs4_auth()\n# Declaro el enlace\nlink = \"https://docs.google.com/spreadsheets/d/1iM7i0nd3EPMzH79oZEo48nIcWbyT14ThsJ8ZO9r3F4Y/edit?usp=sharing\"\n# Lectura de datos\ndata_gs &lt;- read_sheet(link)\n\nstr(boston_housing_gs)"
  },
  {
    "objectID": "program_01_loading-data.html#importar-datos-a-r",
    "href": "program_01_loading-data.html#importar-datos-a-r",
    "title": "Carga de datos",
    "section": "",
    "text": "Conjunto de datos\n\n\nUna vez descargados los archivos de datos, vamos a ubicarlos al interior de una carpeta llamada data que a su vez crearemos dentro del directorio de nuestro proyecto.\n\n\n\n\n# Instalación del paquete tidyverse\ninstall.packages(\"tidyverse\")\n# Si hay problemas de compatibilidad\n# install.packages(\"tidyverse\", dependencies=TRUE, INSTALL_opts = c(\"--no-multiarch\"))\n# Instalación del paquete DBI\ninstall.packages(\"DBI\")\n# Instalación del paquete RMariaDB\ninstall.packages(\"RMariaDB\")\n\ninstall.packages(\"readxl\")\n\n\n# Cargamos los paquetes\nlibrary(\"tidyverse\")\nlibrary(\"DBI\")\nlibrary(\"RMariaDB\")\nlibrary(\"readxl\")\n\nNote que tidyverse es una colección de paquetes ampliamente usada en la ciencia de datos en R, por lo que al instalarlo se instalan a su vez numerosos paquetes que son útiles para cargar, manipular, limpiar, procesar, modelar y visualizar datos.\nVeámonos cómo cargar datos en distintos formatos.\n\n\n\n\n# Una forma de declararlo usando la función read_delim\nread_delim(\n  file = \"01_data/programacion/DataObesidad.txt\",\n  delim = \"|\", \n  locale=locale(decimal_mark = \".\")\n) -&gt; data_txt\n\n# Otra función para leer los datos usando la función read.table\nread.table(\n  file =  \"01_data/programacion/DataObesidad.txt\",\n  sep =  \"|\",\n  header = TRUE,\n  dec = \".\",\n  fileEncoding = \"UTF-8\"\n  ) -&gt; data_txt\n\n\n# Herramientas para examinar los datos\nstr(data_txt)\n\n\n# Herramientas para examinar los datos\nglimpse(data_txt)\n\nAl importar archivos de texto plano revisar:\n\nSímbolo separador\nSímbolo decimal\nCodificación\n\n\n\n\n\nread.csv(\n  file = \"01_data/programacion/DataObesidad.csv\"\n) -&gt; data_csv\n\nstr(data_csv)\n\nAl importar archivos separados por comas revisar:\n\nSímbolo separador (no siempre se separan con comas)\nSímbolo decimal\nCodificación\n\n\n\n\n\nread_excel(\n  path = \"01_data/programacion/DataObesidad.xls\"\n) -&gt; data_xls\n\nstr(data_xls)\n\nread_xlsx(\n  path = \"01_data/programacion/DataObesidad.xlsx\", \n  sheet= \"Obesidad\"\n) -&gt; data_xlsx\n\nstr(data_xlsx)\n\nAl importar datos desde Excel, asegúrese de especificar el nombre exacto de la hoja que contiene sus datos brutos.\n\n\n\n\nlibrary(\"haven\")\nread_dta(\n  file = \"data/DataObesidad.dta\"\n) -&gt; data_dta\n\nstr(data_dta)\n\n\n\n\n\nlibrary(\"haven\")\nread_sav(\n  file = \"data/DataObesidad.sav\"\n) -&gt; data_sav\n\nstr(data_sav)\n\n\n\n\nPodemos leer desde R hojas de cálculo públicas o privadas.\n\nHojas públicas\n\n\nlibrary(\"googlesheets4\")\n# Comando para leer sin autenticación\ngs4_deauth()\n# Declaro el enlace\nlink = \"https://docs.google.com/spreadsheets/d/1iM7i0nd3EPMzH79oZEo48nIcWbyT14ThsJ8ZO9r3F4Y/edit?usp=sharing\"\n# Lectura de datos\ndata_gs &lt;- read_sheet(link)\n\nstr(data_gs)\n\n\nHojas privadas\n\n\n# library(\"googlesheets4\")\n# Comando para realizar la autenticación vía web\ngs4_auth()\n# Declaro el enlace\nlink = \"https://docs.google.com/spreadsheets/d/1iM7i0nd3EPMzH79oZEo48nIcWbyT14ThsJ8ZO9r3F4Y/edit?usp=sharing\"\n# Lectura de datos\ndata_gs &lt;- read_sheet(link)\n\nstr(boston_housing_gs)"
  },
  {
    "objectID": "program_01_loading-data.html#usando-la-interfaz-de-rstudio",
    "href": "program_01_loading-data.html#usando-la-interfaz-de-rstudio",
    "title": "Carga de datos",
    "section": "Usando la interfaz de RStudio",
    "text": "Usando la interfaz de RStudio\n\n\nYa sabemos importar datos."
  },
  {
    "objectID": "program_02_basics.html",
    "href": "program_02_basics.html",
    "title": "Programación básica en R",
    "section": "",
    "text": "Ya aprendimos a importar datos."
  },
  {
    "objectID": "program_02_basics.html#algoritmos",
    "href": "program_02_basics.html#algoritmos",
    "title": "Programación básica en R",
    "section": "Algoritmos",
    "text": "Algoritmos\nUn algoritmo es un conjunto finito de instrucciones que, si se siguen rigurosamente, llevan a cabo una tarea específica.\nTodos los algoritmos se componen de “partes” básicas que se utilizan para crear “partes” más complejas.\n\n\nEl tratamiento, análisis y modelado de datos lo haremos mediante algoritmos."
  },
  {
    "objectID": "program_02_basics.html#variables-y-datos",
    "href": "program_02_basics.html#variables-y-datos",
    "title": "Programación básica en R",
    "section": "Variables y datos",
    "text": "Variables y datos\n\nVariables: característica observable o un aspecto discernible en un objeto de estudio, que puede adoptar diferentes valores o expresarse en varias categorías.\nDato: realización, representación o valor observado de una variable."
  },
  {
    "objectID": "program_02_basics.html#tipos-de-variables-y-operaciones",
    "href": "program_02_basics.html#tipos-de-variables-y-operaciones",
    "title": "Programación básica en R",
    "section": "Tipos de variables y operaciones",
    "text": "Tipos de variables y operaciones\n\nEnfoque teórico\n\n\n\nEnfoque desde la programación\n\n\n\nAsignación\n\n# Una forma de hacer asignación\nobjeto &lt;- \"valor\"\n# Otra forma\nobjeto = \"valor\"\n# Otro forma\n\"valor\" -&gt; objeto\n# Ejemplos\npais = \"Colombia\"\ndepartamentos = 32\ntrm = 4000.5\ntenemos_mar = TRUE\n\n\n\nTipos de variables\n\nBooleanos (lógicos)\n\nVERDADERO\nFALSO\n\n\n\nobjeto_nombrado_por_mi &lt;- TRUE # Siempre con mayúsculas\nobjeto_nombrado_por_mi\nclass(objeto_nombrado_por_mi)\nis.logical(objeto_nombrado_por_mi)\n\n\nNuméricos\n\nEnteros\nReales\n\n\n\npi\nobjeto_nombrado_por_mi &lt;- 0\nobjeto_nombrado_por_mi\nclass(objeto_nombrado_por_mi)\nis.numeric(objeto_nombrado_por_mi)\n\n\nAlfanuméricos: caracteres o cadenas de texto\n\n\nobjeto_nombrado_por_mi &lt;- \"hola mundo\"\nobjeto_nombrado_por_mi\nclass(objeto_nombrado_por_mi)\nis.character(objeto_nombrado_por_mi)\n\n\nFechas\n\n\nobjeto_nombrado_por_mi &lt;- \"1969-07-21\" # Recomendado: ISO 8601 para fechas\nobjeto_nombrado_por_mi\nclass(objeto_nombrado_por_mi)\notro_objeto_distinto &lt;- as.Date(objeto_nombrado_por_mi)\nclass(otro_objeto_distinto)\n\n\nNA\n\n\nobjeto_nombrado_por_mi &lt;- NA # Siempre en mayúsculas NA\nobjeto_nombrado_por_mi\nclass(objeto_nombrado_por_mi)\n\n\n\nMétodo is\n\ncualquier_cosa &lt;- TRUE\nis.logical(cualquier_cosa)\nis.numeric(cualquier_cosa)\nis.character(cualquier_cosa)\n\n\n\nMétodo as\n\nTRUE -&gt; true_logico\ntrue_logico\nclass(true_logico)\nas.character(true_logico) -&gt; true_char\ntrue_char\nclass(true_char)\n\n1 -&gt; uno_numeric\nuno_numeric\nclass(uno_numeric)\nas.character(uno_numeric) -&gt; uno_char\nuno_char\nclass(uno_char)\n\n\n\nConversiones\n\n\n\nDesde\nHacia\n\n\n\n\nlogical\nnumeric\n\n\nlogical\ncharacter\n\n\nnumeric\ncharacter\n\n\nnumeric\nDate\n\n\ncharacter\nDate\n\n\n\n\n\nOperadores matemáticos\n\n2+2\n5-2\n3*4\n5/4\n9 %% 2\n3 ** 3\n3 ^ 3\nlog(10)\nsqrt(16)\n\n\n\nOperadores para comparación\n\n5 &gt; 2\n5 &lt; 2\n10 == 10\n10 == 9\n10 != 9\n10 &gt;= 10\n10 &lt;= 8\n\n\n\nOperadores lógicos\n\nConjunción (se cumplen ambas): &&\nDisyunción (se cumple alguna): ||\nNegación (lo contrario): !\n\n\n\nOrden de las operaciones\n\nPEMDAS\n\nParéntesis\nExponentes\nMultiplicaciones / Divisiones\nAdición / Sustracción"
  },
  {
    "objectID": "program_02_basics.html#variables",
    "href": "program_02_basics.html#variables",
    "title": "Programación básica en R",
    "section": "Variables",
    "text": "Variables\n\npi = 3.1415\nradio = 3\narea = pi * radio**2\narea\nround(area, 2)\n# Aplicado en una función\nlibrary(\"readxl\")\nruta = \"01_data/programacion/DataObesidad.xlsx\"\nhoja = \"Obesidad\"\nread_xlsx(\n  path = ruta, \n  sheet= hoja\n) -&gt; data_xlsx"
  },
  {
    "objectID": "program_02_basics.html#vectores-matrices-arreglos-listas-y-tablas",
    "href": "program_02_basics.html#vectores-matrices-arreglos-listas-y-tablas",
    "title": "Programación básica en R",
    "section": "Vectores, matrices, arreglos, listas y tablas",
    "text": "Vectores, matrices, arreglos, listas y tablas\n\nVectores: Arreglos lineales del mismo tipo. Tienen la misma clase.\nMatrices: Arreglos rectangulares del mismo tipo. Sábanas de la misma clase.\nArreglos: Organizaciones cúbicas y de mayor dimensión.\nListas: Arreglos lineales de distintos tipos.\nTablas: La estructura data.frame permite tener tablas de datos, donde cada columna es de un tipo determinado, pero no todas iguales."
  },
  {
    "objectID": "program_02_basics.html#un-tipo-especial-de-variables-factores",
    "href": "program_02_basics.html#un-tipo-especial-de-variables-factores",
    "title": "Programación básica en R",
    "section": "Un tipo especial de variables: factores",
    "text": "Un tipo especial de variables: factores\nSon vectores numéricos enmascarados como caracteres. Se usan para crear grupos usando clasificaciones o codificaciones de las variables de interés. Estos factores pueden o no tener un orden.\nEjemplos: estrato socioeconómico, nivel de estudios, mes, sexo, localidad.\n\nVectores\n\n1:5\nletters\nLETTERS\nc(1, 3, 2, 15, 4, 0, 0, 0, 1)\nseq(10, 100)\nseq(10, 100, by = 5)\nseq(10, 100, length.out = 8)\n\n\n\nFactores\n\nas.factor(letters)\nestrato = c(2,3,4,1,3,6,5,2,3,4,1,2,3,4,6)\nestrato\nestrato.factor = factor(estrato)\nestrato.factor\nestrato.factor.ordenado = factor(estrato, levels=c(1,2,3,4,5,6))\nestrato.factor.ordenado\n\n\n\nFunciones sobre vectores\n\nvector_logico &lt;- c(TRUE,FALSE,FALSE,TRUE,FALSE)\nvector_cualquiera &lt;- seq(1, 100, by = 3)\nun_vector &lt;- c(1, 2, 3, 4, 5)\notro_vector &lt;- c(6, 7, 8, 9, 10)\n\n\nwhich(vector_logico) # me dice cuales son los verdaderos \nlength(vector_cualquiera) # me dice cuánto mide el vector\nc(un_vector, otro_vector) # concatena los vectores\n\n\n\nOperaciones entre vectores\n\nvector_numerico &lt;- c(2, 4, 6, 8, 10)\nvector_numeric_1 &lt;- 1:3\nvector_numeric_2 &lt;- 3:5\n\n\nvector_numerico &gt; 3\n1:5 %in% 3:8\nouter(vector_numeric_1, vector_numeric_2, \"*\")\nouter(vector_numeric_1, vector_numeric_2, \"&gt;\")\n\n\n\nOperaciones entre vectores (conjuntos)\n\nunion(vector_numeric_1, vector_numeric_2)\nintersect(vector_numeric_1, vector_numeric_2)\nsetdiff(vector_numeric_1, vector_numeric_2)\n\n\n\nMatrices\n\nmatrix(data = 1:12, nrow = 3)\nmatrix(data = 1:12, nrow = 6)\nmatrix(data = 1:12, ncol = 6)\nmatrix(data = 1:12, nrow = 4)\nmatrix(data = 1:12, nrow = 4, byrow = TRUE)\nmatrix(data = seq(0, 9, length.out = 4), nrow = 2) -&gt; mi_matriz\nmi_matriz\n\n\n\nOperaciones sobre matrices\n\notra_matriz # Toca inventársela\nmi_matriz*2 # Producto por un escalar\nmi_matriz + otra_matriz # Suma de matrices\nmi_matriz*otra_matriz # Producto celda por celda\nmi_matriz %*% otra_matriz # Producto de matrices\n\n\n\nFunciones sobre matrices\n\nt(mi_matriz) #mi_matriz transpuesta\ndiag(mi_matriz) #Diagonal de mi_matriz\ndet(mi_matriz) # Determinante, debe dar un número\nsolve(mi_matriz) # Matriz inversa, sólo se puede con matrices cuadradas de determinante distinto de cero\ndim(mi_matriz) # Dimensión de mi matriz\n\n\nBibliografía complementaria: Parte 1 Capítulo 2: Linear Algebra, del libro Deep Learning del MIT\n\n\n\nTablas\n\nTidy tables\n\n\nlibrary(\"tidyverse\")\niris\n?iris\ndiamonds\n?diamonds\nmpg\n?mpg\n\nclass(diamonds)\nclass(mpg)\n\nstr(diamonds)\nstr(mpg)\n\nView(diamonds)\nView(mpg)\n\n\n\nExtracción [.\n\ncuales_extraer &lt;- c(1, 8, 6, 3) # Creo un vactor con las posiciones que deseo extraer\nletters[cuales_extraer] # Extrae las letras 1, 8, 6, 3 del vector letters\nvector_numerico[vector_numerico &gt; 3] # Extrae los valores mayores a 3 en vector_numerico\nun_vector[1] # Extrae el elemento #1 del vector un_vector\nmi_matriz[1,2] # Extrae el valor en la fila 1 columna 2 de mi_matriz\nmi_matriz[,1] # Extrae la primera columna de mi_matriz\nmi_matriz[2,] # Extrae la segunda fila mi_matriz\ndiamonds[,8] # Extrae la fila 8 de diamonds\ndiamonds[\"x\"] # Extrae de diamonds la columna llamada \"x\"\ncuales_extraer = c(\"x\",\"y\",\"z\") # Creo un vector de variables a extraer\ndiamonds[cuales_extraer] #Hago la extracción\n\n\n\nEjemplo: Prueba T\nA partir de la base de datos evaluacion, hagamos una prueba de hipótesis para testear si el puntaje obtenido en ciencias (variable ciencias) está influenciado/afectado por el sexo (variable sexo).\nNota: cuando una variable toma dos valores se puede recodificar como una variable dummy.\n\n# Cargo los datos de evaluacion y lo guardo en un objeto llamado evaluacion_xlsx\nread_xlsx(\n  path = \"01_data/programacion/evaluacion.xlsx\", \n  sheet= \"datos\"\n) -&gt; evaluacion_xlsx\n# Hago la prueba t\nt.test(ciencias ~ sexo, data = evaluacion_xlsx) -&gt; t_test_ciencias_sexo\n# Llamo los resultados de la prueba t\nt_test_ciencias_sexo\n\n¿Qué podríamos extraer de este objeto?\n\nstr(t_test_ciencias_sexo)\n\nExtraigamos el p-valor de la prueba.\n\nt_test_ciencias_sexo[\"p.value\"] \nt_test_ciencias_sexo[[\"p.value\"]]\n# Otra forma\nt_test_ciencias_sexo$p.value\n\n\nPodemos extraer partes de todos los objetos que tengamos en nuestro ambiente de trabajo.\n\n\n\nEjemplo: Modelo de regresión lineal\nAjustemos un modelo de regresión lineal simple usando como variable respuesta el puntaje obtenido en humanidades (variable humanidades) en función del puntaje obtenido en ciencias (variable ciencias).\n\nlm(humanidades ~ ciencias, data = evaluacion_xlsx) -&gt; modelo_humanidades_ciencias\nmodelo_humanidades_ciencias\n\n\n\nsummary(modelo_humanidades_ciencias)\n\n¿Qué podríamos extraer de este objeto?\n\nstr(summary(modelo_humanidades_ciencias))\n\nExtraigamos el \\(R^2\\) ajustado del modelo.\n\nsummary(modelo_humanidades_ciencias)$adj.r.squared"
  },
  {
    "objectID": "program_02_basics.html#control-flow",
    "href": "program_02_basics.html#control-flow",
    "title": "Programación básica en R",
    "section": "Control flow",
    "text": "Control flow\nEl control flow es un conjunto de funciones que permiten manejar las órdenes de manera estructurada y lógica. Las más importantes son:\n\nif\nif - else\nfor\nwhile\nrepeat\nbreak\nnext\n\n\n?Control\n\n\nLoops\nTodos los lenguajes modernos de programación ofrecen una o más maneras de realizar operaciones iterativas. El poder repetir la misma acción una cantidad indefinida de veces es una de las grandes ventajas de realizar las tareas mediante programación."
  },
  {
    "objectID": "program_02_basics.html#for",
    "href": "program_02_basics.html#for",
    "title": "Programación básica en R",
    "section": "for",
    "text": "for\nSirve para crear tareas repetitivas de un número de pasos específico.\nUno de los usos más frecuentes de un ciclo for es la configuración de métodos de remuestreo (bootstraping).\n\n\n#vamos a guardar en una lista los coeficientes de una regresión\ncoeficientes &lt;- list()\n\n# inicializo el ciclo for\nfor(i in 1:1000){\n  #en cada paso\n  \n  # 1. saco una muestra de 30 estudiantes\n  muestra &lt;- sample_n(evaluacion_xlsx, 30)\n  \n  # 2. ajusto un modelo de regresión lineal\n  lm(humanidades ~ ciencias, data = muestra) -&gt; modelo\n  \n  # 3. extraigo y almaceno los coeficientes del modelo\n  coeficientes[[i]] &lt;- coefficients(modelo)\n}\n\n# grafico el comportamiento de los coeficientes\ncoeficientes %&gt;% \n  transpose %&gt;% \n  lapply(unlist) %&gt;% \n  as_tibble() %&gt;% \n  gather(key = coeficiente, value = valor) %&gt;% \n  ggplot +\n  aes(x = valor) + \n  geom_density() +\n  facet_wrap(~coeficiente, nrow = 2,  scales = \"free\")"
  },
  {
    "objectID": "program_02_basics.html#while",
    "href": "program_02_basics.html#while",
    "title": "Programación básica en R",
    "section": "while",
    "text": "while\nSirve para crear tareas repetitivas que no sabemos después de cuántos pasos terminan. Requiere una inicialización cuidadosa.\nEjemplo: ¿Cuántos sobres tengo que comprar para llenar un álbum de 100 cromos?\n\n# inicializo las condiciones de partida\nalbum &lt;- iteracion &lt;- 0\n# creo la condición lógica que permite ejecutar el proceso\naun_falta &lt;- TRUE\n\n# siempre que aun_falta siga siendo verdadero\nwhile(aun_falta){\n  # en cada ciclo\n  \n  # 1. actualizo en qué iteración voy\n  iteracion &lt;- iteracion + 1\n  \n  # 2. extraigo una muestra de 6 números (\"compro un sobre con 6 cromos\")\n  sobre &lt;- sample(100, 6)\n  \n  # 3.1 tomo el álbum\n  # 3.2 le combino los cromos que obtuve\n  # 3.3 ordeno los cromos de menor a mayor\n  # 3.4 dejo valores únicos (quito cromos duplicados)\n  # 3.5 actualizo el álbum\n  album %&gt;% c(sobre) %&gt;% sort %&gt;% unique -&gt; album\n  \n  # 4. si tengo menos de 100 cromos es porque me falta\n  length(album) &lt; 100 -&gt; aun_falta\n}\n\n# muestro el número de iteraciones\n# es decir, cuántos sobres tuve que comprar\niteracion"
  },
  {
    "objectID": "program_02_basics.html#if-else",
    "href": "program_02_basics.html#if-else",
    "title": "Programación básica en R",
    "section": "if, else",
    "text": "if, else\nLa estructura if sirve para ejecutar varias rutinas distintas dependiendo de una condición lógica. En caso de que sea necesario, es posible aplicar una rutina alterna con la estructura else.\nEjemplo: Prueba de normalidad.\nDiversas pruebas y modelos estadísticos requieren verificar el supuesto de normalidad en los datos.\n\n\n# cargo la base de datos del PGN y la almaceno en un objeto llamado pgn\nread_xlsx(\n  path = \"01_data/programacion/Base de datos PGN 2024.xlsx\", \n  sheet= \"Data\"\n) -&gt; pgn\n# extraigo la variable Funcionamiento\n# le hago un test de shapiro\n# guardo los resultados de la prueba en un objeto llamado prueba_sw\npgn[[\"Funcionamiento\"]] %&gt;% shapiro.test() -&gt; prueba_sw\n\n# estructura condicional\nif(prueba_sw$p.value &gt; 0.05){\n  # Si acepto la hipótesis de normalidad en la variable mpg\n  # Hago una prueba t\n  print(\"La variable Funcionamiento sigue una distribución normal\")\n  print(\"Realizo una prueba t\")\n  t.test(mpg ~ vs, data = mtcars)\n} else {\n  # Si rechazo la hipótesis de normalidad en la variable mpg\n  # Hago una prueba Mann-Whitney-Wilcoxon\n  print(\"La variable Funcionamiento no sigue una distribución normal\")\n  print(\"Realizo una prueba Mann-Whitney-Wilcoxon\")\n  wilcox.test(mpg ~ vs, data = mtcars)\n}"
  },
  {
    "objectID": "program_03_tidy.html#objetivos",
    "href": "program_03_tidy.html#objetivos",
    "title": "Limpieza de datos",
    "section": "Objetivos",
    "text": "Objetivos\nVamos a trabajar con datos tabulares. Existen 3 reglas que logran que un conjunto de datos tabulares esté “limpio”:\n\nCada variable debe tener su propia columna\nCada observación debe tener su propia fila\nCada valor debe estar en su propia celda"
  },
  {
    "objectID": "program_03_tidy.html#síntomas-de-datos-desordenados",
    "href": "program_03_tidy.html#síntomas-de-datos-desordenados",
    "title": "Limpieza de datos",
    "section": "Síntomas de datos desordenados",
    "text": "Síntomas de datos desordenados\n\nLos encabezados de las columnas son valores, no nombres de variables\nEn una misma columna se guardan múltiples valores\nLas variables están tanto en columnas como en filas\nMúltiples tipos de unidades observacionales se guardan en la misma tabla\nUna misma unidad observacional está alojada en múltiples tablas"
  },
  {
    "objectID": "program_03_tidy.html#buena-práctica-1-tener-información-de-contexto-del-problema-de-estudio",
    "href": "program_03_tidy.html#buena-práctica-1-tener-información-de-contexto-del-problema-de-estudio",
    "title": "Limpieza de datos",
    "section": "Buena práctica #1: tener información de contexto del problema de estudio",
    "text": "Buena práctica #1: tener información de contexto del problema de estudio\nPara todos los conjuntos de datos con los que trabajemos es importante tener un contexto que nos brinde información sobre cómo fueron recolectados, en qué año(s), qué técnicas de muestreo y recolección se usaron para obtener las observaciones y qué personas o entidades fueron los responsables.\n\nSi los datos los recolectamos nosotros hablamos de información primaria y en nuestros reportes/informes debemos hacer explícito el contexto referido.\nSi los datos los obtuvimos de otra fuenta hablamos de información secundaria y en los reportes derivados igualmente debemos procurar obtener y referenciar el contexto referido para los datos."
  },
  {
    "objectID": "program_03_tidy.html#buena-práctica-2-construir-diccionario-de-datos",
    "href": "program_03_tidy.html#buena-práctica-2-construir-diccionario-de-datos",
    "title": "Limpieza de datos",
    "section": "Buena práctica #2: construir diccionario de datos",
    "text": "Buena práctica #2: construir diccionario de datos\n\nConsiste en asignar atributos a cada variable de nuestras bases de datos.\nFacilita la lectura y tratamiento de los datos.\nAporta a la reproducibilidad y repetibilidad de los análisis.\n\nEjemplo:\n\n\nPara profundizar: revise el material sobre cómo limpiar tablas."
  },
  {
    "objectID": "program_03_tidy.html#práctica",
    "href": "program_03_tidy.html#práctica",
    "title": "Limpieza de datos",
    "section": "Práctica",
    "text": "Práctica\nEn esta práctica vamos a trabajar con la base de datos Boston Housing, y podemos consultar el contexto de dichos datos haciendo clic en este tunel secreto.\n¿Qué aprenderemos?\n\nDetección y tratamiento de datos faltantes\nDetección y tratamiento de datos atípicos\nNaming\nDetección y tratamiento de datos duplicados\nDiscretización de variables\n\nCarguemos los datos:\n\n# Cargamos los paquetes\nlibrary(\"tidyverse\")\nlibrary(\"readxl\")\n\n# Leemos los datos desde un archivo de Excel\nread_xlsx(\n  path = \"01_data/programacion/Boston_Housing.xlsx\", \n  sheet=\"Data\"\n) -&gt; boston_housing_xlsx\nstr(boston_housing_xlsx)\n\n# Convierto variables respectivas a factores\n# Esto es muy importante para procesos de limpieza de datos\nfactores &lt;- c(\"CHAS\")\nboston_housing_xlsx %&gt;% mutate_at(factores,factor) -&gt; boston_housing_xlsx\n\n\nEn cualquier escenario, es posible que tengamos datos faltantes. Veámos cómo abordar esta situación."
  },
  {
    "objectID": "program_03_tidy.html#datos-faltantes",
    "href": "program_03_tidy.html#datos-faltantes",
    "title": "Limpieza de datos",
    "section": "Datos faltantes",
    "text": "Datos faltantes\nLos datos faltantes (missing data) son un problema frecuente en todos los tipos de estudios y análisis, sin importar que el diseño sea muy estricto o que los investigadores/analistas traten de prevenirlo.\nEn ciencia de datos podemos realizar un proceso de imputación de datos, que consiste en asignar un valor a un ítem para el que previamente no se tenia información.\nExisten numerosos métodos de imputación de datos, entre otros:\n\nUsando la media de la variable\n\nLos datos faltantes de cada columna son reemplazados con la media de los datos disponibles de dicha. Útil para variables numéricas con distribuciones aproximadamente normales.\n\n\n\n\nHotDeck\n\nLos datos faltantes son reemplazados por valores tomados de registros similares. Útil en conjuntos de datos grandes.\n\n\n\n\nColdDeck\n\nLos datos faltantes son reemplazados por valores tomados de registros similares de una fuente externa de datos.\n\nAlgoritmos Random Forest\n\nLos datos faltantes son reemplazados a partir de ensamblar árboles de decisión. Útil en conjuntos de datos grandes.\n\nImputación múltiple - MICE\n\nLos datos faltantes son reemplazados mediante estimaciones obtenidas por una mezcla de métodos de simulación y ponderación de varianzas o errores estándar. Útil para automatizar procesos de limpieza de datos.\n\n\n\nVamos a retirar algunos datos de la base de forma aleatoria.\n\n# Instalamos el paquete mice\n# install.packages(\"mice\")\n\n# Cargamos mice\nlibrary(\"mice\")\n\n# Hacemos una copia de la base de datos en otro objeto llamado datos_completos\ndatos_completos &lt;- boston_housing_xlsx\n\n# \"amputamos\" datos usando el método MCAR: missing completely at random\nampute(datos_completos, prop = 0.5, mech = \"MCAR\", run = TRUE)$amp -&gt; datos_incompletos\n\n# Mapeamos el número de NAs por cada columna\ndatos_incompletos %&gt;% map_df(is.na) %&gt;% colSums()\n\nAhora que tenemos datos perdidos en la base de datos, algo que sucede con (mucha) frecuencia, debemos examinar el “comportamiento” de la pérdida de datos, esto es, identificar si existen patrones o situaciones que nos den indicio de por qué se perdieron los datos.\nPodemos tener tres patrones de pérdida de datos:\n\nMCAR (Missing completely at random) - Los datos se pierden de forma aleatoria.\nMAR (Missing at random) - Los datos se pierden de forma aleatoria pero es factible pensar en una variable observada que lo explique.\nNMAR (Not missing at random) - Los datos no se pierden de forma aleatoria\n\n\nEs deseable que si tenemos datos perdidos esto se deba al azar, y en ese caso, podríamos proceder a imputar los datos faltantes en las covariables (no es tan deseable en la variable objetivo/feature). Si tenemos patrones irregulares de pérdida de datos, sería un indicio de que podemos tener problemas en la captura, sistematización, almacenamiento o distribución de los datos.\n\nPodemos hacer distintas visualizaciones para entender el comportamiento de los datos perdidos.\n\n# Instalamos los paquetes naniar y VIM\n# install.packages(\"naniar\")\n# install.packages(\"VIM\")\n\n# Cargamos los paquetes\nlibrary(\"naniar\")\nlibrary(\"VIM\")\n\n# Visualización de patrones con naniar\n# Graficamos en orden descendente las variables con más datos perdidos\n# Nos muestra también si hay relaciones de pérdida de datos entre distintas variables\ngg_miss_upset(datos_incompletos)\n\n\n\n\n\n\n\n# Visualización de patrones con VIM\n# Graficamos la proporción de datos incompletos por variable\n# Nos muestra también si hay relaciones de pérdida de datos entre distintas variables\naggr(datos_incompletos,numbers=T,sortVar=T)\n\n\n\n\n\n\n\n# Visualización de patrones con mice\n# Se enfoca en mostrar si hay relaciones de pérdida de datos entre distintas variables\nmd.pattern(datos_incompletos, plot = TRUE, rotate.names = TRUE)\n\n\n\n\n\n\n\n\n\nAdvertencia: este tipo de análisis tienen sentido si por la naturaleza del problema esperamos tener todos los datos completos. Si hay columnas en donde es esperable tener datos faltantes (por ejemplo, por respuestas opcionales o variables que parten una muestra) deberíamos realizar la gráfica anterior solamente con las columnas (variables) de las que esperamos datos completos.\n\nHabiendo comprobado que en nuestros datos los valores perdidos se deben al azar y no superan umbrales de trabajo en ciencia de datos, podemos ahora así aplicar métodos de imputación de datos.\n\nImputación usando la media\n\n# Filtramos únicamente variables numéricas\ndatos_incompletos_num &lt;- Filter(is.numeric, datos_incompletos)\n# Llamamos el comando mice para imputar los datos\n# Asignamos unos parámetros de imputación\nmice(datos_incompletos_num, \n     m = 1,              # Número de imputaciones múltiples\n     maxit = 1,          # Número de iteraciones\n     method = \"mean\",    # Método de imputación\n     printFlag = FALSE) %&gt;% \n  mice::complete() -&gt; base_datos_imputados_promedios_mice\n\n# Verificamos que efectivamente ya no hayan datos faltantes\nbase_datos_imputados_promedios_mice %&gt;% map_df(is.na) %&gt;% colSums()\n\n\n\nImputación mediante Hotdeck\n\n# Llamamos el comando hotdeck\nhotdeck(datos_incompletos) -&gt; datos_imputados_hd\n\n# Verificamos que efectivamente ya no hayan datos faltantes\ndatos_imputados_hd[,c(1:ncol(datos_incompletos))] %&gt;% map_df(is.na) %&gt;% colSums()\n\n\n\nImputación medidante imputación múltiple\n\n# Llamamos el comando mice para imputar los datos\nmice(datos_incompletos, printFlag = FALSE) %&gt;% \n  mice::complete() -&gt; datos_imputados_mice\n\n# Verificamos que efectivamente ya no hayan datos faltantes\ndatos_imputados_mice %&gt;% map_df(is.na) %&gt;% colSums()"
  },
  {
    "objectID": "program_03_tidy.html#datos-atípicos",
    "href": "program_03_tidy.html#datos-atípicos",
    "title": "Limpieza de datos",
    "section": "Datos atípicos",
    "text": "Datos atípicos\nEn nuestros análisis debemos examinar la presencia de datos atípicos, en la medida que pueden afectar los resultados de las estimaciones, modelos y pruebas de hipótesis.\nPara detectar datos atípicos podemos seguir dos caminos:\n\nBuscar datos atípicos para cada variable (columna). Para esto podríamos usar criterios numéricos o gráficos.\nBuscar filas que consistentemente tengan datos atípicos en todas las celdas.\n\nVeamos dos ejemplos usando la base de datos de Boston Housing.\nPrimero, una detección de atípicos en una variable en específico (MEDV que es una variable objetivo).\n\n# Estadístico de resumen para una variable\nsummary(boston_housing_xlsx$MEDV)\n\n# Boxplot de una variable\nboxplot(boston_housing_xlsx$MEDV)\n\n# Valores de potenciales outliers\nboxplot.stats(boston_housing_xlsx$MEDV)$out\n\n# Filas donde se ubican las observaciones atípicas\noutliers &lt;- boxplot(boston_housing_xlsx$MEDV)$out\noutliers_filas &lt;- which(boston_housing_xlsx$MEDV %in% c(outliers))\noutliers_filas\n\nSegundo, una detección de atípicos basado en un análisis multivariado por medio de componentes principales (hay otras técnicas más, por ejemplo, la distancia de Cook).\n\n# Instalamos paquetes\n# install.packages(\"FactoMineR\")\n# install.packages(\"factoextra\")\n# install.packages(\"ggpubr\")\n# install.packages(\"magrittr\")\n\n# Cargamos paquetes\nlibrary(\"FactoMineR\")\nlibrary(\"factoextra\")\nlibrary(\"ggpubr\")\nlibrary(\"magrittr\")\n\n# Estandarizamos las variables\ndatos_numericos &lt;- Filter(is.numeric, datos_completos)\ndatos_numericos %&gt;%\n  mutate_all(scale) -&gt; data_estandarizada\n\n# Ajusto componentes principales usando el método PCA()\nacp = PCA(data_estandarizada, graph=F)\n\n\n# Gráfico de individuos del ACP\n# Esto puede tardar un poco\nfviz_pca_ind(acp, repel = TRUE)\n\n\nCon el siguiente código calculamos la distancia al vecino más cercano.\n\ndata_estandarizada %&gt;% \n  dist %&gt;% as.matrix() %&gt;% add(diag(Inf, ncol(.))) %&gt;% \n  apply(1, min) %&gt;% enframe() %&gt;% arrange(desc(value))\n\n\nPor medio de diversos procedimientos estadísticos podemos detectar datos atípicos. ¿Qué hacer con ellos? Depende. Hay que sopesar el contexto de los datos, la naturaleza del problema y de cada variable, así como combinar elementos de juicio estadístico como elementos de juicio profesionales de otras áreas."
  },
  {
    "objectID": "program_03_tidy.html#nombrado-adecuado-de-las-variables",
    "href": "program_03_tidy.html#nombrado-adecuado-de-las-variables",
    "title": "Limpieza de datos",
    "section": "Nombrado adecuado de las variables",
    "text": "Nombrado adecuado de las variables\nRecuerde que en programación los nombres importan (naming). Siguiendo ese marco de referencia, asegúrese de que las variables (columnas):\n\nNo tengan acentos\nNo tengan caracteres especiales (@, #, ?, &, *, $, entre otros)\nNo tengan espacios (reemplácelos por un guion medio o bajo)\nDe ser necesario, que sea identificable una relación de orden\n\n\n# Número de variables en la base de datos\nlength(datos_completos)\n\n# Obtiene el nombre de las variables de una base de datos\nnames(datos_completos)\n\n# Podríamos declarar un vector con los nuevos nombres que necesitemos\n# nombres_adecuados &lt;- c(\n#   \"nombre_variable_1\",\n#   \"nombre_variable_2\",\n#   ...,\n# )\n\n# Y luego asignarlos a nuestra base de datos\n# nombres_adecuados -&gt; names(datos_completos)\n\n# Otro ejemplo\n# Si tuviésemos variables con espacios, podríamos reemplazar todos los espacios así\n# names(datos_completos) &lt;- str_replace_all(names(datos_completos), c(\" \" = \"_\"))"
  },
  {
    "objectID": "program_03_tidy.html#valores-duplicados",
    "href": "program_03_tidy.html#valores-duplicados",
    "title": "Limpieza de datos",
    "section": "Valores duplicados",
    "text": "Valores duplicados\nPodemos buscar y eliminar duplicados basados en un columna, por ejemplo, cuando esperamos tener datos únicos de un individuo y tenemos una columna para identificarlo.\n\n## Ejemplo: acá dejamos valores únicos en la columna MEDV\ndistinct(datos_completos, MEDV, .keep_all = TRUE) -&gt; datos_completos_MEDV_unico\n\n## Ejemplo: acá dejamos valores únicos en la columna DIS\ndistinct(datos_completos, DIS, .keep_all = TRUE) -&gt; datos_completos_DIS_unico\n\nTambién podemos buscar y eliminar duplicados basados en toda la fila.\n\ndistinct(datos_completos) -&gt; datos_completos_sin_filas_duplicadas"
  },
  {
    "objectID": "program_03_tidy.html#discretización-de-variables",
    "href": "program_03_tidy.html#discretización-de-variables",
    "title": "Limpieza de datos",
    "section": "Discretización de variables",
    "text": "Discretización de variables\nEn algunos problemas, conviene convertir variables que son continuas en variables agrupadas por intervalos. Este proceso se llama discretización. Veamos un ejemplo creando un rango de edad para las viviendas de la base de datos.\n\n# Instalamos paquete lubridate\n# install.packages(\"lubridate\")\n\n# Cargamos paquete lubridate\nlibrary(\"lubridate\")\n\n# Discretizamos la edad (AGE) de los datos del censo\n# mutate() anexa/crea variables a la base de datos\ndatos_completos %&gt;% mutate(\n  edad_hoy = datos_completos$AGE+50,\n  rango_edad = cut(edad_hoy, c(0, 50, 75, 90, 105, 120, 135, Inf))\n) -&gt; datos_completos_fechas\n\nhead(datos_completos_fechas[(ncol(datos_completos_fechas)-1):ncol(datos_completos_fechas)])\n\nEste mismo principio podría servir para crear rangos etarios en poblaciones, niveles de ingresos, niveles de pobreza, etc."
  },
  {
    "objectID": "program_04_eda.html",
    "href": "program_04_eda.html",
    "title": "Análisis explorartorio de datos en R",
    "section": "",
    "text": "En esta práctica vamos a trabajar con una muestra de la base de datos Puntaje Crediticio, y podemos consultar el contexto de dichos datos haciendo clic en este enlace.\n¿Qué aprenderemos?\n\nCálculo de estadísticas descriptivas univariadas y multivariadas\nMétodo summary()\nOtros métodos de resumen\nGráficas básicas univariadas y bivariadas\nUso del paquete esquisse para crear gráficos en ggplot2"
  },
  {
    "objectID": "program_04_eda.html#introducción",
    "href": "program_04_eda.html#introducción",
    "title": "Análisis explorartorio de datos en R",
    "section": "",
    "text": "En esta práctica vamos a trabajar con una muestra de la base de datos Puntaje Crediticio, y podemos consultar el contexto de dichos datos haciendo clic en este enlace.\n¿Qué aprenderemos?\n\nCálculo de estadísticas descriptivas univariadas y multivariadas\nMétodo summary()\nOtros métodos de resumen\nGráficas básicas univariadas y bivariadas\nUso del paquete esquisse para crear gráficos en ggplot2"
  },
  {
    "objectID": "program_04_eda.html#instalamos-paquetes",
    "href": "program_04_eda.html#instalamos-paquetes",
    "title": "Análisis explorartorio de datos en R",
    "section": "Instalamos paquetes",
    "text": "Instalamos paquetes\n\n# Paquetes para hacer resúmenes de conjuntos de datos\n# install.packages(\"summarytools\")\n# install.packages(\"skimr\")\n\n# Paquete para estimar la moda de un vector univariado\n# install.packages(\"modeest\")\n\n# Paquete para hacer diversas gráficas\n# install.packages(\"ggpubr\")"
  },
  {
    "objectID": "program_04_eda.html#importar-paquetes-y-datos",
    "href": "program_04_eda.html#importar-paquetes-y-datos",
    "title": "Análisis explorartorio de datos en R",
    "section": "Importar paquetes y datos",
    "text": "Importar paquetes y datos\n\n## Cargamos librerías\nlibrary(\"tidyverse\")\nlibrary(\"skimr\")\nlibrary(\"summarytools\")\nlibrary(\"modeest\")\nlibrary(\"ggpubr\")\n\n## Lectura\nlibrary(\"readxl\")\nread_xlsx(\n  path = \"data/Puntaje-Crediticio.xlsx\", \n  sheet= \"DB\"\n) -&gt; data_raw\n\n## Revisar la estructura de los datos\nstr(data_raw)\n\n## Transformamos el ID como una variable tipo char\ndata_raw$ID &lt;- as.character(data_raw$ID)\n\n## Convertimos variables respectivas a factores\nfactores &lt;- c(\"Gender\", \"Has a car\", \"Has a property\", \"Employment status\", \n              \"Education level\", \"Marital status\", \"Dwelling\", \"Has a mobile phone\", \"Has a work phone\", \n              \"Has a phone\", \"Has an email\", \"Job title\", \"Is high risk\")\ndata_raw %&gt;% mutate_at(factores,factor) -&gt; data\n\n## Revisamos que la estructura de los datos esté ajustada\nstr(data)\n\n## Advertencia:\n## Una vez la estructura está ajustada hay que hacer limpieza de datos\n## Vamos a seguir el procesamiento con fines pedagógicos"
  },
  {
    "objectID": "program_04_eda.html#estadísticos-descriptivos-frecuentes",
    "href": "program_04_eda.html#estadísticos-descriptivos-frecuentes",
    "title": "Análisis explorartorio de datos en R",
    "section": "Estadísticos descriptivos frecuentes",
    "text": "Estadísticos descriptivos frecuentes\n\n# Univariados\nmean() # para la media\nmedian() #para la mediana\nmodeest::mfv() # para la moda\nvar() # para la varianza\nsd() # para la desviación estándar\nmax() # para el máximo\nmin() # para el mínimo\nquantile() # para los cuartiles\nIQR() # para el rango intercuartílico\n# Multivariados\ncov() # para la covarianza\ncor() # para la correlación\ntable() # para tablas de contingencia - también sirve en una variable\n\n\nEjemplo: estadísticos descriptivos para la variable income (cuantitativa)\n\nmean(data$Income)\nmedian(data$Income)\nvar(data$Income)\nsd(data$Income)\nmax(data$Income)\nmin(data$Income)\nquantile(data$Income)\nIQR(data$Income)\n\n\n\nEjemplo: estadísticos descriptivos para la variable RAD (cualitativa)\n\n# Moda\nmfv(data$`Education level`)\n# Frecuencias absolutas (Conteos)\ntable(data$`Education level`)\n# Frecuencias relativas (Proporciones) \nprop.table(table(data$`Education level`))"
  },
  {
    "objectID": "program_04_eda.html#un-método-útil-integrado-en-r-summary",
    "href": "program_04_eda.html#un-método-útil-integrado-en-r-summary",
    "title": "Análisis explorartorio de datos en R",
    "section": "Un método útil integrado en R: summary()",
    "text": "Un método útil integrado en R: summary()\n\n## Resumen para una sola variable\nsummary(data$Income)\n## Resumen para todo el conjunto de datos\nsummary(data)\n\nNormalmente estas funciones no trabajan si encuentran datos perdidos, para eliminar esta restricción se usa\n\nna.rm = TRUE\n\n## Cálculo del promedio omitiendo datos perdidos/faltantes\nmean(data$Income, na.rm = TRUE)"
  },
  {
    "objectID": "program_04_eda.html#otros-métodos-de-resumen-útiles",
    "href": "program_04_eda.html#otros-métodos-de-resumen-útiles",
    "title": "Análisis explorartorio de datos en R",
    "section": "Otros métodos de resumen útiles",
    "text": "Otros métodos de resumen útiles\n\n# Resumen usando el paquete skimr\nskim(data)\n\n# Resumen usando el paquete summarytools\nsummarytools::descr(data)"
  },
  {
    "objectID": "program_04_eda.html#gráficas-básicas",
    "href": "program_04_eda.html#gráficas-básicas",
    "title": "Análisis explorartorio de datos en R",
    "section": "Gráficas básicas",
    "text": "Gráficas básicas\n\nDiagrama de puntos\nPresenta los valores de una variable para cada observación usando símbolos, y cada observación se grafica de manera relativa al número de la observación.\n\nplot(data$Income)\n\n\n\n\n\n\n\n\n\n\nHistogramas\nEs un tipo de gráfica de barras que muestra conteos o frecuencias relativas de valores que caen en diferentes intervalos.\n\nhist(data$Income)\n\n\n\n\n\n\n\n\nPodemos cambiar el número de barras usando el parámetro breaks\n\nhist(data$Income, breaks=20)\n\n\n\n\n\n\n\n\n\n\nGráficos de densidad (también llamados gráficas Kernel o histogramas suavizados)\nMuestra las frecuencias relativas locales de los puntos a lo largo del eje X. En otras palabras, en los intervalos donde hay más puntos/observaciones la gráfica crece indicando mayor “densidad”.\n\nplot(density(data$Income))\n\n\n\n\n\n\n\n\n\nLos gráficos de densidad son muy útiles para determinar la función de distribución de probabilidad de una variable cuantitativa."
  },
  {
    "objectID": "program_04_eda.html#gráfico-q-q",
    "href": "program_04_eda.html#gráfico-q-q",
    "title": "Análisis explorartorio de datos en R",
    "section": "Gráfico Q-Q",
    "text": "Gráfico Q-Q\nSe usa para revisar si los datos siguen una distribución aproximadamente normal.\n\nggqqplot(data, x = \"Income\")\n\n\n\n\n\n\n\n\n\nFunción de distribución empírica acumulada\nMuestra la frecuencia relativa acumulada para los valores de la muestra.\n\nggecdf(data, x = \"Income\")\n\n\n\n\n\n\n\n\n\n\nBoxplot\nMuestra la ubicación, dispersión y distribución de una variable mediante la construcción de una figura en forma de caja con un conjunto de líneas (bigotes) que se extienden desde los extremos de la caja. Los bordes del cuadro se dibujan en los percentiles 25 y 75 de los datos, y una línea en el medio del cuadro marca el percentil 50.\n\nboxplot(data$Income)\n\n\n\n\n\n\n\n\n\nNota: en ciencia de datos cuando las variables de la base de datos son cuantitativas se les suele llamar variables contínuas, y en estos casos los tipos de gráficos que más se usan son los histogramas y los boxplots. Para variables cualitativas, a las que se le suele llamar variables categóricas, estas usualmente se representan mediante tablas de contingencia, diagramas de barras o diagramas de torta.\n\n\n\nDiagrama de barras\n\n# Sin orden\nggplot(data, aes(x = Dwelling))+\n  geom_bar(stat=\"count\")+\n  theme_minimal()\n\n\n\n\n\n\n\n# Ordenado\n# Función para reordenar los factores de un vector en orden decreciente\nreorder_size &lt;- function(x) {\n        factor(x, levels = names(sort(table(x), decreasing = TRUE)))\n}\nggplot(data, aes(x = reorder_size(Dwelling)))+\n  geom_bar(stat=\"count\")+\n  theme_minimal()"
  },
  {
    "objectID": "program_04_eda.html#gráficos-bivariados-y-multivariados",
    "href": "program_04_eda.html#gráficos-bivariados-y-multivariados",
    "title": "Análisis explorartorio de datos en R",
    "section": "Gráficos bivariados y multivariados",
    "text": "Gráficos bivariados y multivariados\n\nEjemplo: diagrama de dispersión entre dos variables contínuas\n\nplot(data$`Family member count`, data$`Children count`)\n\n\n\n\n\n\n\n\n\n\nEjemplo: boxplot por grupos (una variable contínua y una categórica)\n\nggboxplot(data, x = \"Gender\", y = \"Income\",\n          palette = c(\"#00AFBB\", \"#E7B800\"))\n\n\n\n\n\n\n\n\n\n\nEjemplo: matríz de correlación gráfica\n\nlibrary(\"corrplot\")\nvariables_seleccionadas &lt;- c(\"Income\",\"Family member count\",\"Account age\",\"Children count\")\ndata[variables_seleccionadas] -&gt; variables_numericas\nvariables_numericas %&gt;% \n  cor %&gt;% \n  corrplot.mixed(lower = \"number\", upper = \"color\", lower.col = \"#aaaaaa\", number.cex = 0.6, tl.cex = 0.6)"
  },
  {
    "objectID": "program_04_eda.html#una-librería-útil-con-una-interfaz-para-hacer-gráficas-esquisse",
    "href": "program_04_eda.html#una-librería-útil-con-una-interfaz-para-hacer-gráficas-esquisse",
    "title": "Análisis explorartorio de datos en R",
    "section": "Una librería útil con una interfaz para hacer gráficas: esquisse",
    "text": "Una librería útil con una interfaz para hacer gráficas: esquisse\n\n## Instalamos la librería\n# install.packages(\"esquisse\")\n\n## Cargamos la librería\n# library(\"esquisse\")\n\n## Llamamos a la función esquisser especificando con cuales datos queremos trabajar\n# esquisser(data = data, viewer = \"browser\")"
  },
  {
    "objectID": "program_04_eda.html#gráficos-profesionales",
    "href": "program_04_eda.html#gráficos-profesionales",
    "title": "Análisis explorartorio de datos en R",
    "section": "Gráficos profesionales",
    "text": "Gráficos profesionales\n\nGalería de gráficas en R\nTop 50 de visualizaciones en R"
  },
  {
    "objectID": "program_05_asociacion.html",
    "href": "program_05_asociacion.html",
    "title": "Asociación e independencia entre variables",
    "section": "",
    "text": "# Cargamos tidyverse\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "program_05_asociacion.html#cargar-librerías",
    "href": "program_05_asociacion.html#cargar-librerías",
    "title": "Asociación e independencia entre variables",
    "section": "",
    "text": "# Cargamos tidyverse\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "program_05_asociacion.html#importar-datos",
    "href": "program_05_asociacion.html#importar-datos",
    "title": "Asociación e independencia entre variables",
    "section": "Importar datos",
    "text": "Importar datos\n\n# Cargamos datos\nread_delim(\n  file = \"01_data/programacion/winequality-red.csv\",\n  delim = \",\", \n  locale=locale(decimal_mark = \".\")\n  ) -&gt; wine_raw\n\nRows: 1599 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (12): fixed acidity, volatile acidity, citric acid, residual sugar, chlo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n## Revisar la estructura de los datos\nstr(wine_raw)\n\nspc_tbl_ [1,599 × 12] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ fixed acidity       : num [1:1599] 7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 7.5 ...\n $ volatile acidity    : num [1:1599] 0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.5 ...\n $ citric acid         : num [1:1599] 0 0 0.04 0.56 0 0 0.06 0 0.02 0.36 ...\n $ residual sugar      : num [1:1599] 1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 6.1 ...\n $ chlorides           : num [1:1599] 0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.071 ...\n $ free sulfur dioxide : num [1:1599] 11 25 15 17 11 13 15 15 9 17 ...\n $ total sulfur dioxide: num [1:1599] 34 67 54 60 34 40 59 21 18 102 ...\n $ density             : num [1:1599] 0.998 0.997 0.997 0.998 0.998 ...\n $ pH                  : num [1:1599] 3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ...\n $ sulphates           : num [1:1599] 0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ...\n $ alcohol             : num [1:1599] 9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ...\n $ quality             : num [1:1599] 5 5 5 6 5 5 5 7 7 5 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `fixed acidity` = col_double(),\n  ..   `volatile acidity` = col_double(),\n  ..   `citric acid` = col_double(),\n  ..   `residual sugar` = col_double(),\n  ..   chlorides = col_double(),\n  ..   `free sulfur dioxide` = col_double(),\n  ..   `total sulfur dioxide` = col_double(),\n  ..   density = col_double(),\n  ..   pH = col_double(),\n  ..   sulphates = col_double(),\n  ..   alcohol = col_double(),\n  ..   quality = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt;"
  },
  {
    "objectID": "program_05_asociacion.html#información-sobre-el-conjunto-de-datos",
    "href": "program_05_asociacion.html#información-sobre-el-conjunto-de-datos",
    "title": "Asociación e independencia entre variables",
    "section": "Información sobre el conjunto de datos",
    "text": "Información sobre el conjunto de datos\n\nInformación general\nLos datos corresponden a una variación de un vino tradicional portugués llamado “Vinho Verde” proveniente de una región llamada Vinho, ubicada muy al norte de Portugal.\n\n\nVariables\n\nfixed acidity: ácidos del vino que no se evaporan fácilmente.\nvolatile acidity: cantidad de ácido acético en el vino, el cual en altas cantidades genera sensaciones no placenteras y un sabor vinagroso.\ncitric acid: cantidad de ácido cítrico en pequeñas cantidades, el cual añade cierta frescura y sabor al vino.\nresidual sugar: cantidad de azucar residual luego del proceso de fermentación. Es raro tener menos de 1g/litro y los vinos con más de 45g/litro se consideran dulces.\nchlorides: cantidad de sal en el vino.\nfree sulfur dioxide: cantidad de dióxido de azufre (S02) libre, el cual previene el crecimiento de microbios y la oxidación del vino.\ntotal sulfur dioxide: cantidad total de dióxido de azufre (S02) en forma libre y fija; en bajas concentraciones es indetectable, en concentraciones superiores a 50ppm el SO2 es evidente para la nariz y el sabor del vino.\ndensity: la densidad del vino es cercana a la del agua dependiendo de la cantidad de azucar y alcohol.\npH: describe qué tan ácido o básico es un vino en un escala desde cero (muy ácido) hasta 14 (muy básico); la gran mayoría de vinos tienen un pH entre 3-4.\nsulphates: un aditivo que contribuye a regular los niveles de dióxido de azufre (S02), el cual actúa como antimicrobios y antioxidante.\nalcohol: porcentaje del alcohol del vino.\nquality: puntuación del vino basada en datos sensoriales, en una escala entre 0 y 10.\n\n\n\nFuente\nCortez, P., Cerdeira, A., Almeida, F., Matos, T., & Reis, J. (2009). Modeling wine preferences by data mining from physicochemical properties. Decision Support Systems, 47(4), 547-553.\n\nNote que es muy importante tener un contexto sobre el conjunto de datos."
  },
  {
    "objectID": "program_05_asociacion.html#dimensionalidad-de-los-datos",
    "href": "program_05_asociacion.html#dimensionalidad-de-los-datos",
    "title": "Asociación e independencia entre variables",
    "section": "Dimensionalidad de los datos",
    "text": "Dimensionalidad de los datos\n\ndim(wine_raw)\n\n[1] 1599   12\n\n\n\n1599 individuos - Número de filas\n12 variables - Número de columnas\n\n\nTodos los cálculos y procedimientos matemáticos y estadísticos, a nivel computacional, se realizan mediante operaciones sobre las estructuras de datos vistas en la práctica de programación básica.\n\n\n\n## Obtener solo el número de filas\nnrow(wine_raw)\n\n[1] 1599\n\n## Obtener solo el número de columnas\nncol(wine_raw)\n\n[1] 12"
  },
  {
    "objectID": "program_05_asociacion.html#limpieza-de-los-datos",
    "href": "program_05_asociacion.html#limpieza-de-los-datos",
    "title": "Asociación e independencia entre variables",
    "section": "Limpieza de los datos",
    "text": "Limpieza de los datos\nEn la práctica la calidad de los datos puede estar afectada por los procesos de captura, sistematización y distribución. Siempre hay que verificar la calidad de nuestros datos.\n\nLimpieza de tablas (3 principios vistos en clase)\nNombrado adecuado de las variables\nDatos faltantes\nDatos atípicos\nValores duplicados\n\n\nPara mayor detalle, consulte el material de la practica de limpieza de datos.\n\nEn este caso la base de datos proporcionada ya tiene una estructura adecuada para el procesamiento, salvo que los nombres de las columnas tienen espacios y no siguen las convenciones del naming de variavbles, por lo que vamos a ponerles un buen nombre\n\nnames(wine_raw) &lt;- str_replace_all(names(wine_raw), c(\" \" = \"_\"))"
  },
  {
    "objectID": "program_05_asociacion.html#enriquecimiento-de-datos",
    "href": "program_05_asociacion.html#enriquecimiento-de-datos",
    "title": "Asociación e independencia entre variables",
    "section": "Enriquecimiento de datos",
    "text": "Enriquecimiento de datos\nDado que inicialmente todas las variables son cuantitativas, vamos a realizar una operación sobre nuestro conjunto de datos para agregar dos nuevas columnas categóricas, de tal manera que podemos explorar algunas medidas y gráficas relevantes.\n\n## Partimos de la base de datos 'wine_raw'\n## y la ontroducimos a un algoritmo de operaciones\nwine_raw %&gt;%\n  ## mutate() crea una nueva variable llamada 'calidad'\n  ## basada en los rangos ya conocidos de la variable quality\n  mutate(\n    calidad = ifelse(\n      quality == '3' | quality == '4','baja',\n      ifelse(\n        quality == '5' | quality == '6','media',\n        'alta'))\n  ) %&gt;% \n  ## mutate_at() recibe la columna 'calidad' y la convierte en un factor\n  mutate_at('calidad', factor) %&gt;%\n  \n  ## mutate() crea una nueva variable llamada 'acetico'\n  ## basada en rangos conocidos de la variable 'volatile_acidity'\n  mutate(\n    acetico = ifelse(volatile_acidity &lt; 0.7, 'bajo', 'alto')\n  ) %&gt;% \n  ## mutate_at() recibe la columna 'acetico' y la convierte en un factor\n  ## el resultado de todas las operaciones se guarda en 'wine_processed'\n  mutate_at('acetico', factor) -&gt; wine_processed\n\n# Especificamos el orden de los factores que acabamos de crear\nwine_processed$calidad &lt;- factor(wine_processed$calidad, levels = c(\"baja\",\"media\",\"alta\"))\nwine_processed$acetico &lt;- factor(wine_processed$acetico, levels = c(\"bajo\",\"alto\"))"
  },
  {
    "objectID": "program_05_asociacion.html#análisis-descriptivo",
    "href": "program_05_asociacion.html#análisis-descriptivo",
    "title": "Asociación e independencia entre variables",
    "section": "Análisis descriptivo",
    "text": "Análisis descriptivo\n\nResumen numérico\nEl método summary() que trae por defecto R nos brinda estadísticas de resumen para cada una de las variables de nuestro conjunto de datos.\n\n## Resumen básico de datos\nsummary(wine_processed)\n\n fixed_acidity   volatile_acidity  citric_acid    residual_sugar  \n Min.   : 4.60   Min.   :0.1200   Min.   :0.000   Min.   : 0.900  \n 1st Qu.: 7.10   1st Qu.:0.3900   1st Qu.:0.090   1st Qu.: 1.900  \n Median : 7.90   Median :0.5200   Median :0.260   Median : 2.200  \n Mean   : 8.32   Mean   :0.5278   Mean   :0.271   Mean   : 2.539  \n 3rd Qu.: 9.20   3rd Qu.:0.6400   3rd Qu.:0.420   3rd Qu.: 2.600  \n Max.   :15.90   Max.   :1.5800   Max.   :1.000   Max.   :15.500  \n   chlorides       free_sulfur_dioxide total_sulfur_dioxide    density      \n Min.   :0.01200   Min.   : 1.00       Min.   :  6.00       Min.   :0.9901  \n 1st Qu.:0.07000   1st Qu.: 7.00       1st Qu.: 22.00       1st Qu.:0.9956  \n Median :0.07900   Median :14.00       Median : 38.00       Median :0.9968  \n Mean   :0.08747   Mean   :15.87       Mean   : 46.47       Mean   :0.9967  \n 3rd Qu.:0.09000   3rd Qu.:21.00       3rd Qu.: 62.00       3rd Qu.:0.9978  \n Max.   :0.61100   Max.   :72.00       Max.   :289.00       Max.   :1.0037  \n       pH          sulphates         alcohol         quality       calidad    \n Min.   :2.740   Min.   :0.3300   Min.   : 8.40   Min.   :3.000   baja :  63  \n 1st Qu.:3.210   1st Qu.:0.5500   1st Qu.: 9.50   1st Qu.:5.000   media:1319  \n Median :3.310   Median :0.6200   Median :10.20   Median :6.000   alta : 217  \n Mean   :3.311   Mean   :0.6581   Mean   :10.42   Mean   :5.636               \n 3rd Qu.:3.400   3rd Qu.:0.7300   3rd Qu.:11.10   3rd Qu.:6.000               \n Max.   :4.010   Max.   :2.0000   Max.   :14.90   Max.   :8.000               \n acetico    \n bajo:1366  \n alto: 233  \n            \n            \n            \n            \n\n\n\n\nResumen gráfico\n\n## Cargamos la librería ggplot2\nlibrary(\"ggplot2\")\n\n## Usamos el método para graficar histogramas\n## Seleccionamos como objetivo la variable quality\nggplot(wine_processed, aes(quality)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nNote que podemos hacer histogramas para las demás variables. Además, podríamos realizar otros tipos de gráficos univariados.\n\n\nNuestro interés ahora será construir resúmenes numéricos y resúmenes gráficos entre dos o más variables."
  },
  {
    "objectID": "program_05_asociacion.html#efectos-de-una-variable-sobre-otra",
    "href": "program_05_asociacion.html#efectos-de-una-variable-sobre-otra",
    "title": "Asociación e independencia entre variables",
    "section": "Efectos de una variable sobre otra",
    "text": "Efectos de una variable sobre otra\nEn una investigación o estudio podemos sospechar de la influencia o efecto de un conjunto de variables sobre una variable particular de interés (target / label / variable crítica / variable explicada). Una parte esencial de la fase de análisis es reunir evidencia para seleccionar las variables que tengan mayor probabilidad de tener un efecto sobre nuestra variable de interés.\n\nExisten distintas herramientas estadísticas para tener una idea bien formada de cómo se relacionan dos o más variables entre sí.\nAntes de explorar dichas herramientas, conviene hacer una revisión sobre algunos conceptos.\n\nAsociación entre dos variables contínuas\nLa covarianza es una medida numérica que nos permite cuantificar la relación (lineal) entre dos variables contínuas.\n\nSu estimador es la covarianza muestral:\n\n\nSi dos variables son independientes su covarianza es nula. El reciproco no es cierto en general, si dos variables tienen covarianza nula se dice que son incorreladas (no hay relación lineal, aunque puede haber una relación no lineal).\nSi la covarianza es positiva indica que a valores grandes de X le corresponden valores grandes de Y (i.e. al incrementar X se incrementa Y) y se dice que hay una relación lineal positiva.\nSi la covarianza es negativa indica que a valores grandes de X le corresponden valores pequeños de Y (i.e. al incrementar X, Y disminuye) y se dice que hay una relación lineal negativa.\n\nCuanto mayor es el valor (absoluto) de la covarianza, mayor es el grado de relación lineal entre las variables. Sin embargo, su valor depende de las escala de las variables por lo que es difícil determinar cuando es grande o pequeña. Para medir el grado de relación lineal puede ser preferible reescalarla, i.e. emplear el coeficiente de correlación:\n\nSu estimador es el coeficiente de correlación muestral:\n\n\nUna correlación positiva entre dos variables indica que a medida que los valores de una variable crecen los valores de la otra variable también crecen. Y viceversa. El máximo valor de una correlación positiva es 1.\nUna correlación negativa entre dos variables indica que a medida que los valores de una variable crecen los valores de la otra variable decrecen. El máximo valor de una correlación negativa es -1.\nUna correlación de cero entre dos variables indica que no existe una asociación lineal entre ellas.\n\nDado que el trabajo estadístico de datos es principalmente matricial y tenemos un número finito de variables aleatorias, en vez de calcular la covarianza entre dos variables podemos construir una matriz de covarianzas y calcular las covarianzas entre todas las variables.\nCovarianzas en R\n\n## Covarianzas entre dos variables\ncov(wine_processed$fixed_acidity, wine_processed$quality)\n\n[1] 0.1744236\n\ncov(wine_processed$chlorides, wine_processed$quality)\n\n[1] -0.004899545\n\n## Matriz de varianzas y covarianzas\n## Note que filtramos la base de datos para solamente calcular correlaciones entre variables cuantitativas \ncov(Filter(is.numeric, wine_processed))\n\n                     fixed_acidity volatile_acidity   citric_acid\nfixed_acidity          3.031416389    -7.985142e-02  0.2278200037\nvolatile_acidity      -0.079851417     3.206238e-02 -0.0192716208\ncitric_acid            0.227820004    -1.927162e-02  0.0379474831\nresidual_sugar         0.281756262     4.841910e-04  0.0394342700\nchlorides              0.007678692     5.165869e-04  0.0018687248\nfree_sulfur_dioxide   -2.800921493    -1.967359e-02 -0.1242521139\ntotal_sulfur_dioxide  -6.482345858     4.504257e-01  0.2276972740\ndensity                0.002195224     7.443665e-06  0.0001341746\npH                    -0.183585704     6.494699e-03 -0.0162975823\nsulphates              0.054010092    -7.921434e-03  0.0103277145\nalcohol               -0.114421153    -3.860022e-02  0.0228151729\nquality                0.174423588    -5.647588e-02  0.0356118929\n                     residual_sugar     chlorides free_sulfur_dioxide\nfixed_acidity          0.2817562623  7.678692e-03       -2.800921e+00\nvolatile_acidity       0.0004841910  5.165869e-04       -1.967359e-02\ncitric_acid            0.0394342700  1.868725e-03       -1.242521e-01\nresidual_sugar         1.9878971330  3.690176e-03        2.758611e+00\nchlorides              0.0036901759  2.215143e-03        2.738303e-03\nfree_sulfur_dioxide    2.7586114522  2.738303e-03        1.094149e+02\ntotal_sulfur_dioxide   9.4164414790  7.338675e-02        2.297375e+02\ndensity                0.0009454109  1.782176e-05       -4.332504e-04\npH                    -0.0186442890 -1.925745e-03        1.136531e-01\nsulphates              0.0013209414  2.961878e-03        9.159247e-02\nalcohol                0.0632189598 -1.109152e-02       -7.736984e-01\nquality                0.0156350457 -4.899545e-03       -4.279071e-01\n                     total_sulfur_dioxide       density            pH\nfixed_acidity               -6.482346e+00  2.195224e-03 -1.835857e-01\nvolatile_acidity             4.504257e-01  7.443665e-06  6.494699e-03\ncitric_acid                  2.276973e-01  1.341746e-04 -1.629758e-02\nresidual_sugar               9.416441e+00  9.454109e-04 -1.864429e-02\nchlorides                    7.338675e-02  1.782176e-05 -1.925745e-03\nfree_sulfur_dioxide          2.297375e+02 -4.332504e-04  1.136531e-01\ntotal_sulfur_dioxide         1.082102e+03  4.424727e-03 -3.376988e-01\ndensity                      4.424727e-03  3.562029e-06 -9.956395e-05\npH                          -3.376988e-01 -9.956395e-05  2.383518e-02\nsulphates                    2.394710e-01  4.750962e-05 -5.146186e-03\nalcohol                     -7.209298e+00 -9.979518e-04  3.383162e-02\nquality                     -4.917237e+00 -2.666037e-04 -7.197822e-03\n                         sulphates       alcohol       quality\nfixed_acidity         5.401009e-02 -0.1144211534  0.1744235876\nvolatile_acidity     -7.921434e-03 -0.0386002214 -0.0564758833\ncitric_acid           1.032771e-02  0.0228151729  0.0356118929\nresidual_sugar        1.320941e-03  0.0632189598  0.0156350457\nchlorides             2.961878e-03 -0.0110915178 -0.0048995449\nfree_sulfur_dioxide   9.159247e-02 -0.7736984004 -0.4279070696\ntotal_sulfur_dioxide  2.394710e-01 -7.2092978950 -4.9172370717\ndensity               4.750962e-05 -0.0009979518 -0.0002666037\npH                   -5.146186e-03  0.0338316166 -0.0071978223\nsulphates             2.873262e-02  0.0169067772  0.0344134084\nalcohol               1.690678e-02  1.1356473950  0.4097890108\nquality               3.441341e-02  0.4097890108  0.6521684000\n\n\nCorrelaciones en R\n\n## Correlaciones entre dos variables\ncor(wine_processed$fixed_acidity, wine_processed$quality, method = 'pearson')\n\n[1] 0.1240516\n\ncor(wine_processed$chlorides, wine_processed$quality, method = 'pearson')\n\n[1] -0.1289066\n\n## Matriz de correlaciones\n## Note que filtramos la base de datos para solamente calcular correlaciones entre variables cuantitativas\ncor(Filter(is.numeric, wine_processed), method = 'pearson')\n\n                     fixed_acidity volatile_acidity citric_acid residual_sugar\nfixed_acidity           1.00000000     -0.256130895  0.67170343    0.114776724\nvolatile_acidity       -0.25613089      1.000000000 -0.55249568    0.001917882\ncitric_acid             0.67170343     -0.552495685  1.00000000    0.143577162\nresidual_sugar          0.11477672      0.001917882  0.14357716    1.000000000\nchlorides               0.09370519      0.061297772  0.20382291    0.055609535\nfree_sulfur_dioxide    -0.15379419     -0.010503827 -0.06097813    0.187048995\ntotal_sulfur_dioxide   -0.11318144      0.076470005  0.03553302    0.203027882\ndensity                 0.66804729      0.022026232  0.36494718    0.355283371\npH                     -0.68297819      0.234937294 -0.54190414   -0.085652422\nsulphates               0.18300566     -0.260986685  0.31277004    0.005527121\nalcohol                -0.06166827     -0.202288027  0.10990325    0.042075437\nquality                 0.12405165     -0.390557780  0.22637251    0.013731637\n                        chlorides free_sulfur_dioxide total_sulfur_dioxide\nfixed_acidity         0.093705186        -0.153794193          -0.11318144\nvolatile_acidity      0.061297772        -0.010503827           0.07647000\ncitric_acid           0.203822914        -0.060978129           0.03553302\nresidual_sugar        0.055609535         0.187048995           0.20302788\nchlorides             1.000000000         0.005562147           0.04740047\nfree_sulfur_dioxide   0.005562147         1.000000000           0.66766645\ntotal_sulfur_dioxide  0.047400468         0.667666450           1.00000000\ndensity               0.200632327        -0.021945831           0.07126948\npH                   -0.265026131         0.070377499          -0.06649456\nsulphates             0.371260481         0.051657572           0.04294684\nalcohol              -0.221140545        -0.069408354          -0.20565394\nquality              -0.128906560        -0.050656057          -0.18510029\n                         density          pH    sulphates     alcohol\nfixed_acidity         0.66804729 -0.68297819  0.183005664 -0.06166827\nvolatile_acidity      0.02202623  0.23493729 -0.260986685 -0.20228803\ncitric_acid           0.36494718 -0.54190414  0.312770044  0.10990325\nresidual_sugar        0.35528337 -0.08565242  0.005527121  0.04207544\nchlorides             0.20063233 -0.26502613  0.371260481 -0.22114054\nfree_sulfur_dioxide  -0.02194583  0.07037750  0.051657572 -0.06940835\ntotal_sulfur_dioxide  0.07126948 -0.06649456  0.042946836 -0.20565394\ndensity               1.00000000 -0.34169933  0.148506412 -0.49617977\npH                   -0.34169933  1.00000000 -0.196647602  0.20563251\nsulphates             0.14850641 -0.19664760  1.000000000  0.09359475\nalcohol              -0.49617977  0.20563251  0.093594750  1.00000000\nquality              -0.17491923 -0.05773139  0.251397079  0.47616632\n                         quality\nfixed_acidity         0.12405165\nvolatile_acidity     -0.39055778\ncitric_acid           0.22637251\nresidual_sugar        0.01373164\nchlorides            -0.12890656\nfree_sulfur_dioxide  -0.05065606\ntotal_sulfur_dioxide -0.18510029\ndensity              -0.17491923\npH                   -0.05773139\nsulphates             0.25139708\nalcohol               0.47616632\nquality               1.00000000\n\n\n\nPor defecto el método cor() calcula una correlación de Pearson, por tanto, el resultado numerico asume que la relación entre las variables es lineal. Dado que en la practica hay muchas relaciones no lineales, una forma más robusta de calcular la asociación es calculando una correlación de Spearman (method = ‘spearman’) o el estadístico Tau de Kendall (method = ‘kendall’).\n\nOtra forma más bonita de mostrar correlaciones usando el paquete correlation\n\n# Instalamos el paquete correlation\ninstall.packages(\"correlation\")\n\n\n# Cargamos el paquete\nlibrary(\"correlation\")\n\n\n# Creamos una matriz detallada de correlaciones\n# Note que acá no estamos filtrando las columnas numéricas como lo hicimos antes\n# Esto debido a que el paquete automáticamente hace el filtrado. Uno debe verificar\ncorrelaciones &lt;- correlation(wine_processed)\ncorrelaciones\n\n# Correlation Matrix (pearson-method)\n\nParameter1           |           Parameter2 |        r |         95% CI | t(1597) |         p\n---------------------------------------------------------------------------------------------\nfixed_acidity        |     volatile_acidity |    -0.26 | [-0.30, -0.21] |  -10.59 | &lt; .001***\nfixed_acidity        |          citric_acid |     0.67 | [ 0.64,  0.70] |   36.23 | &lt; .001***\nfixed_acidity        |       residual_sugar |     0.11 | [ 0.07,  0.16] |    4.62 | &lt; .001***\nfixed_acidity        |            chlorides |     0.09 | [ 0.04,  0.14] |    3.76 | 0.005**  \nfixed_acidity        |  free_sulfur_dioxide |    -0.15 | [-0.20, -0.11] |   -6.22 | &lt; .001***\nfixed_acidity        | total_sulfur_dioxide |    -0.11 | [-0.16, -0.06] |   -4.55 | &lt; .001***\nfixed_acidity        |              density |     0.67 | [ 0.64,  0.69] |   35.88 | &lt; .001***\nfixed_acidity        |                   pH |    -0.68 | [-0.71, -0.66] |  -37.37 | &lt; .001***\nfixed_acidity        |            sulphates |     0.18 | [ 0.14,  0.23] |    7.44 | &lt; .001***\nfixed_acidity        |              alcohol |    -0.06 | [-0.11, -0.01] |   -2.47 | 0.246    \nfixed_acidity        |              quality |     0.12 | [ 0.08,  0.17] |    5.00 | &lt; .001***\nvolatile_acidity     |          citric_acid |    -0.55 | [-0.59, -0.52] |  -26.49 | &lt; .001***\nvolatile_acidity     |       residual_sugar | 1.92e-03 | [-0.05,  0.05] |    0.08 | &gt; .999   \nvolatile_acidity     |            chlorides |     0.06 | [ 0.01,  0.11] |    2.45 | 0.246    \nvolatile_acidity     |  free_sulfur_dioxide |    -0.01 | [-0.06,  0.04] |   -0.42 | &gt; .999   \nvolatile_acidity     | total_sulfur_dioxide |     0.08 | [ 0.03,  0.13] |    3.06 | 0.051    \nvolatile_acidity     |              density |     0.02 | [-0.03,  0.07] |    0.88 | &gt; .999   \nvolatile_acidity     |                   pH |     0.23 | [ 0.19,  0.28] |    9.66 | &lt; .001***\nvolatile_acidity     |            sulphates |    -0.26 | [-0.31, -0.21] |  -10.80 | &lt; .001***\nvolatile_acidity     |              alcohol |    -0.20 | [-0.25, -0.15] |   -8.25 | &lt; .001***\nvolatile_acidity     |              quality |    -0.39 | [-0.43, -0.35] |  -16.95 | &lt; .001***\ncitric_acid          |       residual_sugar |     0.14 | [ 0.10,  0.19] |    5.80 | &lt; .001***\ncitric_acid          |            chlorides |     0.20 | [ 0.16,  0.25] |    8.32 | &lt; .001***\ncitric_acid          |  free_sulfur_dioxide |    -0.06 | [-0.11, -0.01] |   -2.44 | 0.246    \ncitric_acid          | total_sulfur_dioxide |     0.04 | [-0.01,  0.08] |    1.42 | &gt; .999   \ncitric_acid          |              density |     0.36 | [ 0.32,  0.41] |   15.66 | &lt; .001***\ncitric_acid          |                   pH |    -0.54 | [-0.58, -0.51] |  -25.77 | &lt; .001***\ncitric_acid          |            sulphates |     0.31 | [ 0.27,  0.36] |   13.16 | &lt; .001***\ncitric_acid          |              alcohol |     0.11 | [ 0.06,  0.16] |    4.42 | &lt; .001***\ncitric_acid          |              quality |     0.23 | [ 0.18,  0.27] |    9.29 | &lt; .001***\nresidual_sugar       |            chlorides |     0.06 | [ 0.01,  0.10] |    2.23 | 0.366    \nresidual_sugar       |  free_sulfur_dioxide |     0.19 | [ 0.14,  0.23] |    7.61 | &lt; .001***\nresidual_sugar       | total_sulfur_dioxide |     0.20 | [ 0.16,  0.25] |    8.29 | &lt; .001***\nresidual_sugar       |              density |     0.36 | [ 0.31,  0.40] |   15.19 | &lt; .001***\nresidual_sugar       |                   pH |    -0.09 | [-0.13, -0.04] |   -3.44 | 0.015*   \nresidual_sugar       |            sulphates | 5.53e-03 | [-0.04,  0.05] |    0.22 | &gt; .999   \nresidual_sugar       |              alcohol |     0.04 | [-0.01,  0.09] |    1.68 | 0.860    \nresidual_sugar       |              quality |     0.01 | [-0.04,  0.06] |    0.55 | &gt; .999   \nchlorides            |  free_sulfur_dioxide | 5.56e-03 | [-0.04,  0.05] |    0.22 | &gt; .999   \nchlorides            | total_sulfur_dioxide |     0.05 | [ 0.00,  0.10] |    1.90 | 0.639    \nchlorides            |              density |     0.20 | [ 0.15,  0.25] |    8.18 | &lt; .001***\nchlorides            |                   pH |    -0.27 | [-0.31, -0.22] |  -10.98 | &lt; .001***\nchlorides            |            sulphates |     0.37 | [ 0.33,  0.41] |   15.98 | &lt; .001***\nchlorides            |              alcohol |    -0.22 | [-0.27, -0.17] |   -9.06 | &lt; .001***\nchlorides            |              quality |    -0.13 | [-0.18, -0.08] |   -5.19 | &lt; .001***\nfree_sulfur_dioxide  | total_sulfur_dioxide |     0.67 | [ 0.64,  0.69] |   35.84 | &lt; .001***\nfree_sulfur_dioxide  |              density |    -0.02 | [-0.07,  0.03] |   -0.88 | &gt; .999   \nfree_sulfur_dioxide  |                   pH |     0.07 | [ 0.02,  0.12] |    2.82 | 0.102    \nfree_sulfur_dioxide  |            sulphates |     0.05 | [ 0.00,  0.10] |    2.07 | 0.505    \nfree_sulfur_dioxide  |              alcohol |    -0.07 | [-0.12, -0.02] |   -2.78 | 0.110    \nfree_sulfur_dioxide  |              quality |    -0.05 | [-0.10,  0.00] |   -2.03 | 0.514    \ntotal_sulfur_dioxide |              density |     0.07 | [ 0.02,  0.12] |    2.86 | 0.096    \ntotal_sulfur_dioxide |                   pH |    -0.07 | [-0.12, -0.02] |   -2.66 | 0.149    \ntotal_sulfur_dioxide |            sulphates |     0.04 | [-0.01,  0.09] |    1.72 | 0.860    \ntotal_sulfur_dioxide |              alcohol |    -0.21 | [-0.25, -0.16] |   -8.40 | &lt; .001***\ntotal_sulfur_dioxide |              quality |    -0.19 | [-0.23, -0.14] |   -7.53 | &lt; .001***\ndensity              |                   pH |    -0.34 | [-0.38, -0.30] |  -14.53 | &lt; .001***\ndensity              |            sulphates |     0.15 | [ 0.10,  0.20] |    6.00 | &lt; .001***\ndensity              |              alcohol |    -0.50 | [-0.53, -0.46] |  -22.84 | &lt; .001***\ndensity              |              quality |    -0.17 | [-0.22, -0.13] |   -7.10 | &lt; .001***\npH                   |            sulphates |    -0.20 | [-0.24, -0.15] |   -8.02 | &lt; .001***\npH                   |              alcohol |     0.21 | [ 0.16,  0.25] |    8.40 | &lt; .001***\npH                   |              quality |    -0.06 | [-0.11, -0.01] |   -2.31 | 0.314    \nsulphates            |              alcohol |     0.09 | [ 0.04,  0.14] |    3.76 | 0.005**  \nsulphates            |              quality |     0.25 | [ 0.20,  0.30] |   10.38 | &lt; .001***\nalcohol              |              quality |     0.48 | [ 0.44,  0.51] |   21.64 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 1599\n\n\n\nQue exista una asociación fuerte entre dos variables no implica una relación causal. Para testear la causalidad veremos otras herramientas más adelante."
  },
  {
    "objectID": "program_05_asociacion.html#visualización-de-la-relación-entre-variables",
    "href": "program_05_asociacion.html#visualización-de-la-relación-entre-variables",
    "title": "Asociación e independencia entre variables",
    "section": "Visualización de la relación entre variables",
    "text": "Visualización de la relación entre variables\nHemos visto que podemos crear gráficos univariados para tener una fotografía del comportamiento de una variable. De igual manera, es posible construir gráficos que muestren la asociación entre dos o más variables.\n\n\n\nVariable 1\nVariable 2\nVisualización frecuente\n\n\n\n\nCategórica\nCategórica\nTablas de contingencia\n\n\nCategórica\nContínua\nBoxplot por grupos\n\n\nContínua\nContínua\nDiagrama de dispersión\n\n\n\nPara nuestro ejemplo del vino rojo, siguiendo las recomendaciones de la tabla, conviene crear diagramas de dispersión.\nDiagrama de dispersión\nSon útiles porque al cruzar los valores de un par de variables podemos encontrar posibles relaciones matemáticas entre ellas.\n\n\n# install.packages(\"hrbrthemes\")\nlibrary(\"hrbrthemes\")\n\n## Una relación lineal inexistente\nggplot(wine_processed, aes(x=residual_sugar, y=quality)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  theme_ipsum()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n## Una relación lineal positiva\nggplot(wine_processed, aes(x=alcohol, y=quality)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  theme_ipsum()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n## Una relación lineal negativa\nggplot(wine_processed, aes(x=volatile_acidity, y=quality)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  theme_ipsum()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCorrelogramas\nPodemos crear una visualización donde se muestren todos los posibles diagramas de dispersión entre parejas de variables con sus respectivos coeficientes de correlación.\n\n## Instalamos la librería GGally\ninstall.packages('GGally')\n\n\n## Cargamos la librería\nlibrary('GGally')\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n## Creamos la visualización usando el método ggpairs()\nggpairs(\n  wine_processed, \n  title=\"Correlograma\"\n  ) \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nPodemos graficar filtrando ciertas variables de interés. En este caso, vamos a remover aquellas que tengan un coeficiente de correlación menor a 0.2 con nuestra variable target (quality).\n\n## Declaramos un vector con nuestras variables de interés\nvar_interes = c('volatile_acidity','citric_acid','sulphates','alcohol','quality')\n\n## Creamos la visualización usando el método ggpairs() agregando el parámetro columns\nggpairs(\n  wine_processed, \n  title=\"Correlograma\",\n  columns = var_interes\n  ) \n\n\n\n\n\n\n\n\nOtra forma de visualizar la correlación entre variables.\n\nggcorr(\n  Filter(is.numeric, wine_processed), \n  method = c(\"everything\", \"pearson\"),\n  size = 3\n  )\n\n\n\n\n\n\n\n\nDe las anteriores matrices y gráficas podemos observar algunas nuevas correlaciones de interés, por ejemplo, entre el pH y la acidéz. Podemos observar además que para la variable target aproximadamente la mitad de las variables independientes correlacionan positivamente y la otra mitad negativamente.\nEn la práctica, se pueden seleccionar las variables independientes que tienen las medidas de asociación más altas en la medida que sospechamos que nos aportan más información. Una regla de oro sencilla es excluir variables que tengan una correlación menor (en valor absoluto) a 0.2.\n\nExamen detallado de variables de interés\nDe nuestro conjunto de datos iniciales hemos detectado ciertas variables independientes o explicativas que nos pueden aportar mayor información para explicar la calidad del vino.\n\nvolatile_acidity\ncitric_acid\nsulphates\nalcohol\n\nAdicionalmente, tenemos la calidad del vino expresada como una variable contínua (quality) y también de forma categórica (calidad).\n¿Qué deberíamos hacer? Analizar el comportamiento conjunto de cada una de nuestras variables explicativas con la variable crítica. Dicho esto, vamos ahora a realizar estos cruces bivariados en tres escenarios:\n\n\n\nEscenario 1: Asociación entre dos variables contínuas\nYa vimos cómo hacer el cálculo de coeficientes de correlación. Examinemos ahora en diagramas de dispersión las relaciones entre las variables de interés y la variable crítica cuantitativa quality.\nAl ver los diagramas de dispersión, ¿es plausible pensar que hay una relación entre las variables?\nvolatile_acidity\n\nggplot(wine_processed, aes(x=volatile_acidity, y=quality)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  theme_ipsum()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\ncitric_acid\n\nggplot(wine_processed, aes(x=citric_acid, y=quality)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  theme_ipsum()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nsulphates\n\nggplot(wine_processed, aes(x=sulphates, y=quality)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  theme_ipsum()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nalcohol\n\nggplot(wine_processed, aes(x=alcohol, y=quality)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  theme_ipsum()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nEn nuestro conjunto de datos original todas las variables son contínuas. El cálculo de medidas de resumen bivariadas como las covarianzas o coeficientes de correlación, así como los resúmenes gráficos vistos, nos permiten tener una idea bien formada de si existen relaciones entre las variables y el sentido de dichas relaciones.\n\n\n\nEscenario 2: Asociación entre una variable categórica y una variable contínua\n\nEn este escenario no podemos calcular covarianzas ni correlaciones de Pearson, luego debemos disponer de otro conjunto de herramientas para testear las relaciones entre las variables.\n\nCorrelación biserial-puntual\nRegresión logística\nPrueba de Kruskall-Wallis\nEntre otras\n\nPor facilidad, haremos una prueba de Kruskall-Wallis cuyo p-valor testeará la hipótesis de si existe una relación significante entre las variables.\n\nHipótesis nula: las variables son independientes.\nHipótesis alternativa: las variables no son independientes (puede existir un efecto causal).\n\n\nkruskal.test(wine_processed$volatile_acidity, wine_processed$calidad)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  wine_processed$volatile_acidity and wine_processed$calidad\nKruskal-Wallis chi-squared = 168.44, df = 2, p-value &lt; 2.2e-16\n\nkruskal.test(wine_processed$citric_acid, wine_processed$calidad)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  wine_processed$citric_acid and wine_processed$calidad\nKruskal-Wallis chi-squared = 86.083, df = 2, p-value &lt; 2.2e-16\n\nkruskal.test(wine_processed$sulphates, wine_processed$calidad)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  wine_processed$sulphates and wine_processed$calidad\nKruskal-Wallis chi-squared = 147.44, df = 2, p-value &lt; 2.2e-16\n\nkruskal.test(wine_processed$alcohol, wine_processed$calidad)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  wine_processed$alcohol and wine_processed$calidad\nKruskal-Wallis chi-squared = 234.32, df = 2, p-value &lt; 2.2e-16\n\n\nVisualización de las relaciones\nEn este caso podemos construir boxplot por grupos para cruzar la variable crítica calidad con las variables de interés.\n\nggplot(data = wine_processed) +\n  aes(x = calidad, y = volatile_acidity) +\n  geom_boxplot(fill = \"#0c4c8a\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(data = wine_processed) +\n  aes(x = calidad, y = citric_acid) +\n  geom_boxplot(fill = \"#0c4c8a\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(data = wine_processed) +\n  aes(x = calidad, y = sulphates) +\n  geom_boxplot(fill = \"#0c4c8a\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(data = wine_processed) +\n  aes(x = calidad, y = alcohol) +\n  geom_boxplot(fill = \"#0c4c8a\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nEscenario 3: Asociación entre dos variables categóricas\nEn este caso conviene hacer análisis mediante tablas de contingencia, las cuales cuentan las frecuencias observadas en cada categoría.\n\ntbl = table(wine_processed$acetico, wine_processed$calidad) \ntbl\n\n      \n       baja media alta\n  bajo   34  1122  210\n  alto   29   197    7\n\n# Damos nombre a las columnas y las filas \ncolnames(tbl) &lt;- c(\"Calidad baja\", \"Calidad media\", \"Calidad alta\")\nrownames(tbl) &lt;- c(\"Ácido acético bajo\",\"Ácido acético alto\")\ntbl\n\n                    \n                     Calidad baja Calidad media Calidad alta\n  Ácido acético bajo           34          1122          210\n  Ácido acético alto           29           197            7\n\n\nAl tener conformada la tabla de contingencia, la forma de revisar si existe una asociación entre las variables es por medio de una prueba de independencia X2 (Chi-Cuadrado).\nLa prueba indicará si dos características son independientes o tienen una asociación, de manera que las frecuencias elevadas en una de ellas suele ser acompañado con frecuencias altas en la otra.\n\nHipótesis nula: las columnas y las filas de la tabla son independientes.\nHipótesis alternativa: las columnas y las filas son dependientes (puede existir un efecto causal).\n\n\n## Prueba Chi-Cuadrado\nchisq.test(tbl)\n\n\n    Pearson's Chi-squared test\n\ndata:  tbl\nX-squared = 72.67, df = 2, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "program_05_asociacion.html#observaciones-de-cierre",
    "href": "program_05_asociacion.html#observaciones-de-cierre",
    "title": "Asociación e independencia entre variables",
    "section": "Observaciones de cierre",
    "text": "Observaciones de cierre\nIdentificar las relaciones existentes entre dos o más variables es parte arte y parte ciencia, por lo que se recomienda ganar experiencia leyendo articulos cientificos y viendo soluciones a diversos problemas.\nAdemás,\n\nHay que procurar trabajar con variables informativas, es decir, que guarden una relación con la variable objetivo.\nHay que evitar las redundancias, luego lo ideal es que nuestras variables explicativas/independientes/features sean independientes entre sí.\nNuestra intuición puede fallar en dimensiones superiores a 3. En la mayoría de los casos aumentar la cantidad de variables afecta negativamente el entendimiento de un problema si no contamos con una gran cantidad de datos. Por ultimo, una cantidad controlada de variables asegura una mejor interpretabilidad de los análisis y modelos."
  }
]