[
  {
    "objectID": "unidad_04.html",
    "href": "unidad_04.html",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "En esta etapa se evalúan los resultados de los métodos de reducción de dimensiones presentados en la segunda etapa del proyecto. En el material del curso es posible encontrar las bases teóricas de los métodos de evaluación implementados en esta etapa, es importante consultarlo en caso de inquietudes. Al igual que las etapas anteriores, la implementación de los métodos de evaluación se realiza en lenguajes para el manejo de datos R y Python.\nLos métodos aplicados para la evaluación de los resultados de los métodos de reducción de dimensiones son: proporción de la variabilidad y test de Mantel.\n\n\nLa cuarta etapa del proyecto está orientada a cumplir el cuarto objetivo específico:\n\nEvaluar los resultados de los algoritmos de reducción de dimensiones y crear las visualizaciones e interpretaciones correspondientes."
  },
  {
    "objectID": "unidad_04.html#objetivo-actual",
    "href": "unidad_04.html#objetivo-actual",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "La cuarta etapa del proyecto está orientada a cumplir el cuarto objetivo específico:\n\nEvaluar los resultados de los algoritmos de reducción de dimensiones y crear las visualizaciones e interpretaciones correspondientes."
  },
  {
    "objectID": "unidad_04.html#análisis-de-componentes-principales",
    "href": "unidad_04.html#análisis-de-componentes-principales",
    "title": "Proyecto: The joy of programming",
    "section": "Análisis de Componentes principales",
    "text": "Análisis de Componentes principales\nAplicamos el análisis de componentes principales al conjunto de datos.\n\nRPython\n\n\n\nPCA(tb_pinturas_caract, graph = FALSE, ncp = 2) -&gt; ls_pca_resultado\n\nls_pca_resultado %&gt;% \n  pluck(\"ind\", \"coord\") %&gt;% \n  as_tibble() %&gt;% \n  setNames(c(\"X\", \"Y\")) -&gt; tb_pca\n\nEste código realiza un análisis de Componentes Principales (PCA) sobre el dataframe tb_pinturas_caract y posteriormente manipula los resultados para obtener un nuevo dataframe llamado tb_pca. A continuación, se presenta la explicación paso a paso:\nPCA(tb_pinturas_caract, ...): Se utiliza la función PCA del paquete FactoMineR para realizar un análisis de componentes crincipales en el dataframe tb_pinturas_caract.\nPCA(..., graph = FALSE, ncp = 2): El argumento graph = FALSE indica que no se deben generar gráficos durante el análisis. El argumento ncp = 2 especifica que se deben retener los dos primeros componentes principales.\n-&gt; ls_pca_resultado: El resultado es una lista que se almacena en el objeto ls_pca_resultado.\nls_pca_resultado %&gt;% pluck(\"ind\", \"coord\"): La función pluck se utiliza para extraer las coordenadas de las observaciones del resultado del PCA.\n%&gt;% as_tibble(): Se utiliza la función as_tibble para convertir las coordenadas a un formato de tibble.\n%&gt;% setNames(c(\"X\", \"Y\")): La función setNames se usa para renombrar las columnas del tibble como “X” y “Y”.\n-&gt; tb_pca: El resultado final es un datframe que se almacena en el objeto tb_pca.\n\n\n\narr_pinturas_standar = StandardScaler().fit_transform(tb_pinturas_caract)\n\n\nmod_pca = PCA(n_components=10)\nmod_pca = mod_pca.fit(arr_pinturas_standar)\n\ntb_pca = pd.DataFrame(\n    data    = mod_pca.transform(arr_pinturas_standar)[: , [0, 1]],\n    columns = [\"X\", \"Y\"]\n)\n\nEste código utiliza la librería scikit-learn para realizar un análisis de Componentes Principales (PCA) sobre el DataFrame tb_pinturas_caract y crea un nuevo DataFrame llamado tb_pca. A continuación, se presenta la explicación paso a paso:\narr_pinturas_standar =: El resultado de esta operacíon, que es un arreglo, se guarda en el objeto arr_pinturas_standar.\n**StandardScaler():** Se utilizaStandardScalerpara estandarizar el dataframetb_pinturas_caract`. Esto significa que cada característica (columna) se ajusta para tener una media de cero y una desviación estándar de uno.\n.fit_transform(tb_pinturas_caract): El método fit_transform realiza el ajuste y la transformación en una sola llamada.\nmod_pca =: El objeto resultante es un modelo, que se guarda en el objeto mod_pca.\nPCA(n_components=10): Se crea una instancia de la clase PCA con la especificación de retener dos componentes principales (n_components=2).\nmod_pca = mod_pca.fit(arr_pinturas_standar): Luego, se ajusta el modelo a los datos estandarizados utilizando el método fit.\nmod_pca.transform(arr_pinturas_standar): Se utiliza el modelo PCA entrenado para transformar los datos estandarizados. Esto significa proyectar los datos originales en el espacio de las dos primeras componentes principales.\ntb_pca = pd.DataFrame(data = ..., columns = ...): Se crea un nuevo dataframe llamado tb_pca utilizando las componentes principales obtenidas. Este dataframe tiene dos columnas, “X” y “Y”, que representan las dos dimensiones principales del espacio de los componentes principales."
  },
  {
    "objectID": "unidad_04.html#t-sne",
    "href": "unidad_04.html#t-sne",
    "title": "Proyecto: The joy of programming",
    "section": "t-SNE",
    "text": "t-SNE\nRealizamos la reducción a 2 dimensiones aplicando el algoritmo t-SNE a nuestro dataset de características tb_pinturas_caract. En este ejemplo, utilizamos un perplexity = 20, pero podríamos utilizar cualquier otro valor entre 1 y 50.\n\nRPython\n\n\n\ntsne(\n  dist(tb_pinturas_caract),\n  perplexity = 20, \n  k = 2, \n  initial_dims = ncol(tb_pinturas_caract)\n) -&gt; mt_tsne_resultado\n\nmt_tsne_resultado %&gt;% \n  as_tibble(.name_repair = \"minimal\") %&gt;% \n  setNames(c(\"X\", \"Y\")) -&gt; tb_tsne\n\ntsne(dist(tb_pinturas_caract),...): Se utiliza la función tsne para aplicar el método t-SNE a la matriz de distancias de las características de las pinturas contenidas en tb_pinturas_caract. Se especifican parámetros como la perplexidad, el número de dimensiones, y las dimensiones iniciales.\n... -&gt; mt_tsne_resultado: El resultado de la aplicación de t-SNE es una matriz, que se asigna al objeto mt_tsne_resultado.\nmt_tsne_resultado %&gt;% as_tibble(.name_repair = \"minimal\"):  Se utiliza %&gt;% para encadenar operaciones. El resultado de t-SNE se transforma a un dataframe mediante la función as_tibble.\n... %&gt;% setNames(c(\"X\", \"Y\")) -&gt; tb_tsne: Se renombran las columnas como “X” y “Y”. El resultado es un dataframe que se almacena en el objeto tb_tsne.\n\n\n\nmod_tsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=20)\n\nmt_tsne_resultado = mod_tsne.fit_transform(tb_pinturas_caract)\n\ntb_tsne = pd.DataFrame(\n    data    = mt_tsne_resultado,\n    columns = [\"X\", \"Y\"]\n)\n\nmod_tsne = TSNE(n_components=2, ...): Se instancia un modelo t-SNE utilizando la clase TSNE del paquete sklearn. Se especifican parámetros como el número de componentes, la tasa de aprendizaje, el método de inicialización y la perplexidad. La instancia es un modelo que se guarda en el objeto mod_tsne.\nmod_tsne.fit_transform(tb_pinturas_caract): Se ajusta el modelo t-SNE usando los datos de las pinturas contenidas en tb_pinturas_caract utilizando el método fit_transform. Esto realiza el proceso de reducción de dimensionalidad y devuelve las coordenadas en el espacio de baja dimensión en formato de matriz, que se guardae en el objeto mt_tsne_resultado.\ntb_tsne = pd.DataFrame(...): Se crea un DataFrame llamado tb_tsne con las coordenadas resultantes del t-SNE, asignando nombres a las columnas como “X” y “Y”."
  },
  {
    "objectID": "unidad_04.html#proporción-de-variabilidad-conservada",
    "href": "unidad_04.html#proporción-de-variabilidad-conservada",
    "title": "Proyecto: The joy of programming",
    "section": "Proporción de variabilidad conservada",
    "text": "Proporción de variabilidad conservada\nCalculamos la varianza recogida por el análisis de componentes principales.\n\nRPython\n\n\n\nls_pca_resultado %&gt;% \n  get_eig %&gt;% \n  head(10) -&gt; tb_varianza\n\n\n\n\n\n\n\n\n\n\n\n\n\neigenvalue\nvariance.percent\ncumulative.variance.percent\n\n\n\n\nDim.1\n5.56\n8.42\n8.42\n\n\nDim.2\n3.45\n5.22\n13.64\n\n\nDim.3\n3.05\n4.62\n18.26\n\n\nDim.4\n2.34\n3.54\n21.80\n\n\nDim.5\n2.16\n3.27\n25.07\n\n\nDim.6\n2.06\n3.12\n28.19\n\n\nDim.7\n1.98\n3.00\n31.19\n\n\nDim.8\n1.88\n2.85\n34.04\n\n\nDim.9\n1.80\n2.73\n36.77\n\n\nDim.10\n1.68\n2.54\n39.31\n\n\n\n\n\nls_pca_resultado %&gt;% get_eig: Extrae los valores propios (eigenvalues) del análisis de componentes principales mediante get_eig.\n... %&gt;% head(10): Toma las primeras 10 filas de estos valores propios usando head(10).\n... -&gt; tb_varianza: El resultado se almacena en el objeto tb_varianza.\n\n\n\ntb_varianza = pd.DataFrame({\n  'Componente': \n    np.array(range(len(mod_pca.explained_variance_ratio_))) + 1,\n  'Varianza Explicada': mod_pca.explained_variance_ratio_,\n  'Varianza acumulada': mod_pca.explained_variance_ratio_.cumsum()\n})\n\n\n\n\n\n\n\nComponente\nVarianza Explicada\nVarianza acumulada\n\n\n\n\n0\n1\n0.084240\n0.084240\n\n\n1\n2\n0.052203\n0.136443\n\n\n2\n3\n0.046177\n0.182620\n\n\n3\n4\n0.035389\n0.218008\n\n\n4\n5\n0.032660\n0.250668\n\n\n5\n6\n0.031196\n0.281864\n\n\n6\n7\n0.030028\n0.311892\n\n\n7\n8\n0.028464\n0.340356\n\n\n8\n9\n0.027341\n0.367697\n\n\n9\n10\n0.025430\n0.393127\n\n\n\n\n\ntb_varianza = pd.DataFrame(...): Se crea un dataframe llamado tb_varianza que contiene la varianza explicada y la varianza acumulada asociadas con cada componente principal.\nmod_pca.explained_variance_ratio_: Proporciona la proporción de varianza explicada por cada componente.\nmod_pca.explained_variance_ratio_.cumsum(): Proporciona la varianza acumulada.\n\n\n\nA partir de la varianza recogida generamos el gráfico de sedimentación\n\nRPython\n\n\n\nfviz_screeplot(ls_pca_resultado)\n\n\n\n\n\n\n\n\nEl gráfico de sedimentación muestra la proporción de varianza explicada por cada componente principal en el eje y contra el número de componente principal en el eje x. Este gráfico es útil para determinar cuántos componentes principales retener en función de la cantidad de varianza que explican.\nfviz_screeplot:  es una función de visualización que se utiliza para trazar el gráfico de sedimentación de un análisis de componentes principales (PCA).\nls_pca_resultado: es el resultado del análisis de componentes principales previo.\n\n\n\nplt.plot(tb_varianza[\"Componente\"], tb_varianza[\"Varianza Explicada\"], 'ok-')\nplt.bar(tb_varianza[\"Componente\"], tb_varianza[\"Varianza Explicada\"])\nplt.xlabel('Número de Componentes Principales')\nplt.ylabel('Varianza Explicada')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\nplt.plot(..., 'ok-'): - Se utiliza plt.plot para trazar una línea que conecta los puntos (número de componente principal, proporción de varianza explicada). Se especifica 'ok-' para que los puntos sean de color negro ('k'), redondos ('o') y conectados por líneas ('-').\nplt.bar(...): Se utiliza plt.bar para trazar un gráfico de barras de la proporción de varianza explicada por cada componente principal. Esto proporciona una representación visual adicional de la distribución de la varianza entre los componentes principales.\nplt.xlabel(...); plt.ylabel(...): Se añaden etiquetas al eje x y al eje y,\nplt.show(); plt.close(): Se muestra la figura y luego se cierra."
  },
  {
    "objectID": "unidad_04.html#test-de-mantel",
    "href": "unidad_04.html#test-de-mantel",
    "title": "Proyecto: The joy of programming",
    "section": "Test de Mantel",
    "text": "Test de Mantel\nLa prueba de Mantel permite establecer la correlación entre dos matrices de distancias con un nivel de significancia dado.\n\nRPython\n\n\n\ntb_pinturas_caract %&gt;% dist -&gt; mt_dist_pinturas\n\ntb_pca %&gt;% dist -&gt; mt_dist_pca\n\nmantel.randtest(\n   mt_dist_pinturas, mt_dist_pca, \n   nrepet = 1000\n   ) -&gt; mantel_result_pca\n\nmantel_result_pca\n\nMonte-Carlo test\nCall: mantel.randtest(m1 = mt_dist_pinturas, m2 = mt_dist_pca, nrepet = 1000)\n\nObservation: 0,6786 \n\nBased on 1000 replicates\nSimulated p-value: 0,000999001 \nAlternative hypothesis: greater \n\n     Std.Obs  Expectation     Variance \n2,630284e+01 4,859162e-04 6,646608e-04 \n\ntb_tsne %&gt;% dist -&gt; mt_dist_tsne\n\nmantel.randtest(\n   mt_dist_pinturas, mt_dist_tsne, \n   nrepet = 1000\n   ) -&gt; mantel_result_tsne\n\nmantel_result_tsne\n\nMonte-Carlo test\nCall: mantel.randtest(m1 = mt_dist_pinturas, m2 = mt_dist_tsne, nrepet = 1000)\n\nObservation: 0,6670998 \n\nBased on 1000 replicates\nSimulated p-value: 0,000999001 \nAlternative hypothesis: greater \n\n     Std.Obs  Expectation     Variance \n3,482720e+01 3,515885e-04 3,665106e-04 \n\n\nEste código realiza pruebas de Mantel para comparar la similitud entre las matrices de distancia de las características originales (tb_pinturas_caract) con las matrices de distancia de las coordenadas en el espacio de componentes principales (tb_pca) y en el espacio de t-SNE (tb_tsne).\ntb_pinturas_caract %&gt;% dist -&gt; mt_dist_pinturas: Calcula la matriz de distancias entre las filas de tb_pinturas_caract y almacena el resultado en mt_dist_pinturas. El operador %&gt;% (pipe) se utiliza para pasar el resultado de una operación como argumento a la siguiente.\ntb_pca %&gt;% dist -&gt; mt_dist_pca: Calcula la matriz de distancias entre las filas de tb_pca, que contiene las coordenadas de las observaciones en el espacio de componentes principales, y almacena el resultado en mt_dist_pca.\nmantel.randtest(mt_dist_pinturas, mt_dist_pca, ...): Realiza una prueba de Mantel entre las matrices de distancia mt_dist_pinturas y mt_dist_pca; mantel.randtest del paquete ade4 es la función que realiza una prueba de Mantel.\nmantel.randtest(..., nrepet = 1000): indica que se deben realizar 1000 permutaciones aleatorias para calcular el p-valor.\nmantel_result_pca: Almacena los resultados de la prueba de Mantel entre las distancias de tb_pinturas_caract y tb_pca.\ntb_tsne %&gt;% dist -&gt; mt_dist_tsne: Calcula la matriz de distancias entre las filas de tb_tsne, que contiene las coordenadas de las observaciones en el espacio de t-SNE, y almacena el resultado en mt_dist_tsne.\nmantel.randtest(mt_dist_pinturas, mt_dist_tsne, ...): Realiza una prueba de Mantel entre las matrices de distancia mt_dist_pinturas y mt_dist_tsne; mantel.randtest del paquete ade4 es la función que realiza una prueba de Mantel.\nmantel.randtest(..., nrepet = 1000): Se utilizan 1000 permutaciones aleatorias para calcular el p-valor.\nmantel_result_tsne: Almacena los resultados de la prueba de Mantel entre las distancias de tb_pinturas_caract y tb_tsne.\n\n\n\nmt_dist_pinturas = distance_matrix(tb_pinturas_caract.values, tb_pinturas_caract.values)\n\nmt_dist_pca = distance_matrix(tb_pca.values, tb_pca.values)\n\nmantel_result_pca = mantel.test(mt_dist_pinturas, mt_dist_pca, perms=1000, method='pearson', tail='upper')\n\n\nmantel_result_pca.r\n\n0.6785999550226706\n\nmantel_result_pca.p\n\n0.001\n\nmt_dist_tsne = distance_matrix(tb_tsne.values, tb_tsne.values)\n\nmantel_result_tsne = mantel.test(mt_dist_pinturas, mt_dist_tsne, perms=1000, method='pearson', tail='upper')\n\n\nmantel_result_tsne.r\n\n0.6791782039610739\n\nmantel_result_tsne.p\n\n0.001\n\n\nEste código realiza pruebas de Mantel para comparar la similitud entre las matrices de distancia euclidiana de las características originales (tb_pinturas_caract) con las matrices de distancia euclidiana de las coordenadas en el espacio de componentes principales (tb_pca) y en el espacio de t-SNE (tb_tsne).\nmt_dist_pinturas = distance_matrix(...): Calcula la matriz de distancias euclidianas entre todas las filas de tb_pinturas_caract. Guarda el resultado en el objeto mt_dist_pinturas.\ntb_pinturas_caract.values: se utiliza para obtener la representación de matriz de los datos.\nmt_dist_pca = distance_matrix(): Similar a la línea anterior, calcula la matriz de distancias euclidianas entre todas las filas de tb_pca. Guarda el resultado en el objeto mt_dist_pca.\ntb_pca.values: se utiliza para obtener la representación de matriz de las coordenadas en el espacio de componentes principales.\nmantel_result_pca = mantel.test(...): Realiza una prueba de Mantel entre las matrices de distancia; mantel.test del paquete mantel es la función que realiza una prueba de Mantel. El resultado se guarda en el objeto mantel_result_pca.\nmantel.test(mt_dist_pinturas, mt_dist_pca,...): las matrices de distancia evaluadas son la matriz de distancias de los datos de las pinturas mt_dist_pinturas y la matriz de distancias de la representación de los datos en el espacio de componentes principales mt_dist_pca.\nmantel.test(..., perms, method, tail): Se utilizan los siguientes argumentos para la función: perms=1000 indica que se deben realizar 1000 permutaciones aleatorias para calcular el p-valor; method='pearson' especifica que se debe usar la correlación de Pearson como medida de asociación; tail='upper' indica que se está interesado en una prueba de una cola (una dirección).\nmantel_result_pca.r: Muestra el coeficiente de correlación de Mantel obtenido en la prueba de Mantel para tb_pca.\nmantel_result_pca.p: Muestra el p-valor asociado con la prueba de Mantel para tb_pca.\nmt_dist_tsne = distance_matrix(...): Calcula la matriz de distancias euclidianas entre todas las filas de tb_tsne. Guarda el resultado en el objeto mt_dist_tsne.\ntb_tsne.values: se utiliza para obtener la representación de matriz de las coordenadas en el espacio de t-SNE.\nmantel_result_tsne = mantel.test(...): Realiza una prueba de Mantel entre las matrices de distancia; mantel.test del paquete mantel es la función que realiza una prueba de Mantel. El resultado se guarda en el objeto mantel_result_tsne.\nmantel.test(mt_dist_pinturas, mt_dist_tsne, ...): las matrices de distancia evaluadas son la matriz de distancias de los datos de las pinturas mt_dist_pinturas y la matriz de distancias de la representación de los datos en el espacio t-SNE mt_dist_tsne.\nmantel.test(..., perms, method, tail): Utiliza los mismos parámetros que la prueba de Mantel anterior.\nmantel_result_tsne.r: Muestra el coeficiente de correlación de Mantel obtenido en la prueba de Mantel para tb_tsne.\nmantel_result_tsne.p: Muestra el p-valor asociado con la prueba de Mantel para tb_tsne."
  },
  {
    "objectID": "unidad_04.html#círculo-de-correlaciones",
    "href": "unidad_04.html#círculo-de-correlaciones",
    "title": "Proyecto: The joy of programming",
    "section": "Círculo de correlaciones",
    "text": "Círculo de correlaciones\nEl círculo de correlaciones nos ayuda a interpretar el análisis de componentes principales. Las variables son representadas por medio de flechas que se distribuyen en el círculo unitario.\n\nCada variable se representa por medio de una flecha.\nLa dirección de la flecha indica la dirección del plano hacia donde esa variable crece.\nLa longitud de la flecha indica el nivel de representación que tiene esta variable en el plano de los componentes principales.\nEl ángulo entre dos flechas permite intuir su correlación. Flechas con un ángulo muy agudo suelen representar variables con una correlación positiva fuerte. Flechas opuestas, con un ángulo muy obtuso o muy abierto, suelen representar variables con una alta correlación negativa; y flechas que presentan un ángulo recto están generalmente asociadas a variables con una correlación cercana a cero.\n\n\nRPython\n\n\n\nfviz_pca_var(ls_pca_resultado, geom = \"arrow\")\n\n\n\n\n\n\n\nfviz_pca_var(\n  ls_pca_resultado,\n  repel = TRUE,\n  select.var = list(cos2 = 0.15)\n  )\n\n\n\n\n\n\n\n\nEste código genera dos visualizaciones del análisis de componentes principales. La primera muestra la contribución de todas las variables mediante flechas en el espacio de componentes principales. La segunda visualización resalta específicamente las variables con una contribución significativa, utilizando el parámetro select.var para filtrar las variables basándose en su coseno cuadrado. El uso de repel = TRUE ayuda a mejorar la legibilidad de las etiquetas evitando solapamientos.\nfviz_pca_var(...): La función fviz_pca_var se utiliza para visualizar la contribución de las variables originales en el espacio de componentes principales.\nls_pca_resultado: es el resultado de un análisis de componentes principales previo.\ngeom = \"arrow\": El parámetro geom = \"arrow\" indica que se deben utilizar flechas para representar la contribución de las variables.\nrepel = TRUE: indica que las etiquetas de las variables deben colocarse de manera que eviten solapamientos.\nselect.var = list(cos2 = 0.15): selecciona las variables cuyo coseno cuadrado (cos2) es mayor o igual a 0.15, lo que significa que estas variables tienen una contribución sustancial en al menos uno de los ejes principales.\n\n\n\n(fig, ax) = plt.subplots(figsize=(8, 8))\nfor i in range(0, mod_pca.components_.shape[1]):\n  ax.arrow(\n    0, 0, \n    mod_pca.components_[0, i]*1.5,  \n    mod_pca.components_[1, i]*1.5,  \n    head_width=0.05,\n    head_length=0.08, fc='steelblue', ec='steelblue'\n  )\n    \nan = np.linspace(0, 2 * np.pi, 100)\nplt.plot(np.cos(an), np.sin(an))  \nplt.axis('equal')\nax.set_title('Variable factor map')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\n\n\nthreshold = 0.15\nmax_representation = (np.abs(mod_pca.components_)\n  .take([0, 1], axis=0)\n  .max(0))\n\ncondition = np.where(max_representation &gt; threshold)[0]\n\n(fig, ax) = plt.subplots(figsize=(8, 8))\nfor i in condition:\n  ax.arrow(\n    0, 0, \n    mod_pca.components_[0, i]*1.5,  \n    mod_pca.components_[1, i]*1.5,  \n    head_width=0.05,\n    head_length=0.08, fc='steelblue', ec='steelblue'\n  )\n    \n  plt.text(\n    mod_pca.components_[0, i]*1.7,\n    mod_pca.components_[1, i]*1.7,\n    tb_pinturas_caract.columns.values[i]\n  )\n\n\nan = np.linspace(0, 2 * np.pi, 100)\nplt.plot(np.cos(an), np.sin(an))  \nplt.axis('equal')\nax.set_title('Variable factor map')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\nEste código crea dos visualizaciones del mapa de factores variables en un espacio de componentes principales. La primera visualización muestra todas las variables, mientras que la segunda resalta las variables con contribución significativa según un umbral predefinido.\n(fig, ax) = plt.subplots(figsize=(8, 8)): Se crea una nueva figura y ejes con un tamaño de 8x8.\nSe dibujan las flechas usando un bucle.\nfor i in range(0, mod_pca.components_.shape[1]):: Se utiliza un bucle for para iterar sobre todas las variables (columnas) en los componentes principales.\nax.arrow(...): Se añade una flecha para cada variable en el espacio de componentes principales.\nSe añade un círculo unitario para proporcionar una escala.\nan = np.linspace(...): Se genera un conjunto de puntos para un círculo completo.\nplt.plot(np.cos(an), np.sin(an)) Se añade un círculo unitario utilizando las funciones seno y coseno.\ny finalmente\nplt.axis('equal'): Se ajusta la escala de los ejes para que tengan la misma proporción.\nax.set_title('Variable factor map'): Se establece el título del gráfico.\nplt.show(); plt.close(): Se muestra la figura y luego se cierra.\nPara el segundo gráfico, seguimos los mismos pasos adicionando un umbral de 0.15.\nthreshold = 0.15: Se define un umbral para la contribución significativa de las variables.\n(np.abs(...).take(...).max(...)): Se calcula la representación máxima en los dos primeros componentes principales para cada variable. Este resultado se guarda en el objeto max_representation.\nnp.where(max_representation &gt; threshold)[0]: Se encuentran las variables con una contribución mayor que el umbral. Este resultado se guarda en el objeto condition.\n(fig, ax) = plt.subplots(figsize=(8, 8)): Se crea una nueva figura y ejes con un tamaño de 8x8.\nSe dibujan las flechas usando un bucle.\nfor i in range(0, mod_pca.components_.shape[1]):: Se utiliza un bucle for para iterar sobre todas las variables (columnas) en los componentes principales.\nax.arrow(...): Se añade una flecha para cada variable en el espacio de componentes principales.\nplt.text(...): Se añaden los nombres de las variables que fueron filtradas por el umbral.\nSe añade un círculo unitario para proporcionar una escala.\nan = np.linspace(...): Se genera un conjunto de puntos para un círculo completo.\nplt.plot(np.cos(an), np.sin(an)) Se añade un círculo unitario utilizando las funciones seno y coseno.\nPor último, se agregan algunos detalles\nplt.axis('equal'): Se ajusta la escala de los ejes para que tengan la misma proporción.\nax.set_title('Variable factor map'): Se establece el título del gráfico.\nplt.show(); plt.close(): Se muestra la figura y luego se cierra."
  },
  {
    "objectID": "unidad_04.html#visualización-de-datos-reducidos",
    "href": "unidad_04.html#visualización-de-datos-reducidos",
    "title": "Proyecto: The joy of programming",
    "section": "Visualización de datos reducidos",
    "text": "Visualización de datos reducidos\nEn el transcurso del proyecto hemos aprendido a visualizar nuestros resultados utilizando las técnicas de reducción de dimensiones. En esta unidad abordamos dos visualizaciones más. El biplot, para visualizar los resultados del análisis de componentes principales, y el diccionario de gráficos para los resultados del algoritmo t-SNE.\nEl biplot consiste en generar, en una sola visualización, la nube de puntos acompañada del círculo de correlaciones. De manera que se interpreta para cada dato en qué variables tiene mayores valores.\n\nRPython\n\n\n\nfviz_pca_biplot(\n  ls_pca_resultado, geom.ind = \"point\", \n  geom.var =  c(\"arrow\", \"text\"), repel = TRUE,\n  select.var = list(cos2 = 0.15)\n)\n\n\n\n\n\n\n\n\nEste código utiliza la función fviz_pca_biplot para crear un biplot que representa las observaciones como puntos y las variables como flechas con etiquetas de texto. Se destacan las variables con un coseno cuadrado (cos2) mayor al 0.15. Este tipo de visualización es común en análisis de componentes principales para entender la relación entre observaciones y variables.\nfviz_pca_biplot: es una función que crea un biplot a partir de los resultados de un análisis de componentes principales.\ngeom.ind = \"point\": indica que las observaciones deben representarse como puntos en el biplot.\ngeom.var = c(\"arrow\", \"text\"): especifica que las variables se deben representar como flechas y con etiquetas de texto en el biplot.\nrepel = TRUE: indica que las etiquetas de las variables deben ajustarse automáticamente para evitar superposiciones.\nselect.var = list(cos2 = 0.15): selecciona las variables basadas en un umbral de coseno cuadrado (cos2). En este caso, las variables con un cos2 mayor al 0.15 serán resaltadas en el biplot.\n\n\n\nthreshold = 0.15\nmax_representation = (np.abs(mod_pca.components_)\n  .take([0, 1], axis=0)\n  .max(0))\n\ncondition = np.where(max_representation &gt; threshold)[0]\n\n(fig, ax) = plt.subplots(figsize=(10, 10))\n\nax.plot(tb_pca[\"X\"], tb_pca[\"Y\"], \"ko\", markersize = 1.5)\n\nfor i in condition:\n  ax.arrow(\n    0, 0, \n    mod_pca.components_[0, i]*10,  \n    mod_pca.components_[1, i]*10,  \n    head_width=0.2,\n    head_length=0.3, fc='steelblue', ec='steelblue'\n  )\n    \n  ax.text(\n    mod_pca.components_[0, i]*15,\n    mod_pca.components_[1, i]*15,\n    tb_pinturas_caract.columns.values[i]\n  )\n\nax.axis('equal')\nax.set_title('Biplot')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\nEste código crea un biplot que muestra tanto las observaciones como las variables en el espacio de los dos primeros componentes principales. Las variables significativas se destacan con flechas y etiquetas en el biplot.\nthreshold = 0.15: Se define un umbral para la contribución significativa de las variables.\n(np.abs(...).take(...).max(...)): Se calcula la representación máxima en los dos primeros componentes principales para cada variable. Este resultado se guarda en el objeto max_representation.\nnp.where(max_representation &gt; threshold)[0]: Se encuentran las variables con una contribución mayor que el umbral. Este resultado se guarda en el objeto condition.\n(fig, ax) = plt.subplots(figsize=(8, 8)): Se crea una nueva figura y ejes con un tamaño de 8x8.\nax.plot(tb_pca[\"X\"], tb_pca[\"Y\"], \"ko\", markersize = 1.5): Se grafican los puntos del espacio de componentes principales.\nMediante un bucle se dibujan las flechas y los textos:\nfor i in range(0, mod_pca.components_.shape[1]):: Se utiliza un bucle for para iterar sobre todas las variables (columnas) en los componentes principales.\nax.arrow(...): Se añade una flecha para cada variable en el espacio de componentes principales.\nplt.text(...): Se añaden los nombres de las variables que fueron filtradas por el umbral.\nSe realizan ajustes a los ejes y al título del gráfico.\nplt.axis('equal'): Se ajusta la escala de los ejes para que tengan la misma proporción.\nax.set_title('Biplot'): Se establece el título del gráfico.\nplt.show(); plt.close(): Se muestra la figura y luego se cierra.\n\n\n\nEn el caso del t-SNE podemos visualizar un gráfico para cada variable de interés. Este diccionario de gráficos nos ayuda a interpretar el resultado, conociendo cuáles variables se expresan mejor en qué regiones del plano.\n\nRPython\n\n\n\nc(\n  \"OCEAN\", \"MOUNTAIN\", \"WINTER\",\"STRUCTURE\",\n  \"TREE\", \"CONIFER\", \"DECIDUOUS\", \"SNOWY_MOUNTAIN\"\n  ) -&gt; nm_items_relevantes\n\ntb_pinturas_caract %&gt;% \n  select(all_of(nm_items_relevantes)) %&gt;% \n  bind_cols(tb_tsne) %&gt;% \n  pivot_longer(\n    all_of(nm_items_relevantes),\n    names_to = \"objeto\",\n    values_to = \"presencia\"\n  ) %&gt;% \n  ggplot + aes(x = X, y = Y, colour = as.character(presencia)) +\n  geom_point() + \n  scale_color_discrete(guide = \"none\", type = c(\"#AA8888\", \"#88aa88\")) +\n  facet_wrap(~objeto, nrow = 4)\n\n\n\n\n\n\n\n\nEste código crea un gráfico de dispersión facetado que representa la presencia o ausencia de objetos relevantes en un espacio bidimensional (X e Y de tb_tsne). Cada panel del gráfico representa un objeto, y los puntos están coloreados según su presencia o ausencia.\nc(...) -&gt; nm_items_relevantes: crea un vector llamado nm_items_relevantes que contiene los nombres de las columnas relevantes.\ntb_pint.. %&gt;% select(all_of(nm_items_relevantes)): selecciona las columnas relevantes del dataframe tb_pinturas_caract\n... %&gt;% bind_cols(tb_tsne): combina las columnas seleccionadas horizontalmente (bind_cols) con el dataframe tb_tsne.\npivot_longer(...): utiliza la función pivot_longer para convertir los datos de formato ancho a largo.\nall_of(nm_items_relevantes): Las columnas definidas en nm_items_relevantes se transforman en las columnas objeto y presencia.\nnames_to = \"objeto\", values_to = \"presencia\": Una nueva columna denominada objeto contiene los nombres de las variables relevantes, mientras una nueva columna denominada presencia contiene sus valores.\nggplot + aes(...) + geom_point() es la estructura básica de un gráfico de dispersión.\naes(x = X, y = Y, colour = as.character(presencia)): Las estéticas (aes) definen la posición en X, Y y el color de los puntos según la presencia de objetos definidos en nm_items_relevantes.\nscale_color_discrete(...) personaliza los colores que señalan la presencia o ausencia de las características relevantes.\nfacet_wrap(~objeto, ..) distribuye el gráfico en diferentes paneles para cada objeto definido en nm_items_relevantes.\n\n\n\nnm_items_relevantes = [[\"CONIFER\", \"DECIDUOUS\"], [\"MOUNTAIN\", \"OCEAN\"], [\"SNOWY_MOUNTAIN\", \"STRUCTURE\"], [\"TREE\",  \"WINTER\"]] \n\n\ncolors = {0: 'rosybrown', 1: 'darkolivegreen'}\n\nfig, ax = plt.subplots(nrows = 4, ncols=2)\n\nfor i, sublist in enumerate(nm_items_relevantes):\n  for j, item in enumerate(sublist):\n    \n    col = [colors[i] for i in tb_pinturas_caract[item].to_list()]\n    \n    ax[i, j].scatter(tb_tsne[\"X\"], tb_tsne[\"Y\"], c = col, s = 2)\n    ax[i, j].set_title(item)\n\nfig.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\nEste código crea subgráficos para cada par de elementos relevantes y visualiza los puntos en un espacio bidimensional. Los puntos están coloreados según la presencia de los elementos en las pinturas, que se registra en el dataframe tb_pinturas_caract; cada subgráfico representa una variable.\nnm_items_relevantes: es una lista de listas que contiene grupos de elementos relevantes.\ncolors: es un diccionario que asigna valores (0 o 1) a colores (‘rosybrown’ o ‘darkolivegreen’).\nfig, ax = plt.subplots(nrows=4, ncols=2): crea subgráficos en una matriz de 4 filas y 2 columnas.\nfor i, sublist in ...: for j, item in ...: Es un bucle anidado itera sobre las listas de objetos en nm_items_relevantes..\nfor i, sublist in ...: for j, item in ...: Es un bucle anidado itera sobre las listas de objetos en nm_items_relevantes.\ncol = [colors[i] for i in ...]: Para cada objeto en la variable item, se asigna un color según el diccionario colors.\n.scatter(...): agrega puntos al gráfico de dispersión en los subgráficos.\nset_title(...): Se establece el título de cada subgráfico como el nombre del objeto.\nfig.tight_layout(): ajusta automáticamente la disposición de los subgráficos para evitar superposiciones.\nplt.show(); plt.close(): Se muestra la figura y luego se cierra."
  },
  {
    "objectID": "unidad_02.html",
    "href": "unidad_02.html",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "En esta etapa aplicamos los algoritmos de reducción de dimensiones presentados a nuestro dataset de pinturas de Bob Ross. Podemos repasar la explicación teórica de estos algoritmos en el material del curso. La implementación que se presenta a continuación, se ejecuta en dos lenguajes R y Python.\nLos algoritmos que se presentan en esta etapa son análisis de componentes principales y t-SNE. La aplicación de estos algoritmos tiene finalidad la comprensión de los datos a través de distintas visualizaciones; que resumen las características de las pinturas, los elementos que estas contienen y su estructura de similaridad. Además, estas expresiones gráficas se utilizan para visualizar los grupos resultantes de la etapa anterior.\n\n\nLa segunda etapa del proyecto está orientada a cumplir el segundo objetivo específico:\n\nAplicar algoritmos de reducción de dimensiones a las pinturas con el fin de crear visualizaciones e interpretaciones."
  },
  {
    "objectID": "unidad_02.html#objetivo-actual",
    "href": "unidad_02.html#objetivo-actual",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "La segunda etapa del proyecto está orientada a cumplir el segundo objetivo específico:\n\nAplicar algoritmos de reducción de dimensiones a las pinturas con el fin de crear visualizaciones e interpretaciones."
  },
  {
    "objectID": "unidad_02.html#análisis-de-componentes-principales",
    "href": "unidad_02.html#análisis-de-componentes-principales",
    "title": "Proyecto: The joy of programming",
    "section": "Análisis de componentes principales",
    "text": "Análisis de componentes principales\nIniciamos aplicando el análisis de componentes principales al conjunto de datos.\n\nRPython\n\n\n\nPCA(tb_pinturas_caract, graph = FALSE, ncp = 2) -&gt; ls_pca_resultado\n\nls_pca_resultado %&gt;% \n  pluck(\"ind\", \"coord\") %&gt;% \n  as_tibble() %&gt;% \n  setNames(c(\"X\", \"Y\")) -&gt; tb_pca\n\nEste código realiza un análisis de Componentes Principales (PCA) sobre el DataFrame tb_pinturas_caract y posteriormente manipula los resultados para obtener un nuevo DataFrame llamado tb_pca. A continuación, se presenta la explicación paso a paso:\n\nRealizar el Análisis de Componentes Principales (PCA):\n\nSe utiliza la función PCA del paquete FactoMineR para realizar un análisis de Componentes Principales en el DataFrame tb_pinturas_caract.\nLos parámetros graph = FALSE indican que no se deben generar gráficos durante el análisis.\nEl parámetro ncp = 2 especifica que se deben retener las dos primeras componentes principales.\nEl resultado se almacena en la variable ls_pca_resultado.\n\nManipular los Resultados del PCA:\n\nSe utiliza la tubería (%&gt;%) junto con las funciones de purrr y dplyr para realizar varias operaciones en los resultados del PCA.\nls_pca_resultado %&gt;% pluck(\"ind\", \"coord\"):\n\nLa función pluck se utiliza para extraer las coordenadas de las observaciones del resultado del PCA.\n\n%&gt;% as_tibble():\n\nSe utiliza la función as_tibble para convertir las coordenadas a un formato de tibble.\n\n%&gt;% setNames(c(\"X\", \"Y\")):\n\nLa función setNames se usa para renombrar las columnas del tibble como “X” y “Y”.\n\nEl resultado final se almacena en el DataFrame tb_pca.\n\n\n\n\n\narr_pinturas_standar = StandardScaler().fit_transform(tb_pinturas_caract)\n\n\nmod_pca = PCA(n_components=2)\nmod_pca = mod_pca.fit(arr_pinturas_standar)\n\ntb_pca = pd.DataFrame(\n    data    = mod_pca.transform(arr_pinturas_standar),\n    columns = [\"X\", \"Y\"]\n)\n\nEste código utiliza la librería scikit-learn para realizar un análisis de Componentes Principales (PCA) sobre el DataFrame tb_pinturas_caract y crea un nuevo DataFrame llamado tb_pca. A continuación, se presenta la explicación paso a paso:\nExplicación paso a paso del código:\n\nEstandarizar los Datos:\nSe utiliza StandardScaler para estandarizar el DataFrame tb_pinturas_caract. Esto significa que cada característica (columna) se ajusta para tener una media de cero y una desviación estándar de uno. El método fit_transform realiza el ajuste y la transformación en una sola llamada.\nCrear y Ajustar el Modelo PCA:\nSe crea una instancia de la clase PCA con la especificación de retener dos componentes principales (n_components=2). Luego, se ajusta el modelo a los datos estandarizados utilizando el método fit.\nTransformar los Datos con PCA:\nSe utiliza el modelo PCA entrenado para transformar los datos estandarizados. Esto significa proyectar los datos originales en el espacio de las dos primeras componentes principales.\nCrear el DataFrame tb_pca:\nSe crea un nuevo DataFrame llamado tb_pca utilizando las componentes principales obtenidas. Este DataFrame tiene dos columnas, “X” y “Y”, que representan las dos dimensiones principales del espacio transformado por PCA.\n\n\n\n\nAhora tenemos un nuevo dataframe tb_pca en el que guardamos las coordenadas de nuestras pinturas. Vamos a visualizarlas:\n\nRPython\n\n\n\ntb_pca %&gt;% \n  ggplot +\n  aes(x = X, y  = Y) +\n  geom_point()\n\n\n\n\n\n\n\n\nEste código genera un gráfico de dispersión utilizando las coordenadas “X” y “Y” del DataFrame tb_pca. Cada punto en el gráfico representa una observación en el espacio de las dos primeras componentes principales obtenidas a través del análisis de Componentes Principales (PCA). Explicación paso a paso del código:\n\nOperador %&gt;% (pipe):\n\nEl operador %&gt;% se utiliza para encadenar las operaciones, pasando el resultado de una operación como entrada a la siguiente.\n\nggplot:\n\nSe inicia la construcción de un gráfico utilizando la librería ggplot2.\n\naes(x = X, y = Y):\n\nSe especifica que las coordenadas “X” y “Y” del DataFrame tb_pca se utilizarán como ejes x e y, respectivamente.\n\ngeom_point():\n\nAgrega una capa de puntos al gráfico, creando un gráfico de dispersión.\n\n\n\n\n\nsns.scatterplot(x = \"X\", y = \"Y\", data = tb_pca)\n\n\n\n\n\n\n\n\nEl gráfico de dispersión representa cada punto como una observación del DataFrame tb_pca. Las coordenadas “X” y “Y” se utilizan como posiciones en los ejes x e y, proporcionando una visualización de las dos primeras componentes principales obtenidas a través del análisis de Componentes Principales (PCA) utilizando la librería Seaborn. Explicación paso a paso del código:\n\nsns.scatterplot: Se utiliza la función sns.scatterplot para crear un gráfico de dispersión.\nx=\"X\", y=\"Y\" especifica que las coordenadas “X” y “Y” del DataFrame tb_pca se utilizarán como ejes x e y, respectivamente.\ndata=tb_pca indica que los datos provienen de este DataFrame.\n\n\n\n\nTeniendo los resultados del análisis previo, podemos usar el dataframe tb_grupos en esta visualización para revisar los grupos. Es necesario cambiar la variable indicadora del grupo a texto, de lo contrario no vamos a tener una buena visualización.\n\nRPython\n\n\n\ntb_pca %&gt;%\n  bind_cols(tb_grupos) %&gt;% \n  mutate(\n    # en esta línea podemos cambiar los grupos\n    grupo = as.character(grupos_jerar)\n    ) %&gt;% \n  ggplot +\n  aes(x = X, y  = Y, col = grupo) +\n  geom_point() \n\n\n\n\n\n\n\n\nSe obtiene un gráfico de dispersión donde cada punto representa una observación del DataFrame resultante de combinar tb_pca y tb_grupos. Los puntos se colorean según la columna “grupo”. Este gráfico proporciona una visualización de las dos primeras componentes principales junto con la información de grupos proporcionada por tb_grupos. Explicación paso a paso del código:\n\nUtilizar bind_cols para combinar DataFrames:\n\ntb_pca %&gt;% bind_cols(tb_grupos): Utiliza el operador %&gt;% para encadenar operaciones. bind_cols combina horizontalmente (cbind en R) los DataFrames tb_pca y tb_grupos.\n\nAgregar una Columna de Grupos:\n\nmutate(...): Utiliza mutate para agregar o modificar columnas en el DataFrame resultante de la combinación.\ngrupo = as.character(grupos_jerar): Agrega una nueva columna llamada “grupo”, que se obtiene de la columna grupos_jerar. Se convierte a tipo de dato caracter (as.character) para asegurar que sea tratado como categoría.\n\nCrear un Gráfico de Dispersión con Colores por Grupo:\n\nggplot: Inicia la construcción de un gráfico utilizando la librería ggplot2.\naes(x = X, y = Y, col = grupo): Especifica que las coordenadas “X” y “Y” del DataFrame se utilizarán como ejes x e y, respectivamente, y se asignarán colores según la columna “grupo”.\n\nAgregar Puntos al Gráfico:\n\ngeom_point(): Agrega una capa de puntos al gráfico, creando un gráfico de dispersión.\n\n\n\n\n\ntb_tmp = pd.concat([tb_pca, tb_grupos], axis = 1)\n\n# en esta línea podemos cambiar los grupos\ntb_tmp[\"grupo\"] = tb_tmp[\"grupos_jerar\"].astype(\"string\")\n\nsns.scatterplot(x = \"X\", y = \"Y\", hue = \"grupo\", data = tb_tmp)\n\n\n\n\n\n\n\n\nSe obtiene un gráfico de dispersión donde cada punto representa una observación del DataFrame tb_tmp, con las coordenadas “X” y “Y” como posiciones en los ejes x e y, respectivamente. Los puntos se colorearán según la información de la columna “grupo”, proporcionando una visualización de las dos primeras componentes principales junto con la información de grupos proveniente de tb_grupos. Explicación paso a paso del código:\n\nConcatenar DataFrames Horizontalmente:\n\ntb_tmp = pd.concat([tb_pca, tb_grupos], axis=1): Se utiliza la función concat de Pandas para concatenar horizontalmente (axis=1) los DataFrames tb_pca y tb_grupos. El resultado es almacenado en el DataFrame tb_tmp.\n\nAgregar una Columna de Grupos al DataFrame Resultante:\n\ntb_tmp[\"grupo\"] = tb_tmp[\"grupos_jerar\"].astype(\"string\"): Se agrega una nueva columna llamada “grupo” al DataFrame tb_tmp. Esta columna se obtiene de la columna “grupos_jerar” y se convierte al tipo de dato string (astype(\"string\")).\n\nCrear un Gráfico de Dispersión con Seaborn:\n\nsns.scatterplot(x=\"X\", y=\"Y\", hue=\"grupo\", data=tb_tmp): Se utiliza la función scatterplot de Seaborn para crear un gráfico de dispersión.\n\nx=\"X\", y=\"Y\" especifica que las coordenadas “X” y “Y” del DataFrame tb_tmp se utilizarán como ejes x e y, respectivamente.\nhue=\"grupo\" asigna colores según la columna “grupo”, lo que proporciona información de los grupos en el gráfico.\n\n\n\n\n\n\nA continuación, hacemos una gráfica de las pinturas usando las pinturas. De esta forma vamos a poder revisar cuáles pinturas resultan cercanas y a qué regiones pertenecen. Estas gráficas son computacionalmente exigentes; es recomendable guardarlas (no mostrarlas de inmediato), a menos que estemos trabajando en un computador muy potente.\n\nRPython\n\n\n\ntb_pca %&gt;%\n  mutate(\n    archivos_pinturas = file.path(mi_setup$carpeta_pinturas, dir(mi_setup$carpeta_pinturas))\n  ) %&gt;% ggplot() +\n  geom_image(aes(x = X, y = Y, image = archivos_pinturas), size = 0.03) + \n  theme_void() -&gt; gr_tmp # Es mejor guardar la imagen que abrirla en la sesión\n\nggsave(mi_setup$gr_acp_file, gr_tmp, width = 200, height = 200, units = \"mm\")\n\n\nSe elabora un gráfico de dispersión utilizando las imágenes de las pinturas en vez de puntos. Explicación paso a paso del código:\n\nModificar el DataFrame con mutate:\n\nSe utiliza mutate para agregar una nueva columna llamada “archivos_pinturas” al DataFrame tb_pca. Esta columna contiene la ruta completa de los archivos de pinturas ubicados en la carpeta especificada en mi_setup$carpeta_pinturas.\nLa estructura de tuberías (%&gt;%) se utiliza para encadenar las operaciones, pasando el resultado de una a la siguiente.\n\nCrear un Gráfico con Imágenes:\n\nggplot() + geom_image(...): Se inicia la construcción de un gráfico utilizando la librería ggplot2. Se utiliza geom_image para agregar imágenes al gráfico, utilizando las coordenadas “X” y “Y” del DataFrame tb_pca y la ruta de las imágenes en la columna “archivos_pinturas”.\n\nEstablecer un Tema Visual:\n\ntheme_void(): Aplica un tema visual que elimina elementos como ejes y fondos, dejando solo las imágenes.\n\nGuardar el Gráfico en una Variable:\n\n-&gt; gr_tmp: Se utiliza -&gt; para asignar el gráfico resultante a la variable gr_tmp.\n\nGuardar el Gráfico como Archivo:\n\nggsave(...): Se utiliza ggsave para guardar el gráfico en un archivo. La ruta y el nombre del archivo se especifican en mi_setup$gr_acp_file. Se ajustan también las dimensiones del archivo y la unidad de medida.\n\n\n\n\n\ndef getImage(path):\n    return OffsetImage(plt.imread(path), zoom=.1, alpha = 1)\n\nnombres_archivos_pinturas = os.listdir(mi_setup[\"carpeta_pinturas\"])\n\nnombres_archivos_pinturas.sort()\n\ntb_pca[\"archivos_pinturas\"] = list(map(\n  lambda dir: os.path.join(mi_setup[\"carpeta_pinturas\"],dir),\n  nombres_archivos_pinturas\n  ))\n\nfig, ax = plt.subplots(figsize=(25, 25))\nax.scatter(tb_pca[\"X\"], tb_pca[\"Y\"], color=\"white\")\n\nfor index, row in tb_pca.iterrows():\n  ab = AnnotationBbox(getImage(row[\"archivos_pinturas\"]), (row[\"X\"], row[\"Y\"]), frameon=False)\n  ax.add_artist(ab)\n\n\nplt.savefig(mi_setup[\"gr_acp_file\"], dpi = 100)\n\n\nSe elabora un gráfico de dispersión utilizando las imágenes de las pinturas en vez de puntos. Explicación paso a paso del código:\n\nDefinir la Función getImage:\n\nSe define una función llamada getImage que toma una ruta de archivo como argumento y devuelve un objeto OffsetImage que contiene la imagen con ciertas propiedades de zoom y transparencia.\n\nObtener Nombres de Archivos de Pinturas:\n\nSe obtienen los nombres de los archivos de pinturas en la carpeta especificada en mi_setup[\"carpeta_pinturas\"] y se almacenan en la lista nombres_archivos_pinturas. Esta lista se ordena alfabéticamente.\n\nAsignar Rutas Completas a las Imágenes en el DataFrame:\n\nSe utiliza map junto con lambda para crear una nueva columna en el DataFrame tb_pca llamada “archivos_pinturas”, que contiene las rutas completas de las imágenes de pinturas.\n\nCrear un Gráfico de Dispersión Vacío:\n\nSe crea un gráfico de dispersión vacío utilizando plt.subplots, con un tamaño de figura de 25x25.\n\nAgregar Puntos Blancos al Gráfico:\n\nSe agrega una capa de puntos blancos al gráfico de dispersión en las coordenadas especificadas por las columnas “X” y “Y” del DataFrame tb_pca.\n\nAgregar Imágenes al Gráfico:\n\nSe utiliza un bucle for para iterar sobre las filas de tb_pca. Para cada fila, se crea un objeto AnnotationBbox que contiene la imagen de la pintura y se agrega al gráfico en las coordenadas correspondientes.\n\nGuardar el Gráfico como Archivo:\n\nSe guarda el gráfico como un archivo de imagen en la ruta especificada en mi_setup[\"gr_acp_file\"], con una resolución de 100 dpi.\n\n\n\n\n\nMuy bien, es hora de pasar al t-SNE. Queremos hacer las mismas gráficas pero esta vez con el algoritmo t-SNE y observar los cambios."
  },
  {
    "objectID": "unidad_02.html#t-sne",
    "href": "unidad_02.html#t-sne",
    "title": "Proyecto: The joy of programming",
    "section": "t-SNE",
    "text": "t-SNE\nEn primer lugar realizamos la reducción a 2 dimensiones aplicando el algoritmo t-SNE a nuestro dataset de características tb_pinturas_caract. En este ejemplo, utilizamos un perplexity = 20, pero podríamos utilizar cualquier otro valor entre 1 y 50.\n\nRPython\n\n\n\ntsne(\n  dist(tb_pinturas_caract),\n  perplexity = 20, \n  k = 2, \n  initial_dims = ncol(tb_pinturas_caract)\n) -&gt; mt_tsne_resultado\n\nmt_tsne_resultado %&gt;% \n  as_tibble(.name_repair = \"minimal\") %&gt;% \n  setNames(c(\"X\", \"Y\")) -&gt; tb_tsne\n\nExplicación paso a paso del código:\n\nAplicar el Método t-SNE:\n\nSe utiliza la función tsne para aplicar el método t-SNE a la matriz de distancias de las características de las pinturas contenidas en tb_pinturas_caract. Se especifican parámetros como la perplexidad, el número de dimensiones, y las dimensiones iniciales.\n\nAsignar el Resultado a una Variable:\n\nEl resultado de la aplicación de t-SNE se asigna a la variable mt_tsne_resultado.\n\nTransformar el Resultado a un DataFrame Tibble:\n\nSe utiliza %&gt;% para encadenar operaciones. El resultado de t-SNE se transforma a un DataFrame tibble y se renombran las columnas como “X” y “Y”. El resultado se almacena en el DataFrame tb_tsne.\n\n\n\n\n\nmod_tsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=20)\n\nmt_tsne_resultado = mod_tsne.fit_transform(tb_pinturas_caract)\n\ntb_tsne = pd.DataFrame(\n    data    = mt_tsne_resultado,\n    columns = [\"X\", \"Y\"]\n)\n\nExplicación paso a paso del código:\n\nConfiguración del Modelo t-SNE:\n\nSe instancia un modelo t-SNE utilizando la clase TSNE del paquete sklearn. Se especifican parámetros como el número de componentes, la tasa de aprendizaje, el método de inicialización y la perplexidad.\n\nAjuste del Modelo y Transformación de los Datos:\n\nSe ajusta el modelo t-SNE a las características de las pinturas contenidas en tb_pinturas_caract utilizando el método fit_transform. Esto realiza el proceso de reducción de dimensionalidad y devuelve las coordenadas en el espacio de baja dimensión.\n\nCreación de un DataFrame con los Resultados:\n\nSe crea un DataFrame llamado tb_tsne con las coordenadas resultantes del t-SNE, asignando nombres a las columnas como “X” y “Y”.\n\n\n\n\n\nAhora tenemos un nuevo dataframe tb_tsne en el que guardamos las coordenadas de nuestras pinturas. Vamos a visualizarlas:\n\nRPython\n\n\n\ntb_tsne %&gt;% \n  ggplot +\n  aes(x = X, y  = Y) +\n  geom_point() \n\n\n\n\n\n\n\n\nEste código genera un gráfico de dispersión utilizando las coordenadas “X” y “Y” del DataFrame tb_tsne. Cada punto en el gráfico representa una observación en el espacio bidimensional obtenido a través del algoritmo t-SNE. Explicación paso a paso del código:\n\nOperador %&gt;% (pipe):\n\nEl operador %&gt;% se utiliza para encadenar las operaciones, pasando el resultado de una operación como entrada a la siguiente.\n\nggplot:\n\nSe inicia la construcción de un gráfico utilizando la librería ggplot2.\n\naes(x = X, y = Y):\n\nSe especifica que las coordenadas “X” y “Y” del DataFrame tb_tsne se utilizan como ejes x e y, respectivamente.\n\ngeom_point():\n\nAgrega una capa de puntos al gráfico, creando un gráfico de dispersión.\n\n\n\n\n\nsns.scatterplot(x = \"X\", y = \"Y\", data = tb_tsne)\n\n\n\n\n\n\n\n\nEl gráfico de dispersión representa cada punto como una observación del DataFrame tb_tsne. Las coordenadas “X” y “Y” se utilizan como posiciones en los ejes x e y, proporcionando una visualización del espacio bidimensional obtenido a través del algoritmo t-SNE, utilizando la librería Seaborn. Explicación paso a paso del código:\n\nsns.scatterplot: Se utiliza la función sns.scatterplot para crear un gráfico de dispersión.\nx=\"X\", y=\"Y\" especifica que las coordenadas “X” y “Y” del DataFrame tb_tsne se utilizan como ejes x e y, respectivamente.\ndata=tb_tsne indica que los datos provienen de este DataFrame.\n\n\n\n\nTeniendo los resultados del análisis previo, podemos usar el dataframe tb_grupos en esta visualización para revisar los grupos. Es necesario cambiar la variable indicadora del grupo a texto, de lo contrario no vamos a tener una buena visualización.\n\nRPython\n\n\n\ntb_tsne %&gt;%\n  bind_cols(tb_grupos) %&gt;% \n  mutate(\n    # en esta línea podemos cambiar los grupos\n    grupo = as.character(grupos_jerar)\n    ) %&gt;% \n  ggplot +\n  aes(x = X, y  = Y, col = grupo) +\n  geom_point() \n\n\n\n\n\n\n\n\nSe obtiene un gráfico de dispersión donde cada punto representa una observación del DataFrame resultante de combinar tb_tsne y tb_grupos. Los puntos se colorean según la columna “grupo”. Este gráfico proporciona una visualización del espacio bidimensional obtenido a través del algoritmo t-SNE junto con la información de grupos proporcionada por tb_grupos. Explicación paso a paso del código:\n\nUtilizar bind_cols para combinar DataFrames:\n\ntb_tsne %&gt;% bind_cols(tb_grupos): Utiliza el operador %&gt;% para encadenar operaciones. bind_cols combina horizontalmente (cbind en R) los DataFrames tb_tsne y tb_grupos.\n\nAgregar una Columna de Grupos:\n\nmutate(...): Utiliza mutate para agregar o modificar columnas en el DataFrame resultante de la combinación.\ngrupo = as.character(grupos_jerar): Agrega una nueva columna llamada “grupo”, que se obtiene de la columna grupos_jerar. Se convierte a tipo de dato caracter (as.character) para asegurar que sea tratado como categoría.\n\nCrear un Gráfico de Dispersión con Colores por Grupo:\n\nggplot: Inicia la construcción de un gráfico utilizando la librería ggplot2.\naes(x = X, y = Y, col = grupo): Especifica que las coordenadas “X” y “Y” del DataFrame se utilizarán como ejes x e y, respectivamente, y se asignarán colores según la columna “grupo”.\n\nAgregar Puntos al Gráfico:\n\ngeom_point(): Agrega una capa de puntos al gráfico, creando un gráfico de dispersión.\n\n\n\n\n\ntb_tmp = pd.concat([tb_tsne, tb_grupos], axis = 1)\n\n# en esta línea podemos cambiar los grupos\ntb_tmp[\"grupo\"] = tb_tmp[\"grupos_jerar\"].astype(\"string\")\n\nsns.scatterplot(x = \"X\", y = \"Y\", hue = \"grupo\", data = tb_tmp)\n\n\n\n\n\n\n\n\nSe obtiene un gráfico de dispersión donde cada punto representa una observación del DataFrame tb_tmp, con las coordenadas “X” y “Y” como posiciones en los ejes x e y, respectivamente. Los puntos se colorean según la información de la columna “grupo”, proporcionando una visualización del espacio bidimensional obtenido a través del algoritmo t-SNE junto con la información de grupos proveniente de tb_grupos. Explicación paso a paso del código:\n\nConcatenar DataFrames Horizontalmente:\n\ntb_tmp = pd.concat([tb_tsne, tb_grupos], axis=1): Se utiliza la función concat de la librería pandas para concatenar horizontalmente (axis=1) los DataFrames tb_tsne y tb_grupos. El resultado es almacenado en el DataFrame tb_tmp.\n\nAgregar una Columna de Grupos al DataFrame Resultante:\n\ntb_tmp[\"grupo\"] = tb_tmp[\"grupos_jerar\"].astype(\"string\"): Se agrega una nueva columna llamada “grupo” al DataFrame tb_tmp. Esta columna se obtiene de la columna “grupos_jerar” y se convierte al tipo de dato string (.astype(\"string\")).\n\nCrear un Gráfico de Dispersión con Seaborn:\n\nsns.scatterplot(x=\"X\", y=\"Y\", hue=\"grupo\", data=tb_tmp): Se utiliza la función scatterplot de Seaborn para crear un gráfico de dispersión.\n\nx=\"X\", y=\"Y\" especifica que las coordenadas “X” y “Y” del DataFrame tb_tmp se utilizarán como ejes x e y, respectivamente.\nhue=\"grupo\" asigna colores según la columna “grupo”, lo que proporciona información de los grupos en el gráfico.\n\n\n\n\n\n\nA continuación, hacemos una gráfica de las pinturas usando las pinturas. De esta forma vamos a poder revisar cuáles pinturas resultan cercanas y a qué regiones pertenecen. Estas gráficas son computacionalmente exigentes; es recomendable guardarlas (no mostrarlas de inmediato), a menos que estemos trabajando en un computador muy potente.\n\nRPython\n\n\n\ntb_tsne %&gt;%\n  mutate(\n    archivos_pinturas = file.path(mi_setup$carpeta_pinturas, dir(mi_setup$carpeta_pinturas))\n  ) %&gt;% ggplot() +\n  geom_image(aes(x = X, y = Y, image = archivos_pinturas), size = 0.03) + \n  theme_void() -&gt; gr_tmp # Es mejor guardar la imagen que abrirla en la sesión\n\nggsave(mi_setup$gr_tsne_file, gr_tmp, width = 200, height = 200, units = \"mm\")\n\n\nSe elabora un gráfico de dispersión utilizando las imágenes de las pinturas en vez de puntos. Explicación paso a paso del código:\n\nModificar el DataFrame con mutate:\n\nSe utiliza mutate para agregar una nueva columna llamada “archivos_pinturas” al DataFrame tb_tsne. Esta columna contiene la ruta completa de los archivos de pinturas ubicados en la carpeta especificada en mi_setup$carpeta_pinturas.\nLa estructura de tuberías (%&gt;%) se utiliza para encadenar las operaciones, pasando el resultado de una a la siguiente.\n\nCrear un Gráfico con Imágenes:\n\nggplot() + geom_image(...): Se inicia la construcción de un gráfico utilizando la librería ggplot2. Se utiliza geom_image para agregar imágenes al gráfico, utilizando las coordenadas “X” y “Y” del DataFrame tb_tsne y la ruta de las imágenes en la columna “archivos_pinturas”.\n\nEstablecer un Tema Visual:\n\ntheme_void(): Aplica un tema visual que elimina elementos como ejes y fondos, dejando solo las imágenes.\n\nGuardar el Gráfico en una Variable:\n\n-&gt; gr_tmp: Se utiliza -&gt; para asignar el gráfico resultante a la variable gr_tmp.\n\nGuardar el Gráfico como Archivo:\n\nggsave(...): Se utiliza ggsave para guardar el gráfico en un archivo. La ruta y el nombre del archivo se especifican en mi_setup$gr_tsne_file. Se ajustan también las dimensiones del archivo y la unidad de medida.\n\n\n\n\n\n# En caso de que no hayamos corrido el código previo:\n# def getImage(path):\n#     return OffsetImage(plt.imread(path), zoom=.1, alpha = 1)\n# \n# nombres_archivos_pinturas = os.listdir(mi_setup[\"carpeta_pinturas\"])\n# \n# nombres_archivos_pinturas.sort()\n\ntb_tsne[\"archivos_pinturas\"] = list(map(\n  lambda dir: os.path.join(mi_setup[\"carpeta_pinturas\"],dir),\n  nombres_archivos_pinturas\n  ))\n\nfig, ax = plt.subplots(figsize=(25, 25))\nax.scatter(tb_tsne[\"X\"], tb_tsne[\"Y\"], color=\"white\")\n\nfor index, row in tb_tsne.iterrows():\n  ab = AnnotationBbox(getImage(row[\"archivos_pinturas\"]), (row[\"X\"], row[\"Y\"]), frameon=False)\n  ax.add_artist(ab)\n\nplt.savefig(mi_setup[\"gr_tsne_file\"], dpi = 100)\n\n\nSe elabora un gráfico de dispersión utilizando las imágenes de las pinturas en vez de puntos. Explicación paso a paso del código:\n\nDefinir la Función getImage:\n\nSe define una función llamada getImage que toma una ruta de archivo como argumento y devuelve un objeto OffsetImage que contiene la imagen con ciertas propiedades de zoom y transparencia.\n\nObtener Nombres de Archivos de Pinturas:\n\nSe obtienen los nombres de los archivos de pinturas en la carpeta especificada en mi_setup[\"carpeta_pinturas\"] y se almacenan en la lista nombres_archivos_pinturas. Esta lista se ordena alfabéticamente.\n\nAsignar Rutas Completas a las Imágenes en el DataFrame:\n\nSe utiliza map junto con lambda para crear una nueva columna en el DataFrame tb_tsne llamada “archivos_pinturas”, que contiene las rutas completas de las imágenes de pinturas.\n\nCrear un Gráfico de Dispersión Vacío:\n\nSe crea un gráfico de dispersión vacío utilizando plt.subplots, con un tamaño de figura de 25x25.\n\nAgregar Puntos Blancos al Gráfico:\n\nSe agrega una capa de puntos blancos al gráfico de dispersión en las coordenadas especificadas por las columnas “X” y “Y” del DataFrame tb_tsne.\n\nAgregar Imágenes al Gráfico:\n\nSe utiliza un bucle for para iterar sobre las filas de tb_tsne. Para cada fila, se crea un objeto AnnotationBbox que contiene la imagen de la pintura y se agrega al gráfico en las coordenadas correspondientes.\n\nGuardar el Gráfico como Archivo:\n\nSe guarda el gráfico como un archivo de imagen en la ruta especificada en mi_setup[\"gr_tsne_file\"], con una resolución de 100 dpi."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "En los siguientes recursos se encuentra la documentación contextual del proyecto. Es necesario estar en contexto para entender a profundidad lo que estamos haciendo. El contexto es el mismo en todas las etapas del proyecto.\nThe Joy of Parsing\nWhere Are All the Bob Ross Paintings? We Found Them.\nTwoInchBrush\n\n\nPuedes descargar la carpeta del proyecto donde puedes encontrar:\n\n01_data: carpeta para almacenamiento de datos, allí está la base de datos y la carpeta de pinturas.\n02_doc: carpeta para la documentación del proyecto. En esta carpeta puedes guardar los documentos del proyecto que sean relevantes.\n03_scripts: carpeta de programación. En esta carpeta puedes guardar los scripts que elabores.\n\nLos datos porvienen de los siguientes recursos, puedes visitarlos y revisar su funcionamiento.\nBase de datos\nScrapping"
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "Puedes descargar la carpeta del proyecto donde puedes encontrar:\n\n01_data: carpeta para almacenamiento de datos, allí está la base de datos y la carpeta de pinturas.\n02_doc: carpeta para la documentación del proyecto. En esta carpeta puedes guardar los documentos del proyecto que sean relevantes.\n03_scripts: carpeta de programación. En esta carpeta puedes guardar los scripts que elabores.\n\nLos datos porvienen de los siguientes recursos, puedes visitarlos y revisar su funcionamiento.\nBase de datos\nScrapping"
  },
  {
    "objectID": "index.html#objetivos-específicos",
    "href": "index.html#objetivos-específicos",
    "title": "Proyecto: The joy of programming",
    "section": "Objetivos específicos",
    "text": "Objetivos específicos\nLos objetivos específicos del proyecto son:\n\nAplicar algoritmos de agregación al dataset e identificar los grupos subyacentes de las pinturas.\nAplicar algoritmos de reducción de dimensiones a las pinturas con el fin de crear visualizaciones e interpretaciones.\nEvaluar las tendencias identificadas en los datos de las pinturas y seleccionar el mejor resultado. Caracterizarlo e interpretarlo.\nEvaluar los resultados de los algoritmos de reducción de dimensiones y crear las visualizaciones e interpretaciones correspondientes."
  },
  {
    "objectID": "unidad_01.html",
    "href": "unidad_01.html",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "En esta etapa aplicamos los algoritmos de agregación presentados a nuestro dataset de pinturas de Bob Ross. Es necesario que tengamos muy claro el contexto de nuestro trabajo, en caso de que haya dudas podemos volver a la presentación del proyecto.\nLos algoritmos que vamos a aplicar son \\(k\\)-means, agregación jerárquica y DBSCAN. Para ejecutarlos utilizamos código en R y en Python. Al final, revisamos en una tabla cuántos grupos tenemos y cuántas pinturas tiene cada grupo. Esto con el fin de interpretar estos resultados con precisión en las siguientes unidades.\n\n\nLa primera etapa del proyecto está orientada a cumplir el primer objetivo específico:\n\nAplicar algoritmos de agregación al dataset e identificar los grupos subyacentes de las pinturas."
  },
  {
    "objectID": "unidad_01.html#objetivo-actual",
    "href": "unidad_01.html#objetivo-actual",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "La primera etapa del proyecto está orientada a cumplir el primer objetivo específico:\n\nAplicar algoritmos de agregación al dataset e identificar los grupos subyacentes de las pinturas."
  },
  {
    "objectID": "unidad_01.html#algoritmo-jerárquico",
    "href": "unidad_01.html#algoritmo-jerárquico",
    "title": "Proyecto: The joy of programming",
    "section": "Algoritmo jerárquico",
    "text": "Algoritmo jerárquico\nEl siguiente es el código para el algoritmo jerárquico. En primer lugar calculamos la matriz de distancias del conjunto de datos. y luego aplicamos el algoritmo especificando la distancia entre grupos, en este caso es \"ward\".\n\nRPython\n\n\n\ntb_pinturas_caract %&gt;% \n  dist(method = \"euclidean\") %&gt;% \n  hclust(method = \"ward.D2\") -&gt; ls_jerarquico \n\ntb_pinturas_caract %&gt;% dist(): Calcula la matriz de distancias entre las filas del DataFrame tb_pinturas_caract. El operador %&gt;% (pipe) se utiliza para encadenar funciones en un flujo.\n-&gt; mt_dist_pint: Asigna la matriz de distancias resultante a un objeto llamado mt_dist_pint.\nmt_dist_pint %&gt;% hclust(method = \"ward.D2\"): Realiza un clúster jerárquico utilizando el método de enlace “ward.D2” en la matriz de distancias mt_dist_pint.\n-&gt; ls_jerarquico: Asigna el resultado del clúster jerárquico a un objeto llamado ls_jerarquico.\n\n\n\nmod_jerarquico = AgglomerativeClustering(\n    n_clusters = 6, \n    metric = 'euclidean', \n    linkage ='ward'\n    )\nmod_jerarquico = mod_jerarquico.fit(tb_pinturas_caract)\n\nAgglomerativeClustering(...): Se instancia un objeto del tipo AgglomerativeClustering, que implementa el algoritmo de agrupamiento jerárquico.\nn_clusters=6: Especifica el número de clústeres deseados, en este caso, se establece en 6.\nmetric='euclidean': Define la métrica de distancia utilizada para calcular la similaridad entre puntos. En este caso, se utiliza la distancia euclidiana.\nlinkage='ward': Especifica el método de enlace utilizado en el algoritmo, en este caso, el enlace “ward” que minimiza la varianza entre los clústeres.\nmod_jerarquico.fit(tb_pinturas_caract): Se ajusta el modelo de agrupamiento jerárquico a los datos en tb_pinturas_caract. El modelo aprenderá a asignar las observaciones en el número especificado de clústeres utilizando la distancia euclidiana y el método de enlace “ward”.\n\n\n\nA continuación, elaboramos el dendrograma. En este observamos que al hacer un corte podemos obtener 6 grupos bien definidos.\n\nRPython\n\n\n\nls_jerarquico %&gt;% \n  fviz_dend(6, show_labels = FALSE, horiz = TRUE) +\n  geom_hline(yintercept = 9.5)\n\n\n\n\n\n\n\n\nls_jerarquico %&gt;% as.dendrogram() %&gt;% ...: Convierte el resultado del clúster jerárquico (ls_jerarquico) en un dendrograma utilizando la función as.dendrogram() y luego encadena varias funciones de configuración y visualización utilizando el operador %&gt;% (pipe).\nset(\"labels_col\", k=6): Establece colores diferentes para las etiquetas según los grupos definidos por k=6 clústeres.\nset(\"branches_k_color\", k=6): Asigna colores diferentes a las ramas del dendrograma según los grupos definidos por k=6 clústeres.\nset(\"labels\", NULL): Elimina las etiquetas originales del dendrograma.\nplot(horiz=TRUE, axes=FALSE): Realiza la visualización del dendrograma con orientación horizontal y sin mostrar ejes.\nabline(v = 9, lty = 2): Agrega una línea vertical discontinua en la posición 9 en el dendrograma. Esta línea nos proporciona la altura a la que se realiza la partición en 6 grupos.\n\n\n\nplt.figure(figsize=(10,8))\ndendrogram = sch.dendrogram(\n  sch.linkage(tb_pinturas_caract, method  = \"ward\"),\n  color_threshold = 10, orientation = \"left\", no_labels = True)\nplt.title('Dendrogram')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\nplt.figure(figsize=(10,8)): Crea una nueva figura de tamaño 10x8 pulgadas utilizando matplotlib.pyplot.\ndendrogram = sch.dendrogram(...): Calcula y genera el dendrograma utilizando la función dendrogram de scipy.cluster.hierarchy. Se utiliza sch.linkage para realizar el agrupamiento jerárquico con el método de enlace “ward”.\nsch.linkage(tb_pinturas_caract, method=\"ward\"): Realiza el agrupamiento jerárquico con el método de enlace “ward” en los datos contenidos en tb_pinturas_caract.\ncolor_threshold=9: Define el umbral de color para resaltar los clústeres en el dendrograma. Las ramas por debajo de este umbral se muestran en el mismo color.\norientation=\"left\": Orienta el dendrograma hacia la izquierda.\nno_labels=True: Suprime las etiquetas de las hojas del dendrograma.\nplt.title('Dendrogram'): Agrega un título al dendrograma.\nplt.show(): Muestra la figura con el dendrograma.\nplt.close(): Cierra la figura, liberando recursos y previniendo posibles superposiciones en visualizaciones futuras.\n\n\n\nLuego extraemos los grupos y los guardamos en una nueva columna de nuestros datos. Adicionalmente, revisamos el tamaño de cada grupo.\n\nRPython\n\n\n\ntb_grupos %&lt;&gt;% \n  mutate(\n    grupos_jerar = ls_jerarquico %&gt;% \n  cutree(6) \n  )\n\ntb_grupos %&gt;% \n  count(grupos_jerar) -&gt; tb_cuenta_grupos\n\n\n\n\n\n\n\n\n\n\ngrupos_jerar\nn\n\n\n\n\n1\n141\n\n\n2\n41\n\n\n3\n86\n\n\n4\n48\n\n\n5\n39\n\n\n6\n48\n\n\n\n\n\ntb_grupos %&lt;&gt;%: Utiliza el operador %&lt;&gt;% para realizar la asignación en el propio objeto tb_grupos, es decir, modifica tb_grupos in-place.\nmutate(...): Crea o modifica columnas en el DataFrame.\ngrupos_jerar: Se crea una nueva columna llamada grupos_jerar en el DataFrame tb_grupos.\nls_jerarquico %&gt;% cutree(6): Utiliza la función cutree para asignar a cada observación en ls_jerarquico un número de grupo (clúster) según la división en 6 grupos realizada previamente.\ntb_grupos %&gt;% count(grupos_jerar): Utiliza la función count de dplyr para contar la frecuencia de cada valor único en la columna grupos_jerar de tb_grupos.\n\n\n\ntb_grupos['grupos_jerar'] = (mod_jerarquico\n  .fit_predict(tb_pinturas_caract)\n  .tolist())\n\ntb_cuenta_grupos = (tb_grupos.grupos_jerar\n  .value_counts()\n  .rename('count')\n  .reset_index())\n\n\n\n\n\n\n\ngrupos_jerar\ncount\n\n\n\n\n0\n1\n113\n\n\n1\n0\n95\n\n\n2\n5\n64\n\n\n3\n2\n48\n\n\n4\n4\n43\n\n\n5\n3\n40\n\n\n\n\n\ntb_grupos[\"grupos_jerar\"] = mod_jerarquico.fit_predict(tb_pinturas_caract):\nmod_jerarquico.fit_predict(tb_pinturas_caract): Utiliza el método fit_predict de AgglomerativeClustering para realizar el ajuste y predicción de grupos jerárquicos en los datos contenidos en tb_pinturas_caract. Los resultados se asignan a una nueva columna llamada grupos_jerar en el DataFrame tb_grupos.\ntb_grupos.grupos_jerar.value_counts():\ntb_grupos.grupos_jerar: Accede a la columna grupos_jerar en el DataFrame tb_grupos.\nvalue_counts(): Calcula la frecuencia de cada valor único en la columna grupos_jerar."
  },
  {
    "objectID": "unidad_01.html#algoritmo-k-means",
    "href": "unidad_01.html#algoritmo-k-means",
    "title": "Proyecto: The joy of programming",
    "section": "Algoritmo \\(k\\)-means",
    "text": "Algoritmo \\(k\\)-means\nEl algoritmo \\(k\\)-means se ejecuta con el siguiente código.\n\nRPython\n\n\n\ntb_pinturas_caract %&gt;% \n  kmeans(6) -&gt; ls_kmeans\n\ntb_pinturas_caract %&gt;% kmeans(6) -&gt; ls_kmeans:\n%&gt;%: Operador pipe que pasa el resultado de la operación anterior como el primer argumento de la siguiente operación.\nkmeans(6): Aplica el algoritmo k-Means para realizar el agrupamiento en tb_pinturas_caract con 6 clústeres.\n-&gt; ls_kmeans: Asigna el resultado del agrupamiento a un objeto llamado ls_kmeans.\n\n\n\nmod_kmeans = KMeans(\n        init=\"random\",\n        n_clusters=6,\n        n_init = 1\n    )\n\nmod_kmeans = mod_kmeans.fit(tb_pinturas_caract)\n\nKMeans: Crea una instancia del modelo de k-Means utilizando la clase KMeans de scikit-learn.\ninit=\"random\": Especifica que la inicialización de los centroides se realice de manera aleatoria.\nn_clusters=6: Define el número de clústeres deseados, en este caso, se establece en 6.\nfit(tb_pinturas_caract): Ajusta el modelo de k-Means a los datos contenidos en tb_pinturas_caract. El modelo aprenderá a asignar las observaciones en el número especificado de clústeres.\n\n\n\nLuego extraemos los grupos y los guardamos en una nueva columna de nuestros datos. Adicionalmente, revisamos el tamaño de cada grupo.\n\nRPython\n\n\n\ntb_grupos %&lt;&gt;% \n  mutate(\n    grupos_kmeans = ls_kmeans %&gt;% \n  pluck(\"cluster\") \n  )\n\ntb_grupos %&gt;% \n  count(grupos_kmeans) -&gt; tb_cuenta_grupos\n\n\n\n\n\n\n\n\n\n\ngrupos_kmeans\nn\n\n\n\n\n1\n123\n\n\n2\n48\n\n\n3\n51\n\n\n4\n35\n\n\n5\n75\n\n\n6\n71\n\n\n\n\n\n%&lt;&gt;%: Operador pipe que realiza la asignación in-place al propio objeto.\nmutate(...): Crea o modifica columnas en el DataFrame.\ngrupos_kmeans: Se crea una nueva columna llamada grupos_kmeans en el DataFrame tb_grupos.\nls_kmeans %&gt;% pluck(\"cluster\"): Extrae la información de los clústeres asignados por el modelo de k-Means (ls_kmeans) utilizando la función pluck y la clave “cluster”.\ntb_grupos %&gt;% count(grupos_kmeans): Utiliza la función count de dplyr para contar la frecuencia de cada valor único en la columna grupos_kmeans de tb_grupos.\n\n\n\ntb_grupos['grupos_kmeans'] = mod_kmeans.labels_\n\ntb_cuenta_grupos = (tb_grupos.grupos_kmeans\n  .value_counts()\n  .rename('count')\n  .reset_index())\n\n\n\n\n\n\n\ngrupos_kmeans\ncount\n\n\n\n\n0\n2\n94\n\n\n1\n0\n84\n\n\n2\n1\n68\n\n\n3\n5\n60\n\n\n4\n4\n49\n\n\n5\n3\n48\n\n\n\n\n\nmod_kmeans.labels_: Accede a los resultados de la asignación de clústeres realizada por el modelo de k-Means (mod_kmeans). Los resultados se encuentran en el atributo labels_.\ntb_grupos['grupos_kmeans']: Crea una nueva columna llamada grupos_kmeans en el DataFrame tb_grupos y asigna los resultados de la asignación de clústeres.\ntb_grupos.grupos_kmeans: Accede a la columna grupos_kmeans en el DataFrame tb_grupos.\nvalue_counts(): Calcula la frecuencia de cada valor único en la columna grupos_kmeans."
  },
  {
    "objectID": "unidad_01.html#algoritmo-dbscan",
    "href": "unidad_01.html#algoritmo-dbscan",
    "title": "Proyecto: The joy of programming",
    "section": "Algoritmo DBSCAN",
    "text": "Algoritmo DBSCAN\nEl algoritmo DBSCAN se ejecuta con el siguiente código. Es importante probar distintas configuraciones para los hiperparámetros \\(\\epsilon\\) y \\(n_{min}\\). En este caso se utilizamos \\(\\epsilon = 1.5\\) y \\(n_{min} = 3\\).\n\nRPython\n\n\n\ntb_pinturas_caract %&gt;% \n  dbscan(eps = 1.5, minPts = 3) -&gt; ls_dbscan\n\n%&gt;%: Operador pipe que pasa el resultado de la operación anterior como el primer argumento de la siguiente operación.\ndbscan(eps = 1.5, minPts = 3): Aplica el algoritmo DBSCAN para realizar el agrupamiento en tb_pinturas_caract. Se especifican los parámetros eps (radio máximo para formar un clúster) y minPts (número mínimo de puntos para formar un clúster).\n-&gt; ls_dbscan: Asigna el resultado del agrupamiento a un objeto llamado ls_dbscan.\n\n\n\nmod_dbscan = DBSCAN(eps=1.5, min_samples=3)\n\nmod_dbscan = mod_dbscan.fit(tb_pinturas_caract)\n\nDBSCAN: Crea una instancia del modelo de DBSCAN utilizando la clase DBSCAN de scikit-learn.\neps=1.5: Especifica el radio máximo para formar un clúster.\nmin_samples=3: Especifica el número mínimo de puntos para formar un clúster.\nfit(tb_pinturas_caract): Ajusta el modelo DBSCAN a los datos contenidos en tb_pinturas_caract. El modelo aprenderá a asignar las observaciones en clústeres según los parámetros especificados.\n\n\n\nLuego extraemos los grupos y los guardamos en una nueva columna de nuestros datos. Adicionalmente, revisamos el tamaño de cada grupo.\n\nRPython\n\n\n\ntb_grupos %&lt;&gt;% \n  mutate(\n    grupos_dbscan = ls_dbscan %&gt;% \n  pluck(\"cluster\") \n  )\n\ntb_grupos %&gt;% \n  count(grupos_dbscan) -&gt; tb_cuenta_grupos\n\n\n\n\n\n\n\n\n\n\ngrupos_dbscan\nn\n\n\n\n\n0\n123\n\n\n1\n233\n\n\n2\n6\n\n\n3\n7\n\n\n4\n21\n\n\n5\n8\n\n\n6\n5\n\n\n\n\n\n%&lt;&gt;%: Operador pipe que realiza la asignación in-place al propio objeto.\nmutate(...): Crea o modifica columnas en el DataFrame.\ngrupos_dbscan: Se crea una nueva columna llamada grupos_dbscan en el DataFrame tb_pinturas.\nls_dbscan %&gt;% pluck(\"cluster\"): Extrae la información de los clústeres asignados por el modelo DBSCAN (ls_dbscan) utilizando la función pluck y la clave “cluster”.\ntb_pinturas %&gt;% count(grupos_dbscan): Utiliza la función count de dplyr para contar la frecuencia de cada valor único en la columna grupos_dbscan de tb_pinturas.\n\n\n\ntb_grupos['grupos_dbscan'] = mod_dbscan.labels_\n\ntb_cuenta_grupos = (tb_grupos.grupos_dbscan\n  .value_counts()\n  .rename('count')\n  .reset_index())\n\n\n\n\n\n\n\ngrupos_dbscan\ncount\n\n\n\n\n0\n0\n233\n\n\n1\n-1\n123\n\n\n2\n3\n21\n\n\n3\n4\n8\n\n\n4\n2\n7\n\n\n5\n1\n6\n\n\n6\n5\n5\n\n\n\n\n\nmod_dbscan.labels_: Accede a los resultados de la asignación de clústeres realizada por el modelo de DBSCAN (mod_dbscan). Los resultados se encuentran en el atributo labels_.\ntb_grupos['grupos_dbscan']: Crea una nueva columna llamada grupos_dbscan en el DataFrame tb_grupos y asigna los resultados de la asignación de clústeres.\ntb_grupos.grupos_dbscan: Accede a la columna grupos_dbscan en el DataFrame tb_grupos.\nvalue_counts(): Calcula la frecuencia de cada valor único en la columna grupos_dbscan."
  },
  {
    "objectID": "unidad_01.html#guardar-los-resultados",
    "href": "unidad_01.html#guardar-los-resultados",
    "title": "Proyecto: The joy of programming",
    "section": "Guardar los resultados",
    "text": "Guardar los resultados\nNo se nos puede olvidar guardar nuestros resultados. Para esto podemos utilizar archivos tipo .csv.\n\nRPython\n\n\n\nwrite_csv(tb_grupos, mi_setup$archivo_resultados)\n\nEste código guarda los resultados de los procedimientos realizados. Explicación:\n\nwrite_csv: Es una función del paquete readr que permite guardar datos en texto plano.\ntb_grupos: Es el dataframe que contiene los resultados obtenidos.\nmi_setup$archivo_resultados: Es la entrada en la lista de configuración que contiene la ruta del archivo donde se guardan los resultados.\n\n\n\n\ntb_grupos.to_csv(mi_setup[\"archivo_resultados\"], index = False)\n\nEste código guarda los resultados de los procedimientos realizados. Explicación:\n\n.to_csv: es el método de la librería pandas para escribir datos en texto plano. Se utiliza la instrucción index = False para indicar se omita en el archivo resultado la columna del índice de fila.\ntb_grupos: Es el dataframe que contiene los resultados obtenidos.\nmi_setup[\"archivo_resultados\"]: Es la entrada en la lista de configuración que contiene la ruta del archivo donde se guardan los resultados."
  },
  {
    "objectID": "unidad_03.html",
    "href": "unidad_03.html",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "En esta etapa se aplican los métodos de evaluación de la calidad de los agrupamientos que han sido presentados en el curso para evaluar la calidad de los grupos resultantes de la primera etapa. Al igual que las etapas anteriores, la implementación de los métodos de evaluación se realiza en lenguajes para el manejo de datos R y Python.\nLas bases teóricas para la comprensión de los métodos presentados se encuentran en el material del curso. Los métodos que se aplican a continuación son: evaluación a partir de información externa, método del codo, validación cruzada y coeficiente silueta.\n\n\nLa tercera etapa del proyecto está orientada a cumplir el tercer objetivo específico:\n\nEvaluar las tendencias identificadas en los datos de las pinturas y seleccionar el mejor resultado. Caracterizarlo e interpretarlo."
  },
  {
    "objectID": "unidad_03.html#objetivo-actual",
    "href": "unidad_03.html#objetivo-actual",
    "title": "Proyecto: The joy of programming",
    "section": "",
    "text": "La tercera etapa del proyecto está orientada a cumplir el tercer objetivo específico:\n\nEvaluar las tendencias identificadas en los datos de las pinturas y seleccionar el mejor resultado. Caracterizarlo e interpretarlo."
  },
  {
    "objectID": "unidad_03.html#evaluación-a-partir-de-información-externa",
    "href": "unidad_03.html#evaluación-a-partir-de-información-externa",
    "title": "Proyecto: The joy of programming",
    "section": "Evaluación a partir de información externa",
    "text": "Evaluación a partir de información externa\nPara nuestro dataset de pinturas no tenemos el caso de información externa; pero podemos simular un escenario muy parecido. En primer lugar debemos programar la función ps, que nos permite obtener la fuerza de predicción de un sistema de grupos estimados sobre un sistema de grupos verdaderos. El código es el siguiente:\n\nRPython\n\n\n\nps &lt;- function(true_groups, estim_groups){\n  \n  sprintf(\"Id_%04d\", seq_along(true_groups)) -&gt; id_data\n  \n  setNames(\n    true_groups,\n    id_data\n  ) -&gt; vc_true\n  \n  setNames(\n    estim_groups,\n    id_data\n  ) -&gt; vc_estim\n  \nexpand_grid(id_1 = id_data, id_2 = id_data) %&gt;% \n  mutate(\n    true_1 = vc_true[id_1],\n    true_2 = vc_true[id_2],\n    paired_true = as.numeric(true_1 == true_2),\n    estim_1 = vc_estim[id_1],\n    estim_2 = vc_estim[id_2],\n    paired_estim = as.numeric(estim_1 == estim_2),\n    no_perm = as.numeric(paired_true &gt; paired_estim)\n  ) %&gt;% \n  group_by(true_1) %&gt;% \n  summarise(\n    paired_true = sum(paired_true),\n    no_perm = sum(no_perm)\n  ) %&gt;% \n  filter(paired_true != 1) %&gt;% \n  mutate(\n    ps = 1 - no_perm/(paired_true - sqrt(paired_true))\n  ) %&gt;% pull(ps) %&gt;% min -&gt; PS\n  \n  PS\n\n}\n\nEsta función calcula la fuerza de predicción (PS) basada en la comparación de grupos verdaderos y estimados utilizando permutaciones. La lógica exacta detrás del cálculo de la fuerza de predicción reside en la manipulación de datos y las transformaciones realizadas en el dataframe generado por expand_grid.\n\nGeneración de identificadores únicos (id_data):\n\nSe crea una lista de identificadores únicos formateados como “Id_XXXX”, donde XXXX representa un número secuencial de cuatro dígitos. La longitud de la lista está determinada por la cantidad de elementos en true_groups.\nsprintf(\"Id_%04d\", seq_along(true_groups)) -&gt; id_data: Crea un vector de identificación (id_data) utilizando la función sprintf para formatear números como cadenas de texto con un patrón específico (\"Id_%04d\") agregando ceros a la izquierda a la secuencia seq_along(true_groups) de números que van desde 1 hasta la longitud de true_groups.\n\nCreación de diccionarios para mapeo (vc_true y vc_estim):\n\nSe crean diccionarios que mapean los identificadores a los valores correspondientes en los grupos verdaderos (vc_true) y estimados (vc_estim).\nsetNames(true_groups, id_data) -&gt; vc_true: Asigna nombres al vector true_groups utilizando los identificadores en id_data. El resultado se guarda en vc_true.\nsetNames(estim_groups, id_data) -&gt; vc_estim: Similar al paso anterior, asigna nombres al vector estim_groups utilizando los identificadores en id_data. El resultado se guarda en vc_estim.\n\nGeneración de dataframe con todas las combinaciones posibles:\n\nSe generan todas las combinaciones posibles de pares de identificadores en id_data. Estos pares se convierten en columnas ‘id_1’ y ‘id_2’.\nexpand_grid(id_1 = id_data, id_2 = id_data) %&gt;% ...: Utiliza la función expand_grid para crear un dataframe con todas las combinaciones posibles de identificadores de id_data. Este dataframe contiene las columnas id_1 e id_2, que son todas las parejas posibles de observaciones. El operador %&gt;% (pipe) da continuidad a la aplicación de operaciones y transformaciones.\n\nMapeo de valores verdaderos y estimados:\n\nSe agregan columnas al dataFrame df que contienen los valores correspondientes a ‘id_1’ y ‘id_2’ para ambos grupos verdaderos y estimados.\nmutate(...): Agrega nuevas columnas al dataframe.\nSe crean las columnas true_1 y true_2, que muestran en qué grupo, de los grupos verdaderos, se encuentran las observaciones correspondientes a las columnas id_1 e id_2 y la columna paired_true que indica que la pareja correspondiente a id_1 e id_2 se encuentra en el mismo grupo verdadero.\nAnálogamente, se crean las columnas estim_1y estim_2, que muestran en qué grupo, de los grupos estimados, se encuentran las observaciones correspondientes a las columnas id_1 e id_2 y la columna paired_estimque indica que la pareja correspondiente a id_1 e id_2 se encuentra en el mismo grupo estimado.\n\nCálculo de no_perm:\n\nSe compara el número de parejas coincidentes entre paired_true y paired_estim, y se almacena en la columna no_perm. Luego, se agrupa el DataFrame por los valores verdaderos (‘true_1’) y se realiza un resumen.\nLa columna no_perm muestra los casos en los que una pareja en el mismo grupo verdadero (paired_true = 1) no se encuentra en el mismo grupo estimado (paired_estim = 0); es decir, es una pareja que no es permanente.\ngroup_by(true_1) %&gt;% summarise(...): Agrupa los datos por la columna true_1 y calcula la suma de paired_true y no_perm para cada grupo. Con esto se tiene la cantidad de parejas en los grupos reales (paired_true) y la cantidad de parejas que no son permanentes para cada cluster verdadero.\n\nCálculo de la fuerza de predicción (ps):\n\nSe calcula la fuerza de predicción (ps) utilizando la fórmula específica basada en los valores calculados en el paso anterior.\nfilter(paired_true != 1) %&gt;% mutate(...): Retira los grupos verdaderos de un solo elemento; son aquellos donde paired_true es igual a 1.\nps = 1 - no_perm/(paired_true - sqrt(paired_true)) calcula la fuerza de predicción para cada grupo verdadero.\n\nDevolver el valor mínimo de ps (PS):\n\nLa función devuelve el valor mínimo de la fuerza de predicción calculada.\npull(ps) %&gt;% min -&gt; PS: Extrae la columna ps y calcula el mínimo. El resultado se asigna a la variable PS.\nPS: Devuelve el valor calculado de PS.\n\n\n\n\n\ndef ps(true_groups, estim_groups):\n  id_data = [f\"Id_{i:04d}\" for i in range(len(true_groups))]\n  \n  vc_true = dict(zip(id_data, true_groups))\n  vc_estim = dict(zip(id_data, estim_groups))\n  \n  df = pd.DataFrame(list(product(id_data, repeat=2)), columns=['id_1', 'id_2'])\n  df['true_1'] = df['id_1'].map(vc_true)\n  df['true_2'] = df['id_2'].map(vc_true)\n  df['paired_true'] = np.where(df['true_1'] == df['true_2'], 1, 0)\n  df['estim_1'] = df['id_1'].map(vc_estim)\n  df['estim_2'] = df['id_2'].map(vc_estim)\n  df['paired_estim'] = np.where(df['estim_1'] == df['estim_2'], 1, 0)\n  df['no_perm'] = np.where(df['paired_true'] &gt; df['paired_estim'], 1, 0)\n  \n  result_df = df.groupby('true_1').agg(paired_true=('paired_true', 'sum'),\n  no_perm=('no_perm', 'sum')).reset_index()\n  result_df = result_df[result_df['paired_true'] != 1]\n  result_df['ps'] = 1 - result_df['no_perm'] / (result_df['paired_true'] - np.sqrt(result_df['paired_true']))\n  \n  PS = result_df['ps'].min()\n  \n  return PS\n\nLa función ps realiza los siguientes pasos:\n\nGeneración de identificadores únicos (id_data):\n\nSe crea una lista de identificadores únicos formateados como “Id_XXXX”, donde XXXX representa un número secuencial de cuatro dígitos. La longitud de la lista está determinada por la cantidad de elementos en true_groups.\nrange(len(true_groups)): Genera una secuencia de números desde 0 hasta la longitud de la lista true_groups menos 1. Esto proporciona índices únicos para cada elemento en la lista true_groups.\nf\"Id_{i:04d}\": Utiliza una cadena de formato f-string para crear cadenas de texto con el formato “Id_XXXX”, donde XXXX representa el índice con un relleno de ceros a la izquierda para asegurar que siempre tenga cuatro dígitos.\n[...]: Comprende la comprensión de listas, que es una forma concisa de crear listas. En este caso, se genera una lista que contiene las cadenas de texto resultantes.\n\nCreación de diccionarios para mapeo (vc_true y vc_estim):\n\nSe crean diccionarios que mapean los identificadores a los valores correspondientes en los grupos verdaderos (vc_true) y estimados (vc_estim).\nzip(id_data, true_groups): Combina los elementos de las listas id_data y true_groups en pares de tuplas. Cada tupla contiene un elemento de id_data emparejado con el correspondiente elemento de true_groups.\ndict(...): Convierte las tuplas generadas por zip en un diccionario, donde el primer elemento de cada tupla (de id_data) se convierte en la clave y el segundo elemento (de true_groups) se convierte en el valor.\nvc_true = dict(zip(id_data, true_groups)) crea un diccionario (vc_true) donde cada identificador único en id_data se asigna al valor correspondiente en true_groups.\nvc_estim = dict(zip(id_data, estim_groups)) realiza la misma operación pero para las listas id_data y estim_groups, creando un diccionario (vc_estim) con los valores estimados asociados a los identificadores únicos.\n\nGeneración de dataframe con todas las combinaciones posibles (df):\n\nSe utiliza la función product del módulo itertools para generar todas las combinaciones posibles de pares de identificadores en id_data. Estos pares se convierten en columnas ‘id_1’ y ‘id_2’ en el dataframe df.\nproduct(id_data, repeat=2): Utiliza la función product del módulo itertools para generar todas las combinaciones posibles de pares de elementos en la lista id_data. El parámetro repeat=2 indica que se deben generar combinaciones de longitud 2 (pares).\nlist(...): Convierte el objeto iterable resultante de product en una lista.\npd.DataFrame(...): Crea un dataframe de Pandas a partir de la lista generada. El dataframe tendrá dos columnas llamadas ‘id_1’ y ‘id_2’.\ndf = pd.DataFrame(list(product(id_data, repeat=2)), columns=['id_1', 'id_2']) crea un dataframe donde cada fila contiene todas las combinaciones posibles de pares de identificadores únicos en id_data. Cada par de identificadores se coloca en las columnas ‘id_1’ e ‘id_2’, respectivamente.\n\nMapeo de valores verdaderos y estimados en el dataframe (df):\n\nSe agregan columnas al dataframe df que contienen los valores correspondientes a ‘id_1’ y ‘id_2’ para ambos grupos verdaderos y estimados.\nSe crean las columnas true_1 y true_2, que muestran en qué grupo, de los grupos verdaderos, se encuentran las observaciones correspondientes a las columnas id_1 e id_2 y la columna paired_true que indica que la pareja correspondiente a id_1 e id_2 se encuentra en el mismo grupo verdadero.\nAnálogamente, se crean las columnas estim_1y estim_2, que muestran en qué grupo, de los grupos estimados, se encuentran las observaciones correspondientes a las columnas id_1 e id_2 y la columna paired_estimque indica que la pareja correspondiente a id_1 e id_2 se encuentra en el mismo grupo estimado.\n\nCálculo de no_perm y result_df:\n\nSe compara el número de parejas coincidentes entre paired_true y paired_estim, y se almacena en la columna no_perm. Luego, se agrupa el DataFrame por los valores verdaderos (‘true_1’) y se realiza un resumen.\nLa columna no_perm muestra los casos en los que una pareja en el mismo grupo verdadero (paired_true = 1) no se encuentra en el mismo grupo estimado (paired_estim = 0); es decir, es una pareja que no es permanente.\nresult_df = df.groupby('true_1'): Agrupa los datos por la columna true_1.\n.agg(paired_true=('paired_true', 'sum'), no_perm=('no_perm', 'sum')) calcula la suma de paired_true y no_perm para cada grupo. Con esto se tiene la cantidad de parejas en los grupos reales (paired_true) y la cantidad de parejas que no son permanentes para cada cluster verdadero.\n\nCálculo de la fuerza de predicción (ps):\n\nSe calcula la fuerza de predicción (ps) utilizando la fórmula específica basada en los valores calculados en el paso anterior.\nresult_df[result_df['paired_true'] != 1]: Retira los grupos verdaderos de un solo elemento; son aquellos donde paired_true es igual a 1.\nresult_df['ps'] = 1 - result_df['no_perm'] / (result_df['paired_true'] - np.sqrt(result_df['paired_true'])) calcula la fuerza de predicción para cada grupo verdadero.\n\nDevolver el valor mínimo de ps (PS):\n\nLa función devuelve el valor mínimo de la fuerza de predicción calculada.\nPS = result_df['ps'].min(): Extrae la columna ps y calcula el mínimo. El resultado se asigna a la variable PS.\nreturn PS: Devuelve el valor calculado de PS.\n\n\n\n\n\nYa con nuestra función ps, podemos simular el caso de la información externa. Supongamos que tenemos una columna que distribuye las pinturas en 16 grupos dependiendo de si presentan o no montañas, playas, estructuras o invierno. Para fines prácticos, estas 16 categorías se toman como las categorías verdaderas y se mide la aproximación de los grupos conformados previamente con estas categorías. Para eso utilizamos la función ps de la siguiente manera:\n\nRPython\n\n\n\ntb_pinturas_caract %&gt;% \n  mutate(\n    true_clust = sprintf(\"%d%d%d%d\", MOUNTAIN, BEACH, STRUCTURE, WINTER)\n  ) %&gt;% pull(true_clust) -&gt; true_cluster\n\nps(true_cluster, tb_grupos$grupos_jerar)\n\nps(true_cluster, tb_grupos$grupos_kmeans)\n\nps(true_cluster, tb_grupos$grupos_dbscan)\n\nEl código crea una nueva variable llamada true_clust en el dataframe tb_pinturas_caract, que representa un código de cluster formado por la concatenación de las variables MOUNTAIN, BEACH, STRUCTURE, y WINTER. Luego, se extrae esa variable y se asigna a la variable true_cluster. Esta variable se toma como una agrupación verdadera para probar la evaluación a partir de información externa.\n\ntb_pinturas_caract %&gt;%: Indica que se aplicarán operaciones al objeto tb_pinturas_caract utilizando un pipe (%&gt;%), lo que significa que el resultado de la operación previa se pasa como argumento a la siguiente operación.\nmutate(true_clust = sprintf(\"%d%d%d%d\", MOUNTAIN, BEACH, STRUCTURE, WINTER)): Agrega una nueva columna llamada true_clust al dataframe tb_pinturas_caract. El valor de esta columna se genera utilizando la función sprintf para formatear una cadena de texto. Los valores MOUNTAIN, BEACH, STRUCTURE, y WINTER se concatenan.\n%&gt;% pull(true_clust): Extrae la columna recién creada (true_clust) del dataframe resultante y asigna el resultado a la variable true_cluster. Luego se procede a evaluar los grupos conformados anteriormente contrastándolos con esta columna de grupos verdaderos.\n\n\n\n\ntrue_cluster = tb_pinturas_caract.apply(\n    lambda row: \"{:d}{:d}{:d}{:d}\".format(\n        row['MOUNTAIN'], row['BEACH'], row['STRUCTURE'], row['WINTER']\n    ), axis=1\n).tolist()\n\n\nps(true_cluster, tb_grupos['grupos_jerar'])\nps(true_cluster, tb_grupos['grupos_kmeans'])\nps(true_cluster, tb_grupos['grupos_dbscan'])\n\nEl código crea la lista true_cluster, donde cada elemento de la lista es una cadena de texto que representa un código de cluster formado por la concatenación de los valores de las columnas ‘MOUNTAIN’, ‘BEACH’, ‘STRUCTURE’, y ‘WINTER’ para cada fila en el DataFrame tb_pinturas_caract. Esta lista se toma como una agrupación verdadera para probar la evaluación a partir de información externa.\n\ntb_pinturas_caract.apply(...): Se utiliza la función apply de Pandas para aplicar una función a lo largo de las filas del DataFrame tb_pinturas_caract.\nlambda row: \"{:d}{:d}{:d}{:d}\".format(...): Se define una función lambda que toma una fila (row) del DataFrame y devuelve una cadena de texto formateada. En este caso, la cadena de texto se forma concatenando los valores de las columnas ‘MOUNTAIN’, ‘BEACH’, ‘STRUCTURE’, y ‘WINTER’.\naxis=1: Indica que la función lambda se aplicará a lo largo de las filas.\n.tolist(): Convierte el resultado a una lista. Luego se procede a evaluar los grupos conformados anteriormente contrastándolos con esta columna de grupos verdaderos.\n\n\n\n\n¿Cuánto da cada resultado? ¿Cómo se interpreta? ¿Cuál es el método que mejor se adapta a nuestra variable categórica simulada?"
  },
  {
    "objectID": "unidad_03.html#método-del-codo",
    "href": "unidad_03.html#método-del-codo",
    "title": "Proyecto: The joy of programming",
    "section": "Método del codo",
    "text": "Método del codo\nEl método del codo tiene como objetivo encontrar un número de grupos \\(k\\) óptimo. Por esta razón, suele estar enfocado en la estimación mediante el algoritmo \\(k\\)-means. Veamos la construcción de los tráficos correspondientes.\n\nRPython\n\n\n\nfviz_nbclust(x = tb_pinturas_caract, FUNcluster = kmeans, method = \"wss\", k.max = 10)\n\n\n\n\n\n\n\n\nEl código utiliza la función fviz_nbclust para visualizar diferentes métodos y determinar el número óptimo de clusters en el conjunto de datos tb_pinturas_caract. La métrica evaluada es la suma de los cuadrados dentro de los clusters (WSS), y la visualización puede ayudar a identificar el número óptimo de clusters al observar el codo en la gráfica resultante.\n\nfviz_nbclust(...): Utiliza la función fviz_nbclust del paquete factoextra para determinar el número óptimo de clusters en un conjunto de datos.\nx = tb_pinturas_caract: Se especifica que el conjunto de datos tb_pinturas_caract es el objeto en el que se realizará el análisis de clusters.\nFUNcluster = kmeans: Indica que se utilizará el algoritmo de k-means para realizar el análisis de clusters, utilizando la función kmeans.\nmethod = \"wss\": Selecciona el método para determinar el número óptimo de clusters. En este caso, se utiliza “wss” (Within-Cluster Sum of Squares), que evalúa la varianza dentro de los clusters.\nk.max = 10: Especifica el número máximo de clusters a considerar. En este caso, se evaluará hasta un máximo de 10 clusters.\n\n\n\n\nk_values = range(1, 11)\n\nkmeans_model = KMeans(n_init = 1)\n\nvisualizer = KElbowVisualizer(kmeans_model, k=k_values, metric='distortion', timings=False)\n\nvisualizer = visualizer.fit(tb_pinturas_caract)\n\nvisualizer.show()\n\n\n\n\n\n\n\nplt.close()\n\nEl código utiliza el método del codo para determinar el número óptimo de clusters en el conjunto de datos tb_pinturas_caract utilizando el algoritmo de k-medias. La visualización de la distorsión en función del número de clusters ayuda a identificar el punto en el que la distorsión deja de disminuir rápidamente, lo que puede indicar el número óptimo de clusters para el conjunto de datos.\n\nDefinición de k_values: Se crea una secuencia de valores desde 1 hasta 10 para representar diferentes números de clusters.\nInicialización de kmeans_model: Se crea una instancia del modelo de KMeans (k-medias) con n_init=1, indicando que el algoritmo se ejecutará solo una vez con una inicialización diferente.\nCreación del visualizador visualizer: Se utiliza KElbowVisualizer del paquete yellowbrick para visualizar la métrica de “distorsión” en función del número de clusters. La distorsión mide cuánto se alejan las muestras dentro de un cluster promedio. Se utiliza timings=False para evitar la visualización de tiempos de ejecución.\nAjuste del modelo al conjunto de datos: Se ajusta el modelo de k-medias al conjunto de datos tb_pinturas_caract utilizando el método fit() del visualizador.\nVisualización y cierre del gráfico: Se muestra la visualización generada por el visualizador y se cierra la figura gráfica después de ser visualizada.\n\n\n\n\nEn ambos casos, podemos ver el gráfico del método del codo y establecer un número óptimo de grupos."
  },
  {
    "objectID": "unidad_03.html#validación-cruzada",
    "href": "unidad_03.html#validación-cruzada",
    "title": "Proyecto: The joy of programming",
    "section": "Validación cruzada",
    "text": "Validación cruzada\nLos métodos de validación cruzada se basan en la distribución de dos subconjuntos de datos, para entrenamiento del modelo y para prueba. Siguiendo lo aprendido, en primera instancia debemos escribir una función que realice los siguientes pasos dado un conjunto de datos.\n\nDivida el conjunto de datos en segmentos de entrenamiento y prueba.\nEstime la estructura de grupos usando el procedimiento a evaluar. En este caso lo realizamos usando \\(k\\)-means, pero podemos cambiar el código para que funcione con otros métodos.\nA partir del modelo estimado por medio de los datos de entrenamiento, pronosticar los grupos correspondientes a los individuos del conjunto de datos de prueba. Estas categorías se toman como categorías verdaderas,\nAjustar el modelo con los mismos hiperparámetros al conjunto de datos de prueba. De esta forma se obtiene una nueva estructura de grupos. Estas categorías se toman como categorías estimadas.\nComparar las categorías estimadas con las verdaderas utilizando la función ps definida enteriormente.\n\n\nRPython\n\n\n\nps_cv_k &lt;- function(k, tb_data, cv_ratio = 3/4){\n  \n  tb_data %&gt;% \n    initial_split(prop = cv_ratio) -&gt; split_data\n  \n  split_data %&gt;% \n    training %&gt;% \n    kmeans(k) -&gt; ls_kmeans\n  \n  tb_centers &lt;- ls_kmeans$centers\n  n_centers &lt;- nrow(tb_centers)\n  \n  split_data %&gt;%\n    testing %&gt;% \n    rbind(tb_centers) %&gt;% \n    dist %&gt;% \n    as.matrix %&gt;% \n    extract(seq(n_centers), -seq(n_centers)) %&gt;% t %&gt;% \n    \"-\"() %&gt;% \n    max.col() -&gt; vc_predicted\n  \n  split_data %&gt;% \n    testing %&gt;% \n    kmeans(k) %&gt;%\n    pluck(\"cluster\") -&gt; vc_test_clusters\n  \n  \n  ps(vc_predicted, vc_test_clusters)\n  \n}\n\nPaso 1: tb_data %&gt;% initial_split(prop = cv_ratio) -&gt; split_data. Dividir los datos en conjuntos de entrenamiento y prueba.\nPaso 2: split_data %&gt;% training %&gt;% kmeans(k) -&gt; ls_kmeans Aplicar k-medias (kmeans) al conjunto de entrenamiento.\nPaso 3: tb_centers &lt;- ls_kmeans$centers; n_centers &lt;- nrow(tb_centers) Obtener los centros de los clusters del modelo k-medias\nPaso 4: Determinar el grupo de cada observación en el conjunto de prueba, este se toma como etiqueta verdadera.\n\nsplit_data %&gt;% testing %&gt;% rbind(tb_centers) %&gt;% dist %&gt;%  ... Calcular las distancias de los puntos de prueba a los centros de los clusters.\nextract(seq(n_centers), -seq(n_centers)) %&gt;% t Eliminar las distancias a los propios centros\n\"-\"() %&gt;% max.col() -&gt; vc_predicted Encontrar el índice del centro más cercano para cada punto de prueba.\n\nPaso 5: Aplicar el algoritmo y encontrar nuevos grupos para los datos de prueba, estos se toman como etiquetas a evaluar.\n\nsplit_data %&gt;% testing %&gt;% kmeans(k) Aplicar k-medias al conjunto de prueba\npluck(\"cluster\") -&gt; vc_test_clusters Obtener la asignación de clusters para los puntos de prueba\n\nPaso 6: ps(vc_predicted, vc_test_clusters) Calcular y devolver la medida de validez del clustering (ps).\n\n\n\ndef ps_cv_k(k, tb_data, cv_ratio=3/4):\n\n  train_data, test_data = train_test_split(tb_data, test_size=1-cv_ratio)\n  \n  kmeans_model = KMeans(n_clusters=k, n_init=1)\n  kmeans_model = kmeans_model.fit(train_data)\n  \n  vc_predicted = kmeans_model.predict(test_data).tolist()\n  \n  kmeans_test = KMeans(n_clusters=k, n_init=1)\n  vc_test_clusters = kmeans_test.fit_predict(test_data).tolist()\n  \n  return ps(vc_predicted, vc_test_clusters)\n\nPaso 1: train_data, test_data = train_test_split(tb_data, test_size=1-cv_ratio) Dividir los datos en conjuntos de entrenamiento y prueba\nPaso 2: kmeans_model = KMeans(n_clusters=k, n_init=1); kmeans_model = kmeans_model.fit(train_data) Aplicar k-medias (KMeans) al conjunto de entrenamiento\nPaso 3: vc_predicted = kmeans_model.predict(test_data).tolist() Obtener las asignaciones de cluster para los puntos de prueba\nPaso 4: kmeans_test = KMeans(n_clusters=k, n_init=1); vc_test_clusters = kmeans_test.fit_predict(test_data).tolist() Aplicar k-medias al conjunto de prueba\nPaso 5: return ps(vc_predicted, vc_test_clusters) Calcular y devolver la medida de validez del clustering (ps)\n\n\n\nLuego de tener nuestra función lista, resulta sencillo realizar un estudio de simulación que repita la estimación de la fuerza de predicción un número arbitrario de veces. Con esto podemos establecer la distribución de nuestro estadístico de validación y además obtener su promedio.\n\nRPython\n\n\n\nK_seq &lt;- 2:10\n\nmap(\n  1:50,\n  function(iter){\n    tibble(\n      iter = iter,\n      K = K_seq,\n      PS = map_dbl(\n        K,\n        ps_cv_k,\n        tb_data = tb_pinturas_caract,\n        cv_ratio = 0.6\n      )\n    )\n    \n  }\n) %&gt;% bind_rows() -&gt; tb_sim\n\ntb_sim %&gt;% group_by(K) %&gt;% summarise(PS = mean(PS)) -&gt; tb_sim_mean\n\nggplot() +\n  geom_jitter(aes(K, PS), tb_sim, size = 0.7) + \n  geom_point(aes(K, PS), tb_sim_mean, colour = \"#55aacc\", size = 3) +\n  geom_line(aes(K, PS), tb_sim_mean, colour = \"#55aacc\", size = 1) + \n  scale_x_continuous(breaks = K_seq)\n\n\n\n\n\n\n\n\nEl código realiza 50 iteraciones, en cada iteración evalúa la medida de validez del clustering (PS) para diferentes valores de K utilizando la función ps_cv_k. Luego, se calcula la media de la medida de validez para cada valor de K a lo largo de todas las iteraciones. Finalmente, se visualizan los resultados mediante un gráfico utilizando ggplot2, donde los puntos representan las medidas de validez para cada iteración y el punto y la línea en azul representan la media para cada valor de K. La dispersión de puntos proporciona una idea de la variabilidad de la medida de validez para cada valor de K.\nPaso 1: K_seq &lt;- 2:10 Definir una secuencia de valores para K (número de clusters)\nPaso 2: map(1:50,function(iter){...}) %&gt;% Realizar un mapeo (map) sobre las iteraciones de 1 a 50\nPaso 3: tibble(...) Para cada iteración, calcular la medida de validez del clustering (PS)\nPaso 4: bind_rows() -&gt; tb_sim Unir los resultados de todas las iteraciones en un solo marco de datos\nPaso 5: tb_sim %&gt;% group_by(K) %&gt;% summarise(PS = mean(PS)) -&gt; tb_sim_mean Calcular la media de la medida de validez del clustering para cada valor de K\nPaso 6: ggplot() + ... Crear un gráfico utilizando ggplot2 para visualizar los resultados\n\n\n\nK_seq = range(2, 11)\n\ntb_sim_list = []\n\nfor iter in range(1, 51):\n  ps_values = list(map(\n    lambda k: ps_cv_k(k, tb_data=tb_pinturas_caract, cv_ratio=0.6),\n    K_seq\n    ))\n  iter_data = pd.DataFrame({\n    'iter': [iter] * len(K_seq),\n    'K': K_seq,\n    'PS': ps_values\n    })\n    \n  tb_sim_list.append(iter_data)\n\n\ntb_sim = pd.concat(tb_sim_list, ignore_index=True)\n\ntb_sim_mean = tb_sim.groupby('K')['PS'].mean().reset_index()\n\n\n\nsns.stripplot(\n  x='K', y='PS', \n  data=tb_sim, size=1.3, legend=False, \n  color = \"black\"\n  )\nsns.pointplot(x='K', y='PS', data=tb_sim_mean, color='#55aacc')\nplt.xlabel('Número de clústeres (K)')\nplt.ylabel('Índice de Pureza (PS)')\nplt.xticks(K_seq)\n\n\n\n\n\n\n\n\ncódigo realiza 50 iteraciones, en cada iteración evalúa la medida de validez del clustering (PS) para diferentes valores de K utilizando la función ps_cv_k. Luego, se concatenan los resultados de todas las iteraciones en un solo dataframe (tb_sim). Se calcula la media de la medida de validez para cada valor de K y se visualizan los resultados mediante un gráfico utilizando la biblioteca seaborn. Los puntos en el gráfico representan las medidas de validez para cada iteración, y los puntos y la línea azul representan la media para cada valor de K. La dispersión de puntos proporciona una idea de la variabilidad de la medida de validez para cada valor de K.\nPaso 1: K_seq = range(2, 11) Definir una secuencia de valores para K (número de clústeres)\nPaso 2: tb_sim_list = [] Inicializar una lista para almacenar los resultados de cada iteración\nPaso 3: for iter in range(1, 51): Realizar un bucle sobre las iteraciones de 1 a 50, al interior de este bucle se realizan los pasos 4, 5 y 6.\nPaso 4: ps_values = list(map(lambda k: ps_cv_k(k, tb_data=tb_pinturas_caract, cv_ratio=0.6), K_seq)) Calcular la medida de validez del clustering (PS) para cada valor de K\nPaso 5: iter_data = pd.DataFrame({...}) Crear un DataFrame con los resultados de la iteración actual\nPaso 6: tb_sim_list.append(iter_data) Agregar los resultados a la lista\nPaso 7: tb_sim = pd.concat(tb_sim_list, ignore_index=True) Concatenar los resultados de todas las iteraciones en un solo DataFrame\nPaso 8: tb_sim_mean = tb_sim.groupby('K')['PS'].mean().reset_index() Calcular la media de la medida de validez del clustering para cada valor de K\nPaso 9: sns.... Crear un gráfico utilizando seaborn para visualizar los resultados\n\n\n\nLuego de simular los resultados y obtener la medida promedio de la fuerza de predicción es necesario interpretar. Si la fuerza de predicción es alta (cercana a 1), significa que la estructura de grupos estimados es muy parecida a los verdaderos. Lo que indica que el método recupera aproximadamente los mismos grupos. Si la fuerza de predicción es baja significa que el algoritmo está encontrando distintos grupos al examinar los datos de prueba y al pronosticarlos mediante la estructura estimada usando los datos de entrenamiento. Esto puede suceder por dos motivos. 1. Puede ser que los datos no presenten una esstructura de agrupamiento, o 2. Puede ser que los hiperparámetros del algoritmo, como su número de grupos, o su distancia entre grupos (linkage) o su epsilon, sean incorrectos."
  },
  {
    "objectID": "unidad_03.html#coeficiente-silueta",
    "href": "unidad_03.html#coeficiente-silueta",
    "title": "Proyecto: The joy of programming",
    "section": "Coeficiente Silueta",
    "text": "Coeficiente Silueta\nEl coeficiente silueta es una métrica de ajuste de un algoritmo de agrupación. También puede ser usado para la selección del número de grupos \\(k\\) en una estimación de \\(k\\)-means. El código es el siguiente.\n\nRPython\n\n\n\nfviz_nbclust(\n  x = tb_pinturas_caract, \n  FUNcluster = kmeans, \n  method = \"silhouette\",\n  k.max = 15\n  ) \n\n\n\n\n\n\n\n\nEl código utiliza la función fviz_nbclust para visualizar determinar el número óptimo de clusters en el conjunto de datos tb_pinturas_caract. La métrica evaluada es el coeficiente silueta, y la visualización permite identificar el número óptimo de clusters al observar el cambiuo en el coeficiente silueta en función del número de clusters.\n\nfviz_nbclust(...): Utiliza la función fviz_nbclust del paquete factoextra en R para determinar el número óptimo de clusters en un conjunto de datos.\nx = tb_pinturas_caract: Se especifica que el conjunto de datos tb_pinturas_caract es el objeto en el que se realizará el análisis de clusters.\nFUNcluster = kmeans: Indica que se utilizará el algoritmo de k-means para realizar el análisis de clusters, utilizando la función kmeans.\nmethod = \"silhouette\": Selecciona el método para determinar el número óptimo de clusters. En este caso, se utiliza “silhouette”, que es una medida de cuán bien definidos están los clusters.\nk.max = 15: Especifica el número máximo de clusters a considerar. En este caso, se evaluará hasta un máximo de 15 clusters.\n\n\n\n\nk_values = range(2, 16)\n\nkmeans_model = KMeans(n_init = 1)\n\nvisualizer = KElbowVisualizer(\n  kmeans_model, k=k_values, metric='silhouette', timings=False\n  )\n\nvisualizer = visualizer.fit(tb_pinturas_caract)\n\nvisualizer.show()\n\n\n\n\n\n\n\nplt.close()\n\nEl código utiliza el método del codo para determinar el número óptimo de clusters en el conjunto de datos tb_pinturas_caract utilizando el algoritmo de k-medias. La visualización de la métrica de silhouette en función del número de clusters ayuda a identificar el punto en el que los clusters son más cohesivos y separados, lo que permite identificar el número óptimo de clusters para el conjunto de datos.\n\nDefinición de k_values: Se crea una secuencia de valores desde 2 hasta 15 para representar diferentes números de clusters.\nInicialización de kmeans_model: Se crea una instancia del modelo de KMeans (k-medias) con n_init=1, indicando que el algoritmo se ejecutará solo una vez con una inicialización diferente.\nCreación del visualizador visualizer: Se utiliza KElbowVisualizer del paquete yellowbrick para visualizar la métrica de “silhouette” en función del número de clusters. La métrica de silhouette mide cuán similar es un objeto a su propio cluster (cohesión) en comparación con otros clusters (separación). Se utiliza timings=False para evitar la visualización de tiempos de ejecución.\nAjuste del modelo al conjunto de datos: Se ajusta el modelo de k-medias al conjunto de datos tb_pinturas_caract utilizando el método fit() del visualizador.\nVisualización y cierre del gráfico: Se muestra la visualización generada por el visualizador y se cierra la figura gráfica después de ser visualizada.\n\n\n\n\nEn el gráfico es posible ver el número óptimo de grupos. Pero también es importante aprender a calcular el coeficientes silueta para cualquier resultado. Por esta razón, incluimos los siguientes fragmentos de código.\n\nRPython\n\n\n\ntb_grupos %&gt;% \n  pull(grupos_jerar) %&gt;% \n  silhouette(\n    dist(tb_pinturas_caract)\n  ) %&gt;% summary %&gt;% pluck(\"avg.width\") -&gt; jerar_sil\n\n\ntb_grupos %&gt;% \n  pull(grupos_kmeans) %&gt;% \n  silhouette(\n    dist(tb_pinturas_caract)\n  ) %&gt;% summary %&gt;% pluck(\"avg.width\") -&gt; kmeans_sil\n\n\nSe utiliza la función pull para extraer la columna de grupos jerárquicos (grupos_jerar) del dataframe tb_grupos.\nSe aplica la función silhouette para calcular el coeficiente de silueta. Se utiliza la función dist para calcular la matriz de distancias entre las observaciones en tb_pinturas_caract.\nSe utiliza la función summary para resumir los resultados del coeficiente de silueta.\nSe utiliza la función pluck para extraer el ancho promedio del coeficiente de silueta y se almacena en la variable jerar_sil.\n\nLuego, se repite el mismo proceso para los grupos de k-means (grupos_kmeans) y se almacena el ancho promedio del coeficiente de silueta en la variable kmeans_sil.\nEstos valores (jerar_sil y kmeans_sil) proporcionarán información sobre la calidad de la agrupación según el coeficiente de silueta para los grupos jerárquicos y de k-means, respectivamente. Un coeficiente de silueta más cercano a 1 indica una mejor separación y cohesión de los grupos.\n\n\n\njerar_sil = silhouette_score(tb_pinturas_caract, tb_grupos[\"grupos_jerar\"])\n\nkmeans_sil =silhouette_score(tb_pinturas_caract, tb_grupos[\"grupos_kmeans\"])\n\n\nSe utiliza la función silhouette_score de scikit-learn.\ntb_pinturas_caract es el conjunto de datos de características de las pinturas.\ntb_grupos[\"grupos_jerar\"] proporciona los grupos jerárquicos asignados a cada observación.\nEl resultado se almacena en la variable jerar_sil.\nSe utiliza la misma función silhouette_score.\ntb_grupos[\"grupos_kmeans\"] proporciona los grupos de k-means asignados a cada observación.\nEl resultado se almacena en la variable kmeans_sil.\n\nEl coeficiente de silueta es una medida de la calidad de la agrupación, y un valor más cercano a 1 indica una mejor separación y cohesión de los grupos. Estos valores (jerar_sil y kmeans_sil) proporcionarán información sobre la calidad de la agrupación para los grupos jerárquicos y de k-means, respectivamente.\n\n\n\n¿Cuál es el resultado para el clustering jerárquico? ¿Cuál es el resultado para el algoritmo \\(k\\)-means?"
  },
  {
    "objectID": "unidad_03.html#visualización-de-los-grupos-conformados",
    "href": "unidad_03.html#visualización-de-los-grupos-conformados",
    "title": "Proyecto: The joy of programming",
    "section": "Visualización de los grupos conformados",
    "text": "Visualización de los grupos conformados\nEn el siguiente código, hemos dispuesto la caracterización de los grupos conformados mediante el algoritmo \\(k\\)-means. Utilizando el mapa de calor, hemos establecido las principales tendencias usando las siguientes etiquetas: \"RIVER\" (ríos), \"MOUNTAIN\" (montañas), \"AUTUMN\" (otoño), \"HOME\" (hogar), \"SEA\" (mar), PEAK (picos nevados) y \"WINTER\" (invierno). Podemos darnos cuenta de que, los resultados en python difieren de los resultados en R. Esto se debe a la fuerte dependencia que tiene \\(k\\)-means sobre los puntos iniciales. Es muy probable que las agrupaciones generadas con otros métodos nos permitan encontrar otras etiquetas, en el código están los espacios para la interpretación.\n\nRPython\n\n\n\nc(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\") -&gt; nombres_kmeans\nc(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\") -&gt; nombres_jerar\nc(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\") -&gt; nombres_dbscan\n\ntb_grupos %&gt;% \n  mutate(\n    # Podemos cambiar este código para elegir otro agrupamiento\n    # tendencia = nombres_jerar[grupos_jerar],\n    tendencia = nombres_kmeans[grupos_kmeans],\n    # tendencia = nombres_dbscan[grupos_dbscan + 1],\n  ) %&gt;% \n  select(-starts_with(\"grupos\")) %&gt;% \n  inner_join(\n    tb_pinturas\n  ) %&gt;% \n  group_by(tendencia) %&gt;% \n  summarise(across(is.numeric, mean)) -&gt; tb_centroides\n\n\ntb_centroides %&gt;% \n  select(- tendencia) %&gt;% \n  map_dbl(var) %&gt;% \n  as_tibble(rownames = \"item\") %&gt;% \n  top_n(30, value) %&gt;% \n  pull(item) -&gt; nm_items_relevantes\n\n\ntb_centroides %&gt;% \n  gather(\"rotulo\", \"value\", -tendencia) %&gt;% \n  filter(rotulo %in% nm_items_relevantes) %&gt;% \n  ggplot +\n  aes(x = tendencia, y = rotulo, fill = value) +\n  geom_raster() +\n  geom_text(aes(label = percent(value, accuracy = 1.0)), hjust = 1) +\n  scale_x_discrete(name = \"Tendencias\", position = \"top\") +\n  scale_y_discrete(name = \"Objetos\") +\n  scale_fill_gradient(high = \"#bb8899\", low = \"#ffeeee\", guide = \"none\") +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(hjust=0)\n  ) \n\n\n\n\n\n\n\n\nDefinición de Nombres de Grupos: Se definen nombres para los grupos obtenidos mediante k-medias, jerárquicos y DBSCAN.\nCreación de Centroides:\n\nSe agrega una columna “tendencia” al dataframe tb_grupos basada en los nombres de los grupos de k-medias.\nSe eliminan las columnas de grupos originales.\nSe realiza una unión con el marco de datos tb_pinturas.\nSe calculan los centroides (medias) de las variables numéricas para cada grupo.\n\nSelección de Elementos Relevantes:\n\nSe seleccionan las 30 variables con mayor varianza entre los centroides.\n\nVisualización con ggplot2:\n\nSe utiliza ggplot2 para representar gráficamente los elementos relevantes.\nSe utiliza geom_raster para mostrar colores proporcionales a los valores.\nSe utiliza geom_text para agregar etiquetas de porcentaje a los valores.\nSe personalizan los ejes y la paleta de colores.\n\n\n\n\nnombres_kmeans = dict(zip(\n  range(6),\n  [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n  ))\n\nnombres_jerar = dict(zip(\n  range(6),\n  [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n  ))\n\nnombres_dbscan = dict(zip(\n  [x - 1 for x in range(6)],\n  [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n  ))\n\n\n# Podemos cambiar este código para elegir otro agrupamiento\ntb_grupos[\"tendencia\"] = tb_grupos[\"grupos_kmeans\"].map(nombres_kmeans)\n# tb_grupos[\"tendencia\"] = tb_grupos[\"grupos_jerar\"].map(nombres_jerar)\n# tb_grupos[\"tendencia\"] = tb_grupos[\"grupos_dbscan\"].map(nombres_dbscan)\n\ntb_joined = pd.merge(\n  tb_grupos.drop(tb_grupos.filter(like='grupos').columns, axis=1),\n  tb_pinturas,\n  how='inner')\n\ntb_centroides = (tb_joined\n  .drop([\"EPISODE\", \"TITLE\"], axis=1)\n  .groupby('tendencia').agg('mean'))\n\nnm_items_relevantes = (tb_centroides\n  .apply(np.var, axis = 0)\n  .sort_values()[-30 :]\n  .index)\n\ntb_long = pd.melt(\n  tb_centroides.reset_index(), \n  id_vars=['tendencia'], var_name='rotulo', value_name='value'\n  )\ntb_filtered = tb_long[tb_long['rotulo'].isin(nm_items_relevantes)]\n\n\ncustom_palette = sns.color_palette(\"blend:#fee,#b89\", as_cmap=True)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n  pd.pivot_table(\n    tb_filtered, values='value', index='rotulo', columns='tendencia'\n    ),\n  annot=True, fmt=\".1%\", cmap = custom_palette,\n  cbar=False\n  )\nplt.xlabel('Tendencias')\nplt.ylabel('Objetos')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\nDefinición de Nombres de Grupos: Se definen diccionarios para mapear los índices de los grupos a nombres representativos para k-medias, jerárquicos y DBSCAN.\nMapeo de Tendencias: Se agrega una columna “tendencia” al marco de datos tb_grupos basada en los nombres de los grupos de k-medias (puedes cambiar esto para otros agrupamientos).\nUnión de Marcos de Datos: Se realiza una unión del marco de datos tb_grupos con tb_pinturas basada en las tendencias.\nCálculo de Centroides: Se calculan los centroides (medias) para cada tendencia.\nSelección de Elementos Relevantes: Se seleccionan las 30 variables con mayor varianza entre los centroides.\nDerretir el Marco de Datos: Se derrete el marco de datos para facilitar su uso en seaborn.\nVisualización con Seaborn y Matplotlib:\n\nSe utiliza seaborn para crear un mapa de calor utilizando el marco de datos derretido.\nSe personaliza el mapa de calor y se muestra utilizando matplotlib."
  },
  {
    "objectID": "unidad_03.html#análisis",
    "href": "unidad_03.html#análisis",
    "title": "Proyecto: The joy of programming",
    "section": "Análisis",
    "text": "Análisis\nA lo largo de los ejercicios de validación, hemos encontrado varias alternativas para dar soporte a nuestros resultados. Al respecto es necesario realizar un análisis de los resultados que hemos observado.\n\nLos resultados no son estables: en los ejercicios de validación, podemos observar resultados inconsistentes. Al cambiar de método, al cambiar de lenguaje o al cambiar de métrica. En un principio, esto puede parecer confuso, pero obedece a una realidad evidente desde la unidad anterior. La estructura interna de los datos presenta dos grupos bien diferenciados. Un primer grupo con pinturas de playas, palmeras y océanos; y otro grupo con paisajes del interior. Los demás grupos no se encuentran lo suficientemente separados, de hecho se mezclan entre sí.\nLas métricas de evaluación de los grupos realizados no son las más altas. De hecho, muchas veces tenemos un mejor ajuste para 2, 3 o 4 grupos. Esto hace que nuestras agrupaciones (6 grupos) no constituyan la forma más orgánica de agrupar las pinturas.\nEl más afectado es el algoritmo DBSCAN. Diseñado para capturar estructuras de grupos bien definidos, el algoritmo DBSCAN en este escenario no ha tenido un buen desempeño, generando grupos de tamaños muy diferentes y etiquetando como ruido a la mayoría de las observaciones. Se trata de un algoritmo que debe ser usado en problemas con grupos más separados.\nAunque la falta de una estructura de grupos en los datos le resta estabilidad a los algoritmos, es posible llegar a una caracterización definida. Hemos logrado obtener caracterizaciones de las pinturas forzando los grupos."
  }
]